<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://example.kafka-site-md.dev/0101/getting-started/><link rel=alternate type=application/rss+xml href=https://example.kafka-site-md.dev/0101/getting-started/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Getting Started | </title><meta name=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:title" content="Getting Started"><meta property="og:description" content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:type" content="website"><meta property="og:url" content="https://example.kafka-site-md.dev/0101/getting-started/"><meta itemprop=name content="Getting Started"><meta itemprop=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta name=twitter:card content="summary"><meta name=twitter:title content="Getting Started"><meta name=twitter:description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/40/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.b12dd2b63080ea49c35456712ace836e.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/0101/getting-started/>Return to the regular view of this page</a>.</p></div><h1 class=title>Getting Started</h1><div class=lead>This section provides an overview of what Kafka is, why it is useful, and how to get started using it.</div><ul><li>1: <a href=#pg-7a02e0cd06af80d5d814d953b2daeab2>Introduction</a></li><li>2: <a href=#pg-f4906305710cc5d2c55d4b7ed65328ec>Use Cases</a></li><li>3: <a href=#pg-049921c2db31a0d063bc0c32d1f83af2>Quick Start</a></li><li>4: <a href=#pg-a9232a629beaf81ffe246e5b5f22f293>Ecosystem</a></li><li>5: <a href=#pg-8bf3acd8c433ea8e154f9aed7a6afc76>Upgrading</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-7a02e0cd06af80d5d814d953b2daeab2>1 - Introduction</h1><h1 id=kafka-is-_a-distributed-streaming-platform_-what-exactly-does-that-mean>Kafka® is <em>a distributed streaming platform</em>. What exactly does that mean?<a class=td-heading-self-link href=#kafka-is-_a-distributed-streaming-platform_-what-exactly-does-that-mean aria-label="Heading self-link"></a></h1><p>We think of a streaming platform as having three key capabilities:</p><ol><li>It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.</li><li>It lets you store streams of records in a fault-tolerant way.</li><li>It lets you process streams of records as they occur.</li></ol><p>What is Kafka good for?</p><p>It gets used for two broad classes of application:</p><ol><li>Building real-time streaming data pipelines that reliably get data between systems or applications</li><li>Building real-time streaming applications that transform or react to the streams of data</li></ol><p>To understand how Kafka does these things, let&rsquo;s dive in and explore Kafka&rsquo;s capabilities from the bottom up.</p><p>First a few concepts:</p><ul><li>Kafka is run as a cluster on one or more servers.</li><li>The Kafka cluster stores streams of <em>records</em> in categories called <em>topics</em>.</li><li>Each record consists of a key, a value, and a timestamp.</li></ul><p>Kafka has four core APIs:</p><ul><li>The <a href=/documentation.html#producerapi>Producer API</a> allows an application to publish a stream records to one or more Kafka topics.</li><li>The <a href=/documentation.html#consumerapi>Consumer API</a> allows an application to subscribe to one or more topics and process the stream of records produced to them.</li><li>The <a href=/documentation.html#streams>Streams API</a> allows an application to act as a <em>stream processor</em> , consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li><li>The <a href=/documentation.html#connect>Connector API</a> allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.
<img src=/0101/images/kafka-apis.png></li></ul><p>In Kafka the communication between the clients and the servers is done with a simple, high-performance, language agnostic <a href=/protocol.html>TCP protocol</a>. This protocol is versioned and maintains backwards compatibility with older version. We provide a Java client for Kafka, but clients are available in <a href=https://cwiki.apache.org/confluence/display/KAFKA/Clients>many languages</a>.</p><h2 id=topics-and-logs>Topics and Logs<a class=td-heading-self-link href=#topics-and-logs aria-label="Heading self-link"></a></h2><p>Let&rsquo;s first dive into the core abstraction Kafka provides for a stream of records&ndash;the topic.</p><p>A topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.</p><p>For each topic, the Kafka cluster maintains a partitioned log that looks like this:</p><p><img src=/0101/images/log_anatomy.png></p><p>Each partition is an ordered, immutable sequence of records that is continually appended to&ndash;a structured commit log. The records in the partitions are each assigned a sequential id number called the <em>offset</em> that uniquely identifies each record within the partition.</p><p>The Kafka cluster retains all published records&ndash;whether or not they have been consumed&ndash;using a configurable retention period. For example, if the retention policy is set to two days, then for the two days after a record is published, it is available for consumption, after which it will be discarded to free up space. Kafka&rsquo;s performance is effectively constant with respect to data size so storing data for a long time is not a problem.</p><p><img src=/0101/images/log_consumer.png></p><p>In fact, the only metadata retained on a per-consumer basis is the offset or position of that consumer in the log. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads records, but, in fact, since the position is controlled by the consumer it can consume records in any order it likes. For example a consumer can reset to an older offset to reprocess data from the past or skip ahead to the most recent record and start consuming from &ldquo;now&rdquo;.</p><p>This combination of features means that Kafka consumers are very cheap&ndash;they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to &ldquo;tail&rdquo; the contents of any topic without changing what is consumed by any existing consumers.</p><p>The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism&ndash;more on that in a bit.</p><h2 id=distribution>Distribution<a class=td-heading-self-link href=#distribution aria-label="Heading self-link"></a></h2><p>The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.</p><p>Each partition has one server which acts as the &ldquo;leader&rdquo; and zero or more servers which act as &ldquo;followers&rdquo;. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.</p><h2 id=producers>Producers<a class=td-heading-self-link href=#producers aria-label="Heading self-link"></a></h2><p>Producers publish data to the topics of their choice. The producer is responsible for choosing which record to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the record). More on the use of partitioning in a second!</p><h2 id=consumers>Consumers<a class=td-heading-self-link href=#consumers aria-label="Heading self-link"></a></h2><p>Consumers label themselves with a <em>consumer group</em> name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p><p>If all the consumer instances have the same consumer group, then the records will effectively be load balanced over the consumer instances.</p><p>If all the consumer instances have different consumer groups, then each record will be broadcast to all the consumer processes.</p><p><img src=/0101/images/consumer-groups.png></p><p>A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.</p><p>More commonly, however, we have found that topics have a small number of consumer groups, one for each &ldquo;logical subscriber&rdquo;. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is a cluster of consumers instead of a single process.</p><p>The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a &ldquo;fair share&rdquo; of partitions at any point in time. This process of maintaining membership in the group is handled by the Kafka protocol dynamically. If new instances join the group they will take over some partitions from other members of the group; if an instance dies, its partitions will be distributed to the remaining instances.</p><p>Kafka only provides a total order over records <em>within</em> a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over records this can be achieved with a topic that has only one partition, though this will mean only one consumer process per consumer group.</p><h2 id=guarantees>Guarantees<a class=td-heading-self-link href=#guarantees aria-label="Heading self-link"></a></h2><p>At a high-level Kafka gives the following guarantees:</p><ul><li>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a record M1 is sent by the same producer as a record M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.</li><li>A consumer instance sees records in the order they are stored in the log.</li><li>For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any records committed to the log.</li></ul><p>More details on these guarantees are given in the design section of the documentation.</p><h2 id=kafka-as-a-messaging-system>Kafka as a Messaging System<a class=td-heading-self-link href=#kafka-as-a-messaging-system aria-label="Heading self-link"></a></h2><p>How does Kafka&rsquo;s notion of streams compare to a traditional enterprise messaging system?</p><p>Messaging traditionally has two models: <a href=http://en.wikipedia.org/wiki/Message_queue>queuing</a> and <a href=http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern>publish-subscribe</a>. In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers. Each of these two models has a strength and a weakness. The strength of queuing is that it allows you to divide up the processing of data over multiple consumer instances, which lets you scale your processing. Unfortunately, queues aren&rsquo;t multi-subscriber&ndash;once one process reads the data it&rsquo;s gone. Publish-subscribe allows you broadcast data to multiple processes, but has no way of scaling processing since every message goes to every subscriber.</p><p>The consumer group concept in Kafka generalizes these two concepts. As with a queue the consumer group allows you to divide up processing over a collection of processes (the members of the consumer group). As with publish-subscribe, Kafka allows you to broadcast messages to multiple consumer groups.</p><p>The advantage of Kafka&rsquo;s model is that every topic has both these properties&ndash;it can scale processing and is also multi-subscriber&ndash;there is no need to choose one or the other.</p><p>Kafka has stronger ordering guarantees than a traditional messaging system, too.</p><p>A traditional queue retains records in-order on the server, and if multiple consumers consume from the queue then the server hands out records in the order they are stored. However, although the server hands out records in order, the records are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the records is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of &ldquo;exclusive consumer&rdquo; that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.</p><p>Kafka does it better. By having a notion of parallelism&ndash;the partition&ndash;within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions.</p><h2 id=kafka-as-a-storage-system>Kafka as a Storage System<a class=td-heading-self-link href=#kafka-as-a-storage-system aria-label="Heading self-link"></a></h2><p>Any message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages. What is different about Kafka is that it is a very good storage system.</p><p>Data written to Kafka is written to disk and replicated for fault-tolerance. Kafka allows producers to wait on acknowledgement so that a write isn&rsquo;t considered complete until it is fully replicated and guaranteed to persist even if the server written to fails.</p><p>The disk structures Kafka uses scale well&ndash;Kafka will perform the same whether you have 50 KB or 50 TB of persistent data on the server.</p><p>As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.</p><h2 id=kafka-for-stream-processing>Kafka for Stream Processing<a class=td-heading-self-link href=#kafka-for-stream-processing aria-label="Heading self-link"></a></h2><p>It isn&rsquo;t enough to just read, write, and store streams of data, the purpose is to enable real-time processing of streams.</p><p>In Kafka a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics.</p><p>For example, a retail application might take in input streams of sales and shipments, and output a stream of reorders and price adjustments computed off this data.</p><p>It is possible to do simple processing directly using the producer and consumer APIs. However for more complex transformations Kafka provides a fully integrated <a href=/documentation.html#streams>Streams API</a>. This allows building applications that do non-trivial processing that compute aggregations off of streams or join streams together.</p><p>This facility helps solve the hard problems this type of application faces: handling out-of-order data, reprocessing input as code changes, performing stateful computations, etc.</p><p>The streams API builds on the core primitives Kafka provides: it uses the producer and consumer APIs for input, uses Kafka for stateful storage, and uses the same group mechanism for fault tolerance among the stream processor instances.</p><h2 id=putting-the-pieces-together>Putting the Pieces Together<a class=td-heading-self-link href=#putting-the-pieces-together aria-label="Heading self-link"></a></h2><p>This combination of messaging, storage, and stream processing may seem unusual but it is essential to Kafka&rsquo;s role as a streaming platform.</p><p>A distributed file system like HDFS allows storing static files for batch processing. Effectively a system like this allows storing and processing <em>historical</em> data from the past.</p><p>A traditional enterprise messaging system allows processing future messages that will arrive after you subscribe. Applications built in this way process future data as it arrives.</p><p>Kafka combines both of these capabilities, and the combination is critical both for Kafka usage as a platform for streaming applications as well as for streaming data pipelines.</p><p>By combining storage and low-latency subscriptions, streaming applications can treat both past and future data the same way. That is a single application can process historical, stored data but rather than ending when it reaches the last record it can keep processing as future data arrives. This is a generalized notion of stream processing that subsumes batch processing as well as message-driven applications.</p><p>Likewise for streaming data pipelines the combination of subscription to real-time events make it possible to use Kafka for very low-latency pipelines; but the ability to store data reliably make it possible to use it for critical data where the delivery of data must be guaranteed or for integration with offline systems that load data only periodically or may go down for extended periods of time for maintenance. The stream processing facilities make it possible to transform data as it arrives.</p><p>For more information on the guarantees, apis, and capabilities Kafka provides see the rest of the <a href=/documentation.html>documentation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f4906305710cc5d2c55d4b7ed65328ec>2 - Use Cases</h1><p>Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see <a href=http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying>this blog post</a>.</p><h2 id=messaging>Messaging<a class=td-heading-self-link href=#messaging aria-label="Heading self-link"></a></h2><p>Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</p><p>In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.</p><p>In this domain Kafka is comparable to traditional messaging systems such as <a href=http://activemq.apache.org>ActiveMQ</a> or <a href=https://www.rabbitmq.com>RabbitMQ</a>.</p><h2 id=website-activity-tracking>Website Activity Tracking<a class=td-heading-self-link href=#website-activity-tracking aria-label="Heading self-link"></a></h2><p>The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.</p><p>Activity tracking is often very high volume as many activity messages are generated for each user page view.</p><h2 id=metrics>Metrics<a class=td-heading-self-link href=#metrics aria-label="Heading self-link"></a></h2><p>Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.</p><h2 id=log-aggregation>Log Aggregation<a class=td-heading-self-link href=#log-aggregation aria-label="Heading self-link"></a></h2><p>Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.</p><h2 id=stream-processing>Stream Processing<a class=td-heading-self-link href=#stream-processing aria-label="Heading self-link"></a></h2><p>Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an &ldquo;articles&rdquo; topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include <a href=https://storm.apache.org/>Apache Storm</a> and <a href=http://samza.apache.org/>Apache Samza</a>.</p><h2 id=event-sourcing>Event Sourcing<a class=td-heading-self-link href=#event-sourcing aria-label="Heading self-link"></a></h2><p><a href=http://martinfowler.com/eaaDev/EventSourcing.html>Event sourcing</a> is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka&rsquo;s support for very large stored log data makes it an excellent backend for an application built in this style.</p><h2 id=commit-log>Commit Log<a class=td-heading-self-link href=#commit-log aria-label="Heading self-link"></a></h2><p>Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The <a href=/documentation.html#compaction>log compaction</a> feature in Kafka helps support this usage. In this usage Kafka is similar to <a href=http://zookeeper.apache.org/bookkeeper/>Apache BookKeeper</a> project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-049921c2db31a0d063bc0c32d1f83af2>3 - Quick Start</h1><p>This tutorial assumes you are starting fresh and have no existing Kafka® or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use <code>bin\\windows\</code> instead of <code>bin/</code>, and change the script extension to <code>.bat</code>.</p><h2 id=step-1-download-the-code>Step 1: Download the code<a class=td-heading-self-link href=#step-1-download-the-code aria-label="Heading self-link"></a></h2><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz" title="Kafka downloads">Download</a> the 0.10.1.0 release and un-tar it.</p><pre><code>&gt; **tar -xzf kafka_2.11-0.10.1.0.tgz**
&gt; **cd kafka_2.11-0.10.1.0**
</code></pre><h2 id=step-2-start-the-server>Step 2: Start the server<a class=td-heading-self-link href=#step-2-start-the-server aria-label="Heading self-link"></a></h2><p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p><pre><code>&gt; **bin/zookeeper-server-start.sh config/zookeeper.properties**
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
</code></pre><p>Now start the Kafka server:</p><pre><code>&gt; **bin/kafka-server-start.sh config/server.properties**
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
</code></pre><h2 id=step-3-create-a-topic>Step 3: Create a topic<a class=td-heading-self-link href=#step-3-create-a-topic aria-label="Heading self-link"></a></h2><p>Let&rsquo;s create a topic named &ldquo;test&rdquo; with a single partition and only one replica:</p><pre><code>&gt; **bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test**
</code></pre><p>We can now see that topic if we run the list topic command:</p><pre><code>&gt; **bin/kafka-topics.sh --list --zookeeper localhost:2181**
test
</code></pre><p>Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.</p><h2 id=step-4-send-some-messages>Step 4: Send some messages<a class=td-heading-self-link href=#step-4-send-some-messages aria-label="Heading self-link"></a></h2><p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default, each line will be sent as a separate message.</p><p>Run the producer and then type a few messages into the console to send to the server.</p><pre><code>&gt; **bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test**
**This is a message**
**This is another message**
</code></pre><h2 id=step-5-start-a-consumer>Step 5: Start a consumer<a class=td-heading-self-link href=#step-5-start-a-consumer aria-label="Heading self-link"></a></h2><p>Kafka also has a command line consumer that will dump out messages to standard output.</p><pre><code>&gt; **bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning**
This is a message
This is another message
</code></pre><p>If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.</p><p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p><h2 id=step-6-setting-up-a-multi-broker-cluster>Step 6: Setting up a multi-broker cluster<a class=td-heading-self-link href=#step-6-setting-up-a-multi-broker-cluster aria-label="Heading self-link"></a></h2><p>So far we have been running against a single broker, but that&rsquo;s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let&rsquo;s expand our cluster to three nodes (still all on our local machine).</p><p>First we make a config file for each of the brokers (on Windows use the <code>copy</code> command instead):</p><pre><code>&gt; **cp config/server.properties config/server-1.properties**
&gt; **cp config/server.properties config/server-2.properties**
</code></pre><p>Now edit these new files and set the following properties:</p><pre><code>config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dir=/tmp/kafka-logs-1

config/server-2.properties:
    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dir=/tmp/kafka-logs-2
</code></pre><p>The <code>broker.id</code> property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each other&rsquo;s data.</p><p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p><pre><code>&gt; **bin/kafka-server-start.sh config/server-1.properties &amp;**
...
&gt; **bin/kafka-server-start.sh config/server-2.properties &amp;**
...
</code></pre><p>Now create a new topic with a replication factor of three:</p><pre><code>&gt; **bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic**
</code></pre><p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the &ldquo;describe topics&rdquo; command:</p><pre><code>&gt; **bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic**
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
</code></pre><p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p><ul><li>&ldquo;leader&rdquo; is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li><li>&ldquo;replicas&rdquo; is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li><li>&ldquo;isr&rdquo; is the set of &ldquo;in-sync&rdquo; replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li></ul><p>Note that in my example node 1 is the leader for the only partition of the topic.</p><p>We can run the same command on the original topic we created to see where it is:</p><pre><code>&gt; **bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test**
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
</code></pre><p>So there is no surprise there&ndash;the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p><p>Let&rsquo;s publish a few messages to our new topic:</p><pre><code>&gt; **bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic**
...
**my test message 1**
**my test message 2**
**^C**
</code></pre><p>Now let&rsquo;s consume these messages:</p><pre><code>&gt; **bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic**
...
my test message 1
my test message 2
**^C**
</code></pre><p>Now let&rsquo;s test out fault-tolerance. Broker 1 was acting as the leader so let&rsquo;s kill it:</p><pre><code>&gt; **ps aux | grep server-1.properties**
_7564_ ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...
&gt; **kill -9 7564**
</code></pre><p>On Windows use:</p><pre><code>&gt; **wmic process get processid,caption,commandline | find &quot;java.exe&quot; | find &quot;server-1.properties&quot;**
java.exe    java  -Xmx1G -Xms1G -server -XX:+UseG1GC ... build\\libs\\kafka_2.10-0.10.1.0.jar&quot;  kafka.Kafka config\\server-1.properties    _644_
&gt; **taskkill /pid 644 /f**
</code></pre><p>Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:</p><pre><code>&gt; **bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic**
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
</code></pre><p>But the messages are still available for consumption even though the leader that took the writes originally is down:</p><pre><code>&gt; **bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic**
...
my test message 1
my test message 2
**^C**
</code></pre><h2 id=step-7-use-kafka-connect-to-importexport-data>Step 7: Use Kafka Connect to import/export data<a class=td-heading-self-link href=#step-7-use-kafka-connect-to-importexport-data aria-label="Heading self-link"></a></h2><p>Writing data from the console and writing it back to the console is a convenient place to start, but you&rsquo;ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data.</p><p>Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs <em>connectors</em> , which implement the custom logic for interacting with an external system. In this quickstart we&rsquo;ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file.</p><p>First, we&rsquo;ll start by creating some seed data to test with:</p><pre><code>&gt; **echo -e &quot;foo\nbar&quot; &gt; test.txt**
</code></pre><p>Next, we&rsquo;ll start two connectors running in <em>standalone</em> mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p><pre><code>&gt; **bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties**
</code></pre><p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file.</p><p>During startup you&rsquo;ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from <code>test.txt</code> and producing them to the topic <code>connect-test</code>, and the sink connector should start reading messages from the topic <code>connect-test</code> and write them to the file <code>test.sink.txt</code>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p><pre><code>&gt; **cat test.sink.txt**
foo
bar
</code></pre><p>Note that the data is being stored in the Kafka topic <code>connect-test</code>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p><pre><code>&gt; **bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning**
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;foo&quot;}
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;bar&quot;}
...
</code></pre><p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p><pre><code>&gt; **echo &quot;Another line&quot; &gt;&gt; test.txt**
</code></pre><p>You should see the line appear in the console consumer output and in the sink file.</p><h2 id=step-8-use-kafka-streams-to-process-data>Step 8: Use Kafka Streams to process data<a class=td-heading-self-link href=#step-8-use-kafka-streams-to-process-data aria-label="Heading self-link"></a></h2><p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the <code>WordCountDemo</code> example code (converted to use Java 8 lambda expressions for easy reading).</p><pre><code>KTable wordCounts = textLines
    // Split each text line, by whitespace, into words.
    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(&quot;\\W+&quot;)))

    // Ensure the words are available as record keys for the next aggregate operation.
    .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value))

    // Count the occurrences of each word (record key) and store the results into a table named &quot;Counts&quot;.
    .countByKey(&quot;Counts&quot;)
</code></pre><p>It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an <strong>infinite, unbounded stream</strong> of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed &ldquo;all&rdquo; the input data.</p><p>We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.</p><pre><code>&gt; **echo -e &quot;all streams lead to kafka\nhello kafka streams\njoin kafka summit&quot; &gt; file-input.txt**
</code></pre><p>Or on Windows:</p><pre><code>&gt; **echo all streams lead to kafka&gt; file-input.txt**
&gt; **echo hello kafka streams&gt;&gt; file-input.txt**
&gt; **echo|set /p=join kafka summit&gt;&gt; file-input.txt**
</code></pre><p>Next, we send this input data to the input topic named <strong>streams-file-input</strong> using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):</p><pre><code>&gt; **bin/kafka-topics.sh --create \**
            **--zookeeper localhost:2181 \**
            **--replication-factor 1 \**
            **--partitions 1 \**
            **--topic streams-file-input**



&gt; **bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input &lt; file-input.txt**
</code></pre><p>We can now run the WordCount demo application to process the input data:</p><pre><code>&gt; **bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo**
</code></pre><p>There won&rsquo;t be any STDOUT output except log entries as the results are continuously written back into another topic named <strong>streams-wordcount-output</strong> in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p><p>We can now inspect the output of the WordCount demo application by reading from its output topic:</p><pre><code>&gt; **bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \**
            **--topic streams-wordcount-output \**
            **--from-beginning \**
            **--formatter kafka.tools.DefaultMessageFormatter \**
            **--property print.key=true \**
            **--property print.value=true \**
            **--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \**
            **--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer**
</code></pre><p>with the following output data being printed to the console:</p><pre><code>all     1
lead    1
to      1
hello   1
streams 2
join    1
kafka   3
summit  1
</code></pre><p>Here, the first column is the Kafka message key, and the second column is the message value, both in in <code>java.lang.String</code> format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as &ldquo;kafka&rdquo;. For multiple records with the same key, each later record is an update of the previous one.</p><p>Now you can write more input messages to the <strong>streams-file-input</strong> topic and observe additional messages added to <strong>streams-wordcount-output</strong> topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).</p><p>You can stop the console consumer via <strong>Ctrl-C</strong>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a9232a629beaf81ffe246e5b5f22f293>4 - Ecosystem</h1><p>There are a plethora of tools that integrate with Kafka outside the main distribution. The <a href=https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem>ecosystem page</a> lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8bf3acd8c433ea8e154f9aed7a6afc76>5 - Upgrading</h1><h2 id=upgrading-from-08x-09x-or-0100x-to-01010>Upgrading from 0.8.x, 0.9.x or 0.10.0.X to 0.10.1.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-or-0100x-to-01010 aria-label="Heading self-link"></a></h2><p>0.10.1.0 has wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please notice the Potential breaking changes in 0.10.1.0 before upgrade.<br>Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients (i.e. 0.10.1.x clients only support 0.10.1.x or later brokers while 0.10.1.x brokers also support older clients).</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following properties:<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2.0, 0.9.0.0 or 0.10.0.0).</li><li>log.message.format.version=CURRENT_KAFKA_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.1.0.</li><li>If your previous message format is 0.10.0, change log.message.format.version to 0.10.1 (this is a no-op as the message format is the same for both 0.10.0 and 0.10.1). If your previous message format version is lower than 0.10.0, do not change log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0 or later.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If log.message.format.version is still lower than 0.10.0 at this point, wait until all consumers have been upgraded to 0.10.0 or later, then change log.message.format.version to 0.10.1 on each broker and restart them one by one.</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-breaking-changes-in-01010>Potential breaking changes in 0.10.1.0<a class=td-heading-self-link href=#potential-breaking-changes-in-01010 aria-label="Heading self-link"></a></h3><ul><li>The log retention time is no longer based on last modified time of the log segments. Instead it will be based on the largest timestamp of the messages in a log segment.</li><li>The log rolling time is no longer depending on log segment create time. Instead it is now based on the timestamp in the messages. More specifically. if the timestamp of the first message in the segment is T, the log will be rolled out when a new message has a timestamp greater than or equal to T + log.roll.ms</li><li>The open file handlers of 0.10.0 will increase by ~33% because of the addition of time index files for each segment.</li><li>The time index and offset index share the same index size configuration. Since each time index entry is 1.5x the size of offset index entry. User may need to increase log.index.size.max.bytes to avoid potential frequent log rolling.</li><li>Due to the increased number of index files, on some brokers with large amount the log segments (e.g. >15K), the log loading process during the broker startup could be longer. Based on our experiment, setting the num.recovery.threads.per.data.dir to one may reduce the log loading time.</li></ul><h3 id=notable-changes-in-01010>Notable changes in 0.10.1.0<a class=td-heading-self-link href=#notable-changes-in-01010 aria-label="Heading self-link"></a></h3><ul><li>The new Java consumer is no longer in beta and we recommend it for all new development. The old Scala consumers are still supported, but they will be deprecated in the next release and will be removed in a future major release.</li><li>The <code>--new-consumer</code>/<code>--new.consumer</code> switch is no longer required to use tools like MirrorMaker and the Console Consumer with the new consumer; one simply needs to pass a Kafka broker to connect to instead of the ZooKeeper ensemble. In addition, usage of the Console Consumer with the old consumer has been deprecated and it will be removed in a future major release.</li><li>Kafka clusters can now be uniquely identified by a cluster id. It will be automatically generated when a broker is upgraded to 0.10.1.0. The cluster id is available via the kafka.server:type=KafkaServer,name=ClusterId metric and it is part of the Metadata response. Serializers, client interceptors and metric reporters can receive the cluster id by implementing the ClusterResourceListener interface.</li><li>The BrokerState &ldquo;RunningAsController&rdquo; (value 4) has been removed. Due to a bug, a broker would only be in this state briefly before transitioning out of it and hence the impact of the removal should be minimal. The recommended way to detect if a given broker is the controller is via the kafka.controller:type=KafkaController,name=ActiveControllerCount metric.</li><li>The new Java Consumer now allows users to search offsets by timestamp on partitions.</li><li>The new Java Consumer now supports heartbeating from a background thread. There is a new configuration <code>max.poll.interval.ms</code> which controls the maximum time between poll invocations before the consumer will proactively leave the group (5 minutes by default). The value of the configuration <code>request.timeout.ms</code> must always be larger than <code>max.poll.interval.ms</code> because this is the maximum time that a JoinGroup request can block on the server while the consumer is rebalancing, so we have changed its default value to just above 5 minutes. Finally, the default value of <code>session.timeout.ms</code> has been adjusted down to 10 seconds, and the default value of <code>max.poll.records</code> has been changed to 500.</li><li>When using an Authorizer and a user doesn&rsquo;t have <strong>Describe</strong> authorization on a topic, the broker will no longer return TOPIC_AUTHORIZATION_FAILED errors to requests since this leaks topic names. Instead, the UNKNOWN_TOPIC_OR_PARTITION error code will be returned. This may cause unexpected timeouts or delays when using the producer and consumer since Kafka clients will typically retry automatically on unknown topic errors. You should consult the client logs if you suspect this could be happening.</li><li>Fetch responses have a size limit by default (50 MB for consumers and 10 MB for replication). The existing per partition limits also apply (1 MB for consumers and replication). Note that neither of these limits is an absolute maximum as explained in the next point.</li><li>Consumers and replicas can make progress if a message larger than the response/partition size limit is found. More concretely, if the first message in the first non-empty partition of the fetch is larger than either or both limits, the message will still be returned.</li><li>Overloaded constructors were added to <code>kafka.api.FetchRequest</code> and <code>kafka.javaapi.FetchRequest</code> to allow the caller to specify the order of the partitions (since order is significant in v3). The previously existing constructors were deprecated and the partitions are shuffled before the request is sent to avoid starvation issues.</li></ul><h3 id=new-protocol-versions>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions aria-label="Heading self-link"></a></h3><ul><li>ListOffsetRequest v1 supports accurate offset search based on timestamps.</li><li>MetadataResponse v2 introduces a new field: &ldquo;cluster_id&rdquo;.</li><li>FetchRequest v3 supports limiting the response size (in addition to the existing per partition limit), it returns messages bigger than the limits if required to make progress and the order of partitions in the request is now significant.</li><li>JoinGroup v1 introduces a new field: &ldquo;rebalance_timeout&rdquo;.</li></ul><h2 id=upgrading-from-08x-or-09x-to-01000>Upgrading from 0.8.x or 0.9.x to 0.10.0.0<a class=td-heading-self-link href=#upgrading-from-08x-or-09x-to-01000 aria-label="Heading self-link"></a></h2><p>0.10.0.0 has potential breaking changes (please review before upgrading) and possible performance impact following the upgrade. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.<br>Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.</p><p><strong>Notes to clients with version 0.9.0.0:</strong> Due to a bug introduced in 0.9.0.0, clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 <strong>before</strong> brokers are upgraded to 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following properties:<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2 or 0.9.0.0).</li><li>log.message.format.version=CURRENT_KAFKA_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.0.0. NOTE: You shouldn&rsquo;t touch log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>Once all consumers have been upgraded to 0.10.0, change log.message.format.version to 0.10.0 on each broker and restart them one by one.</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-performance-impact-following-upgrade-to-01000>Potential performance impact following upgrade to 0.10.0.0<a class=td-heading-self-link href=#potential-performance-impact-following-upgrade-to-01000 aria-label="Heading self-link"></a></h3><p>The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can&rsquo;t use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore, it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.</p><p>For clients that are upgraded to 0.10.0.0, there is no performance impact.</p><p><strong>Note:</strong> By setting the message format version, one certifies that all existing messages are on or below that message format version. Otherwise consumers before 0.10.0.0 might break. In particular, after the message format is set to 0.10.0, one should not change it back to an earlier format as it may break consumers on versions before 0.10.0.0.</p><p><strong>Note:</strong> Due to the additional timestamp introduced in each message, producers sending small messages may see a message throughput degradation because of the increased overhead. Likewise, replication now transmits an additional 8 bytes per message. If you&rsquo;re running close to the network capacity of your cluster, it&rsquo;s possible that you&rsquo;ll overwhelm the network cards and see failures and performance issues due to the overload.</p><p><strong>Note:</strong> If you have enabled compression on producers, you may notice reduced producer throughput and/or lower compression rate on the broker in some cases. When receiving compressed messages, 0.10.0 brokers avoid recompressing the messages, which in general reduces the latency and improves the throughput. In certain cases, however, this may reduce the batching size on the producer, which could lead to worse throughput. If this happens, users can tune linger.ms and batch.size of the producer for better throughput. In addition, the producer buffer used for compressing messages with snappy is smaller than the one used by the broker, which may have a negative impact on the compression ratio for the messages on disk. We intend to make this configurable in a future Kafka release.</p><h3 id=potential-breaking-changes-in-01000>Potential breaking changes in 0.10.0.0<a class=td-heading-self-link href=#potential-breaking-changes-in-01000 aria-label="Heading self-link"></a></h3><ul><li>Starting from Kafka 0.10.0.0, the message format version in Kafka is represented as the Kafka version. For example, message format 0.9.0 refers to the highest message version supported by Kafka 0.9.0.</li><li>Message format 0.10.0 has been introduced and it is used by default. It includes a timestamp field in the messages and relative offsets are used for compressed messages.</li><li>ProduceRequest/Response v2 has been introduced and it is used by default to support message format 0.10.0</li><li>FetchRequest/Response v2 has been introduced and it is used by default to support message format 0.10.0</li><li>MessageFormatter interface was changed from <code>def writeTo(key: Array[Byte], value: Array[Byte], output: PrintStream)</code> to <code>def writeTo(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]], output: PrintStream)</code></li><li>MessageReader interface was changed from <code>def readMessage(): KeyedMessage[Array[Byte], Array[Byte]]</code> to <code>def readMessage(): ProducerRecord[Array[Byte], Array[Byte]]</code></li><li>MessageFormatter&rsquo;s package was changed from <code>kafka.tools</code> to <code>kafka.common</code></li><li>MessageReader&rsquo;s package was changed from <code>kafka.tools</code> to <code>kafka.common</code></li><li>MirrorMakerMessageHandler no longer exposes the <code>handle(record: MessageAndMetadata[Array[Byte], Array[Byte]])</code> method as it was never called.</li><li>The 0.7 KafkaMigrationTool is no longer packaged with Kafka. If you need to migrate from 0.7 to 0.10.0, please migrate to 0.8 first and then follow the documented upgrade process to upgrade from 0.8 to 0.10.0.</li><li>The new consumer has standardized its APIs to accept <code>java.util.Collection</code> as the sequence type for method parameters. Existing code may have to be updated to work with the 0.10.0 client library.</li><li>LZ4-compressed message handling was changed to use an interoperable framing specification (LZ4f v1.5.1). To maintain compatibility with old clients, this change only applies to Message format 0.10.0 and later. Clients that Produce/Fetch LZ4-compressed messages using v0/v1 (Message format 0.9.0) should continue to use the 0.9.0 framing implementation. Clients that use Produce/Fetch protocols v2 or later should use interoperable LZ4f framing. A list of interoperable LZ4 libraries is available at <a href=http://www.lz4.org/>http://www.lz4.org/</a></li></ul><h3 id=notable-changes-in-01000>Notable changes in 0.10.0.0<a class=td-heading-self-link href=#notable-changes-in-01000 aria-label="Heading self-link"></a></h3><ul><li>Starting from Kafka 0.10.0.0, a new client library named <strong>Kafka Streams</strong> is available for stream processing on data stored in Kafka topics. This new client library only works with 0.10.x and upward versioned brokers due to message format changes mentioned above. For more information please read this section.</li><li>The default value of the configuration parameter <code>receive.buffer.bytes</code> is now 64K for the new consumer.</li><li>The new consumer now exposes the configuration parameter <code>exclude.internal.topics</code> to restrict internal topics (such as the consumer offsets topic) from accidentally being included in regular expression subscriptions. By default, it is enabled.</li><li>The old Scala producer has been deprecated. Users should migrate their code to the Java producer included in the kafka-clients JAR as soon as possible.</li><li>The new consumer API has been marked stable.</li></ul><h2 id=upgrading-from-080-081x-or-082x-to-0900>Upgrading from 0.8.0, 0.8.1.X or 0.8.2.X to 0.9.0.0<a class=td-heading-self-link href=#upgrading-from-080-081x-or-082x-to-0900 aria-label="Heading self-link"></a></h2><p>0.9.0.0 has potential breaking changes (please review before upgrading) and an inter-broker protocol change from previous versions. This means that upgraded brokers and clients may not be compatible with older versions. It is important that you upgrade your Kafka cluster before upgrading your clients. If you are using MirrorMaker downstream clusters should be upgraded first as well.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following property: inter.broker.protocol.version=0.8.2.X</li><li>Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.9.0.0.</li><li>Restart the brokers one by one for the new protocol version to take effect</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-breaking-changes-in-0900>Potential breaking changes in 0.9.0.0<a class=td-heading-self-link href=#potential-breaking-changes-in-0900 aria-label="Heading self-link"></a></h3><ul><li>Java 1.6 is no longer supported.</li><li>Scala 2.9 is no longer supported.</li><li>Broker IDs above 1000 are now reserved by default to automatically assigned broker IDs. If your cluster has existing broker IDs above that threshold make sure to increase the reserved.broker.max.id broker configuration property accordingly.</li><li>Configuration parameter replica.lag.max.messages was removed. Partition leaders will no longer consider the number of lagging messages when deciding which replicas are in sync.</li><li>Configuration parameter replica.lag.time.max.ms now refers not just to the time passed since last fetch request from replica, but also to time since the replica last caught up. Replicas that are still fetching messages from leaders but did not catch up to the latest messages in replica.lag.time.max.ms will be considered out of sync.</li><li>Compacted topics no longer accept messages without key and an exception is thrown by the producer if this is attempted. In 0.8.x, a message without key would cause the log compaction thread to subsequently complain and quit (and stop compacting all compacted topics).</li><li>MirrorMaker no longer supports multiple target clusters. As a result it will only accept a single &ndash;consumer.config parameter. To mirror multiple source clusters, you will need at least one MirrorMaker instance per source cluster, each with its own consumer configuration.</li><li>Tools packaged under <em>org.apache.kafka.clients.tools.*</em> have been moved to <em>org.apache.kafka.tools.*</em>. All included scripts will still function as usual, only custom code directly importing these classes will be affected.</li><li>The default Kafka JVM performance options (KAFKA_JVM_PERFORMANCE_OPTS) have been changed in kafka-run-class.sh.</li><li>The kafka-topics.sh script (kafka.admin.TopicCommand) now exits with non-zero exit code on failure.</li><li>The kafka-topics.sh script (kafka.admin.TopicCommand) will now print a warning when topic names risk metric collisions due to the use of a &lsquo;.&rsquo; or &lsquo;_&rsquo; in the topic name, and error in the case of an actual collision.</li><li>The kafka-console-producer.sh script (kafka.tools.ConsoleProducer) will use the Java producer instead of the old Scala producer be default, and users have to specify &lsquo;old-producer&rsquo; to use the old producer.</li><li>By default, all command line tools will print all logging messages to stderr instead of stdout.</li></ul><h3 id=notable-changes-in-0901>Notable changes in 0.9.0.1<a class=td-heading-self-link href=#notable-changes-in-0901 aria-label="Heading self-link"></a></h3><ul><li>The new broker id generation feature can be disabled by setting broker.id.generation.enable to false.</li><li>Configuration parameter log.cleaner.enable is now true by default. This means topics with a cleanup.policy=compact will now be compacted by default, and 128 MB of heap will be allocated to the cleaner process via log.cleaner.dedupe.buffer.size. You may want to review log.cleaner.dedupe.buffer.size and the other log.cleaner configuration values based on your usage of compacted topics.</li><li>Default value of configuration parameter fetch.min.bytes for the new consumer is now 1 by default.</li></ul><h3 id=deprecations-in-0900>Deprecations in 0.9.0.0<a class=td-heading-self-link href=#deprecations-in-0900 aria-label="Heading self-link"></a></h3><ul><li>Altering topic configuration from the kafka-topics.sh script (kafka.admin.TopicCommand) has been deprecated. Going forward, please use the kafka-configs.sh script (kafka.admin.ConfigCommand) for this functionality.</li><li>The kafka-consumer-offset-checker.sh (kafka.tools.ConsumerOffsetChecker) has been deprecated. Going forward, please use kafka-consumer-groups.sh (kafka.admin.ConsumerGroupCommand) for this functionality.</li><li>The kafka.tools.ProducerPerformance class has been deprecated. Going forward, please use org.apache.kafka.tools.ProducerPerformance for this functionality (kafka-producer-perf-test.sh will also be changed to use the new class).</li><li>The producer config block.on.buffer.full has been deprecated and will be removed in future release. Currently its default value has been changed to false. The KafkaProducer will no longer throw BufferExhaustedException but instead will use max.block.ms value to block, after which it will throw a TimeoutException. If block.on.buffer.full property is set to true explicitly, it will set the max.block.ms to Long.MAX_VALUE and metadata.fetch.timeout.ms will not be honoured</li></ul><h2 id=upgrading-from-081-to-082>Upgrading from 0.8.1 to 0.8.2<a class=td-heading-self-link href=#upgrading-from-081-to-082 aria-label="Heading self-link"></a></h2><p>0.8.2 is fully compatible with 0.8.1. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.</p><h2 id=upgrading-from-080-to-081>Upgrading from 0.8.0 to 0.8.1<a class=td-heading-self-link href=#upgrading-from-080-to-081 aria-label="Heading self-link"></a></h2><p>0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.</p><h2 id=upgrading-from-07>Upgrading from 0.7<a class=td-heading-self-link href=#upgrading-from-07 aria-label="Heading self-link"></a></h2><p>Release 0.7 is incompatible with newer releases. Major changes were made to the API, ZooKeeper data structures, and protocol, and configuration in order to add replication (Which was missing in 0.7). The upgrade from 0.7 to later versions requires a <a href=https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8>special tool</a> for migration. This migration can be done without downtime.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>