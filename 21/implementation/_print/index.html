<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://example.kafka-site-md.dev/21/implementation/><link rel=alternate type=application/rss+xml href=https://example.kafka-site-md.dev/21/implementation/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Implementation | </title><meta name=description content="Apache Kafka"><meta property="og:title" content="Implementation"><meta property="og:description" content="Apache Kafka"><meta property="og:type" content="website"><meta property="og:url" content="https://example.kafka-site-md.dev/21/implementation/"><meta itemprop=name content="Implementation"><meta itemprop=description content="Apache Kafka"><meta name=twitter:card content="summary"><meta name=twitter:title content="Implementation"><meta name=twitter:description content="Apache Kafka"><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/40/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.b12dd2b63080ea49c35456712ace836e.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/21/implementation/>Return to the regular view of this page</a>.</p></div><h1 class=title>Implementation</h1><ul><li>1: <a href=#pg-1bdccbc3e013ed53dbcabbcdfaf86f2c>Network Layer</a></li><li>2: <a href=#pg-bd9c516488f29d7c28ba0265f17718cd>Messages</a></li><li>3: <a href=#pg-3c42c9b8fd75b25513aef6cfd327a34a>Message Format</a></li><li>4: <a href=#pg-e73beac694b33e829e068a9a6b1ee6f5>Log</a></li><li>5: <a href=#pg-6f0747362878853b3aaaa08bea53ae77>Distribution</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-1bdccbc3e013ed53dbcabbcdfaf86f2c>1 - Network Layer</h1><div class=lead>Network Layer</div><h1 id=network-layer>Network Layer<a class=td-heading-self-link href=#network-layer aria-label="Heading self-link"></a></h1><p>The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the <code>MessageSet</code> interface a <code>writeTo</code> method. This allows the file-backed message set to use the more efficient <code>transferTo</code> implementation instead of an in-process buffered write. The threading model is a single acceptor thread and <em>N</em> processor threads which handle a fixed number of connections each. This design has been pretty thoroughly tested <a href=http://sna-projects.com/blog/2009/08/introducing-the-nio-socketserver-implementation>elsewhere</a> and found to be simple to implement and fast. The protocol is kept quite simple to allow for future implementation of clients in other languages.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bd9c516488f29d7c28ba0265f17718cd>2 - Messages</h1><div class=lead>Messages</div><h1 id=messages>Messages<a class=td-heading-self-link href=#messages aria-label="Heading self-link"></a></h1><p>Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage. The <code>RecordBatch</code> interface is simply an iterator over messages with specialized methods for bulk reading and writing to an NIO <code>Channel</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3c42c9b8fd75b25513aef6cfd327a34a>3 - Message Format</h1><div class=lead>Message Format</div><h1 id=message-format>Message Format<a class=td-heading-self-link href=#message-format aria-label="Heading self-link"></a></h1><p>Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.</p><h2 id=record-batch>Record Batch<a class=td-heading-self-link href=#record-batch aria-label="Heading self-link"></a></h2><p>The following is the on-disk format of a RecordBatch.</p><pre><code>		baseOffset: int64
		batchLength: int32
		partitionLeaderEpoch: int32
		magic: int8 (current magic value is 2)
		crc: int32
		attributes: int16
			bit 0~2:
				0: no compression
				1: gzip
				2: snappy
				3: lz4
				4: zstd
			bit 3: timestampType
			bit 4: isTransactional (0 means not transactional)
			bit 5: isControlBatch (0 means not a control batch)
			bit 6~15: unused
		lastOffsetDelta: int32
		firstTimestamp: int64
		maxTimestamp: int64
		producerId: int64
		producerEpoch: int16
		baseSequence: int32
		records: [Record]
</code></pre><p>Note that when compression is enabled, the compressed record data is serialized directly following the count of the number of records.</p><p>The CRC covers the data from the attributes to the end of the batch (i.e. all the bytes that follow the CRC). It is located after the magic byte, which means that clients must parse the magic byte before deciding how to interpret the bytes between the batch length and the magic byte. The partition leader epoch field is not included in the CRC computation to avoid the need to recompute the CRC when this field is assigned for every batch that is received by the broker. The CRC-32C (Castagnoli) polynomial is used for the computation.</p><p>On compaction: unlike the older message formats, magic v2 and above preserves the first and last offset/sequence numbers from the original batch when the log is cleaned. This is required in order to be able to restore the producer&rsquo;s state when the log is reloaded. If we did not retain the last sequence number, for example, then after a partition leader failure, the producer might see an OutOfSequence error. The base sequence number must be preserved for duplicate checking (the broker checks incoming Produce requests for duplicates by verifying that the first and last sequence numbers of the incoming batch match the last from that producer). As a result, it is possible to have empty batches in the log when all the records in the batch are cleaned but batch is still retained in order to preserve a producer&rsquo;s last sequence number. One oddity here is that the baseTimestamp field is not preserved during compaction, so it will change if the first record in the batch is compacted away.</p><h3 id=control-batches>Control Batches<a class=td-heading-self-link href=#control-batches aria-label="Heading self-link"></a></h3><p>A control batch contains a single record called the control record. Control records should not be passed on to applications. Instead, they are used by consumers to filter out aborted transactional messages.</p><p>The key of a control record conforms to the following schema:</p><pre><code>       version: int16 (current version is 0)
       type: int16 (0 indicates an abort marker, 1 indicates a commit)
</code></pre><p>The schema for the value of a control record is dependent on the type. The value is opaque to clients.</p><h2 id=record>Record<a class=td-heading-self-link href=#record aria-label="Heading self-link"></a></h2><p>Record level headers were introduced in Kafka 0.11.0. The on-disk format of a record with Headers is delineated below.</p><pre><code>		length: varint
		attributes: int8
			bit 0~7: unused
		timestampDelta: varint
		offsetDelta: varint
		keyLength: varint
		key: byte[]
		valueLen: varint
		value: byte[]
		Headers =&gt; [Header]
</code></pre><h3 id=record-header>Record Header<a class=td-heading-self-link href=#record-header aria-label="Heading self-link"></a></h3><pre><code>		headerKeyLength: varint
		headerKey: String
		headerValueLength: varint
		Value: byte[]
</code></pre><p>We use the same varint encoding as Protobuf. More information on the latter can be found <a href=https://developers.google.com/protocol-buffers/docs/encoding#varints>here</a>. The count of headers in a record is also encoded as a varint.</p><h2 id=old-message-format>Old Message Format<a class=td-heading-self-link href=#old-message-format aria-label="Heading self-link"></a></h2><p>Prior to Kafka 0.11, messages were transferred and stored in <em>message sets</em>. In a message set, each message has its own metadata. Note that although message sets are represented as an array, they are not preceded by an int32 array size like other array elements in the protocol.</p><p><strong>Message Set:</strong></p><pre><code>    MessageSet (Version: 0) =&gt; [offset message_size message]
        offset =&gt; INT64
        message_size =&gt; INT32
        message =&gt; crc magic_byte attributes key value
            crc =&gt; INT32
            magic_byte =&gt; INT8
            attributes =&gt; INT8
                bit 0~2:
                    0: no compression
                    1: gzip
                    2: snappy
                bit 3~7: unused
            key =&gt; BYTES
            value =&gt; BYTES
    


    MessageSet (Version: 1) =&gt; [offset message_size message]
        offset =&gt; INT64
        message_size =&gt; INT32
        message =&gt; crc magic_byte attributes key value
            crc =&gt; INT32
            magic_byte =&gt; INT8
            attributes =&gt; INT8
                bit 0~2:
                    0: no compression
                    1: gzip
                    2: snappy
                    3: lz4
                bit 3: timestampType
                    0: create time
                    1: log append time
                bit 4~7: unused
            timestamp =&gt;INT64
            key =&gt; BYTES
            value =&gt; BYTES
</code></pre><p>In versions prior to Kafka 0.10, the only supported message format version (which is indicated in the magic value) was 0. Message format version 1 was introduced with timestamp support in version 0.10.</p><ul><li>Similarly to version 2 above, the lowest bits of attributes represent the compression type.</li><li>In version 1, the producer should always set the timestamp type bit to 0. If the topic is configured to use log append time, (through either broker level config log.message.timestamp.type = LogAppendTime or topic level config message.timestamp.type = LogAppendTime), the broker will overwrite the timestamp type and the timestamp in the message set.</li><li>The highest bits of attributes must be set to 0.</li></ul><p>In message format versions 0 and 1 Kafka supports recursive messages to enable compression. In this case the message&rsquo;s attributes must be set to indicate one of the compression types and the value field will contain a message set compressed with that type. We often refer to the nested messages as &ldquo;inner messages&rdquo; and the wrapping message as the &ldquo;outer message.&rdquo; Note that the key should be null for the outer message and its offset will be the offset of the last inner message.</p><p>When receiving recursive version 0 messages, the broker decompresses them and each inner message is assigned an offset individually. In version 1, to avoid server side re-compression, only the wrapper message will be assigned an offset. The inner messages will have relative offsets. The absolute offset can be computed using the offset from the outer message, which corresponds to the offset assigned to the last inner message.</p><p>The crc field contains the CRC32 (and not CRC-32C) of the subsequent message bytes (i.e. from magic byte to the value).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e73beac694b33e829e068a9a6b1ee6f5>4 - Log</h1><div class=lead>Log</div><h1 id=log>Log<a class=td-heading-self-link href=#log aria-label="Heading self-link"></a></h1><p>A log for a topic named &ldquo;my_topic&rdquo; with two partitions consists of two directories (namely <code>my_topic_0</code> and <code>my_topic_1</code>) populated with data files containing the messages for that topic. The format of the log files is a sequence of &ldquo;log entries&rdquo;"; each log entry is a 4 byte integer <em>N</em> storing the message length which is followed by the <em>N</em> message bytes. Each message is uniquely identified by a 64-bit integer <em>offset</em> giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition. The on-disk format of each message is given below. Each log file is named with the offset of the first message it contains. So the first file created will be 00000000000.kafka, and each additional file will have an integer name roughly <em>S</em> bytes from the previous file where <em>S</em> is the max log file size given in the configuration.</p><p>The exact binary format for records is versioned and maintained as a standard interface so record batches can be transferred between producer, broker, and client without recopying or conversion when desirable. The previous section included details about the on-disk format of records.</p><p>The use of the message offset as the message id is unusual. Our original idea was to use a GUID generated by the producer, and maintain a mapping from GUID to offset on each broker. But since a consumer must maintain an ID for each server, the global uniqueness of the GUID provides no value. Furthermore, the complexity of maintaining the mapping from a random id to an offset requires a heavy weight index structure which must be synchronized with disk, essentially requiring a full persistent random-access data structure. Thus to simplify the lookup structure we decided to use a simple per-partition atomic counter which could be coupled with the partition id and node id to uniquely identify a message; this makes the lookup structure simpler, though multiple seeks per consumer request are still likely. However once we settled on a counter, the jump to directly using the offset seemed natural&ndash;both after all are monotonically increasing integers unique to a partition. Since the offset is hidden from the consumer API this decision is ultimately an implementation detail and we went with the more efficient approach.</p><p><img src=/21/images/kafka_log.png></p><h2 id=writes>Writes<a class=td-heading-self-link href=#writes aria-label="Heading self-link"></a></h2><p>The log allows serial appends which always go to the last file. This file is rolled over to a fresh file when it reaches a configurable size (say 1GB). The log takes two configuration parameters: <em>M</em> , which gives the number of messages to write before forcing the OS to flush the file to disk, and <em>S</em> , which gives a number of seconds after which a flush is forced. This gives a durability guarantee of losing at most <em>M</em> messages or <em>S</em> seconds of data in the event of a system crash.</p><h2 id=reads>Reads<a class=td-heading-self-link href=#reads aria-label="Heading self-link"></a></h2><p>Reads are done by giving the 64-bit logical offset of a message and an <em>S</em> -byte max chunk size. This will return an iterator over the messages contained in the <em>S</em> -byte buffer. <em>S</em> is intended to be larger than any single message, but in the event of an abnormally large message, the read can be retried multiple times, each time doubling the buffer size, until the message is read successfully. A maximum message and buffer size can be specified to make the server reject messages larger than some size, and to give a bound to the client on the maximum it needs to ever read to get a complete message. It is likely that the read buffer ends with a partial message, this is easily detected by the size delimiting.</p><p>The actual process of reading from an offset requires first locating the log segment file in which the data is stored, calculating the file-specific offset from the global offset value, and then reading from that file offset. The search is done as a simple binary search variation against an in-memory range maintained for each file.</p><p>The log provides the capability of getting the most recently written message to allow clients to start subscribing as of &ldquo;right now&rdquo;. This is also useful in the case the consumer fails to consume its data within its SLA-specified number of days. In this case when the client attempts to consume a non-existent offset it is given an OutOfRangeException and can either reset itself or fail as appropriate to the use case.</p><p>The following is the format of the results sent to the consumer.</p><pre><code>    MessageSetSend (fetch result)

    total length     : 4 bytes
    error code       : 2 bytes
    message 1        : x bytes
    ...
    message n        : x bytes
    


    MultiMessageSetSend (multiFetch result)

    total length       : 4 bytes
    error code         : 2 bytes
    messageSetSend 1
    ...
    messageSetSend n
</code></pre><h2 id=deletes>Deletes<a class=td-heading-self-link href=#deletes aria-label="Heading self-link"></a></h2><p>Data is deleted one log segment at a time. The log manager allows pluggable delete policies to choose which files are eligible for deletion. The current policy deletes any log with a modification time of more than <em>N</em> days ago, though a policy which retained the last <em>N</em> GB could also be useful. To avoid locking reads while still allowing deletes that modify the segment list we use a copy-on-write style segment list implementation that provides consistent views to allow a binary search to proceed on an immutable static snapshot view of the log segments while deletes are progressing.</p><h2 id=guarantees>Guarantees<a class=td-heading-self-link href=#guarantees aria-label="Heading self-link"></a></h2><p>The log provides a configuration parameter <em>M</em> which controls the maximum number of messages that are written before forcing a flush to disk. On startup a log recovery process is run that iterates over all messages in the newest log segment and verifies that each message entry is valid. A message entry is valid if the sum of its size and offset are less than the length of the file AND the CRC32 of the message payload matches the CRC stored with the message. In the event corruption is detected the log is truncated to the last valid offset.</p><p>Note that two kinds of corruption must be handled: truncation in which an unwritten block is lost due to a crash, and corruption in which a nonsense block is ADDED to the file. The reason for this is that in general the OS makes no guarantee of the write order between the file inode and the actual block data so in addition to losing written data the file can gain nonsense data if the inode is updated with a new size but a crash occurs before the block containing that data is written. The CRC detects this corner case, and prevents it from corrupting the log (though the unwritten messages are, of course, lost).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6f0747362878853b3aaaa08bea53ae77>5 - Distribution</h1><div class=lead>Distribution</div><h1 id=distribution>Distribution<a class=td-heading-self-link href=#distribution aria-label="Heading self-link"></a></h1><h2 id=consumer-offset-tracking>Consumer Offset Tracking<a class=td-heading-self-link href=#consumer-offset-tracking aria-label="Heading self-link"></a></h2><p>Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker). Consumer groups are assigned to coordinators based on their group names. A consumer can look up its coordinator by issuing a FindCoordinatorRequest to any Kafka broker and reading the FindCoordinatorResponse which will contain the coordinator details. The consumer can then proceed to commit or fetch offsets from the coordinator broker. In case the coordinator moves, the consumer will need to rediscover the coordinator. Offset commits can be done automatically or manually by consumer instance.</p><p>When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __<em>consumer_offsets</em>. The broker sends a successful offset commit response to the consumer only after all the replicas of the offsets topic receive the offsets. In case the offsets fail to replicate within a configurable timeout, the offset commit will fail and the consumer may retry the commit after backing off. The brokers periodically compact the offsets topic since it only needs to maintain the most recent offset commit per partition. The coordinator also caches the offsets in an in-memory table in order to serve offset fetches quickly.</p><p>When the coordinator receives an offset fetch request, it simply returns the last committed offset vector from the offsets cache. In case coordinator was just started or if it just became the coordinator for a new set of consumer groups (by becoming a leader for a partition of the offsets topic), it may need to load the offsets topic partition into the cache. In this case, the offset fetch will fail with an CoordinatorLoadInProgressException and the consumer may retry the OffsetFetchRequest after backing off.</p><h2 id=zookeeper-directories>ZooKeeper Directories<a class=td-heading-self-link href=#zookeeper-directories aria-label="Heading self-link"></a></h2><p>The following gives the ZooKeeper structures and algorithms used for co-ordination between consumers and brokers.</p><h2 id=notation>Notation<a class=td-heading-self-link href=#notation aria-label="Heading self-link"></a></h2><p>When an element in a path is denoted [xyz], that means that the value of xyz is not fixed and there is in fact a ZooKeeper znode for each possible value of xyz. For example /topics/[topic] would be a directory named /topics containing a sub-directory for each topic name. Numerical ranges are also given such as [0&mldr;5] to indicate the subdirectories 0, 1, 2, 3, 4. An arrow -> is used to indicate the contents of a znode. For example /hello -> world would indicate a znode /hello containing the value &ldquo;world&rdquo;.</p><h2 id=broker-node-registry>Broker Node Registry<a class=td-heading-self-link href=#broker-node-registry aria-label="Heading self-link"></a></h2><pre><code>    /brokers/ids/[0...N] --&gt; {&quot;jmx_port&quot;:...,&quot;timestamp&quot;:...,&quot;endpoints&quot;:[...],&quot;host&quot;:...,&quot;version&quot;:...,&quot;port&quot;:...} (ephemeral node)
</code></pre><p>This is a list of all present broker nodes, each of which provides a unique logical broker id which identifies it to consumers (which must be given as part of its configuration). On startup, a broker node registers itself by creating a znode with the logical broker id under /brokers/ids. The purpose of the logical broker id is to allow a broker to be moved to a different physical machine without affecting consumers. An attempt to register a broker id that is already in use (say because two servers are configured with the same broker id) results in an error.</p><p>Since the broker registers itself in ZooKeeper using ephemeral znodes, this registration is dynamic and will disappear if the broker is shutdown or dies (thus notifying consumers it is no longer available).</p><h2 id=broker-topic-registry>Broker Topic Registry<a class=td-heading-self-link href=#broker-topic-registry aria-label="Heading self-link"></a></h2><pre><code>    /brokers/topics/[topic]/partitions/[0...N]/state --&gt; {&quot;controller_epoch&quot;:...,&quot;leader&quot;:...,&quot;version&quot;:...,&quot;leader_epoch&quot;:...,&quot;isr&quot;:[...]} (ephemeral node)
</code></pre><p>Each broker registers itself under the topics it maintains and stores the number of partitions for that topic.</p><h2 id=cluster-id>Cluster Id<a class=td-heading-self-link href=#cluster-id aria-label="Heading self-link"></a></h2><p>The cluster id is a unique and immutable identifier assigned to a Kafka cluster. The cluster id can have a maximum of 22 characters and the allowed characters are defined by the regular expression [a-zA-Z0-9_\-]+, which corresponds to the characters used by the URL-safe Base64 variant with no padding. Conceptually, it is auto-generated when a cluster is started for the first time.</p><p>Implementation-wise, it is generated when a broker with version 0.10.1 or later is successfully started for the first time. The broker tries to get the cluster id from the <code>/cluster/id</code> znode during startup. If the znode does not exist, the broker generates a new cluster id and creates the znode with this cluster id.</p><h2 id=broker-node-registration>Broker node registration<a class=td-heading-self-link href=#broker-node-registration aria-label="Heading self-link"></a></h2><p>The broker nodes are basically independent, so they only publish information about what they have. When a broker joins, it registers itself under the broker node registry directory and writes information about its host name and port. The broker also register the list of existing topics and their logical partitions in the broker topic registry. New topics are registered dynamically when they are created on the broker.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>