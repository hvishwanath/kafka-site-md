<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://example.kafka-site-md.dev/40/getting-started/><link rel=alternate type=application/rss+xml href=https://example.kafka-site-md.dev/40/getting-started/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Getting Started | </title><meta name=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:title" content="Getting Started"><meta property="og:description" content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:type" content="website"><meta property="og:url" content="https://example.kafka-site-md.dev/40/getting-started/"><meta itemprop=name content="Getting Started"><meta itemprop=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta name=twitter:card content="summary"><meta name=twitter:title content="Getting Started"><meta name=twitter:description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/41/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/41/>4.1</a></li><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e58a36913f949563db0a14b5eaf8f6a5.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/40/getting-started/>Return to the regular view of this page</a>.</p></div><h1 class=title>Getting Started</h1><div class=lead>This section provides an overview of what Kafka is, why it is useful, and how to get started using it.</div><ul><li>1: <a href=#pg-18cb84658ea274059968ea6aee464417>Introduction</a></li><li>2: <a href=#pg-87be86f466ccf95b3e25ecfa713a9ae9>Use Cases</a></li><li>3: <a href=#pg-808eec61f3f1629c7f824e92a5d5d2fb>Quick Start</a></li><li>4: <a href=#pg-a0632c888d774a65bfc52087a226533b>Ecosystem</a></li><li>5: <a href=#pg-94bef8fa786f550271be06febb7d60a8>Upgrading</a></li><li>6: <a href=#pg-454ace89e9084b5fe25ece7be47eddfc>KRaft vs ZooKeeper</a></li><li>7: <a href=#pg-bd44e44b3bd848d14840f2f3c80dcfb3>Compatibility</a></li><li>8: <a href=#pg-e2ace19dfe1cc49207a18f13b8bf083f>Docker</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-18cb84658ea274059968ea6aee464417>1 - Introduction</h1><h2 id=what-is-event-streaming>What is event streaming?<a class=td-heading-self-link href=#what-is-event-streaming aria-label="Heading self-link"></a></h2><p>Event streaming is the digital equivalent of the human body&rsquo;s central nervous system. It is the technological foundation for the &lsquo;always-on&rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.</p><p>Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.</p><h2 id=what-can-i-use-event-streaming-for>What can I use event streaming for?<a class=td-heading-self-link href=#what-can-i-use-event-streaming-for aria-label="Heading self-link"></a></h2><p>Event streaming is applied to a <a href=/powered-by>wide variety of use cases</a> across a plethora of industries and organizations. Its many examples include:</p><ul><li>To process payments and financial transactions in real-time, such as in stock exchanges, banks, and insurances.</li><li>To track and monitor cars, trucks, fleets, and shipments in real-time, such as in logistics and the automotive industry.</li><li>To continuously capture and analyze sensor data from IoT devices or other equipment, such as in factories and wind parks.</li><li>To collect and immediately react to customer interactions and orders, such as in retail, the hotel and travel industry, and mobile applications.</li><li>To monitor patients in hospital care and predict changes in condition to ensure timely treatment in emergencies.</li><li>To connect, store, and make available data produced by different divisions of a company.</li><li>To serve as the foundation for data platforms, event-driven architectures, and microservices.</li></ul><h2 id=apache-kafka-is-an-event-streaming-platform-what-does-that-mean>Apache Kafka® is an event streaming platform. What does that mean?<a class=td-heading-self-link href=#apache-kafka-is-an-event-streaming-platform-what-does-that-mean aria-label="Heading self-link"></a></h2><p>Kafka combines three key capabilities so you can implement <a href=/powered-by>your use cases</a> for event streaming end-to-end with a single battle-tested solution:</p><ol><li>To <strong>publish</strong> (write) and <strong>subscribe to</strong> (read) streams of events, including continuous import/export of your data from other systems.</li><li>To <strong>store</strong> streams of events durably and reliably for as long as you want.</li><li>To <strong>process</strong> streams of events as they occur or retrospectively.</li></ol><p>And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.</p><h2 id=how-does-kafka-work-in-a-nutshell>How does Kafka work in a nutshell?<a class=td-heading-self-link href=#how-does-kafka-work-in-a-nutshell aria-label="Heading self-link"></a></h2><p>Kafka is a distributed system consisting of <strong>servers</strong> and <strong>clients</strong> that communicate via a high-performance <a href=/protocol.html>TCP network protocol</a>. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.</p><p><strong>Servers</strong> : Kafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run <a href=/#connect>Kafka Connect</a> to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss.</p><p><strong>Clients</strong> : They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka ships with some such clients included, which are augmented by <a href=https://cwiki.apache.org/confluence/display/KAFKA/Clients>dozens of clients</a> provided by the Kafka community: clients are available for Java and Scala including the higher-level <a href=/streams/>Kafka Streams</a> library, for Go, Python, C/C++, and many other programming languages as well as REST APIs.</p><h2 id=main-concepts-and-terminology>Main Concepts and Terminology<a class=td-heading-self-link href=#main-concepts-and-terminology aria-label="Heading self-link"></a></h2><p>An <strong>event</strong> records the fact that &ldquo;something happened&rdquo; in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here&rsquo;s an example event:</p><ul><li>Event key: &ldquo;Alice&rdquo;</li><li>Event value: &ldquo;Made a payment of $200 to Bob&rdquo;</li><li>Event timestamp: &ldquo;Jun. 25, 2020 at 2:06 p.m.&rdquo;</li></ul><p><strong>Producers</strong> are those client applications that publish (write) events to Kafka, and <strong>consumers</strong> are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers. Kafka provides various <a href=/#semantics>guarantees</a> such as the ability to process events exactly-once.</p><p>Events are organized and durably stored in <strong>topics</strong>. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be &ldquo;payments&rdquo;. Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka&rsquo;s performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.</p><p>Topics are <strong>partitioned</strong> , meaning a topic is spread over a number of &ldquo;buckets&rdquo; located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic&rsquo;s partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka <a href=/#semantics>guarantees</a> that any consumer of a given topic-partition will always read that partition&rsquo;s events in exactly the same order as they were written.</p><p><img src=/images/streams-and-tables-p1_p4.png> Figure: This example topic has four partitions P1–P4. Two different producer clients are publishing, independently from each other, new events to the topic by writing events over the network to the topic&rsquo;s partitions. Events with the same key (denoted by their color in the figure) are written to the same partition. Note that both producers can write to the same partition if appropriate.</p><p>To make your data fault-tolerant and highly-available, every topic can be <strong>replicated</strong> , even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. This replication is performed at the level of topic-partitions.</p><p>This primer should be sufficient for an introduction. The <a href=/#design>Design</a> section of the documentation explains Kafka&rsquo;s various concepts in full detail, if you are interested.</p><h2 id=kafka-apis>Kafka APIs<a class=td-heading-self-link href=#kafka-apis aria-label="Heading self-link"></a></h2><p>In addition to command line tooling for management and administration tasks, Kafka has five core APIs for Java and Scala:</p><ul><li>The <a href=/documentation.html#adminapi>Admin API</a> to manage and inspect topics, brokers, and other Kafka objects.</li><li>The <a href=/documentation.html#producerapi>Producer API</a> to publish (write) a stream of events to one or more Kafka topics.</li><li>The <a href=/documentation.html#consumerapi>Consumer API</a> to subscribe to (read) one or more topics and to process the stream of events produced to them.</li><li>The <a href=/streams>Kafka Streams API</a> to implement stream processing applications and microservices. It provides higher-level functions to process event streams, including transformations, stateful operations like aggregations and joins, windowing, processing based on event-time, and more. Input is read from one or more topics in order to generate output to one or more topics, effectively transforming the input streams to output streams.</li><li>The <a href=/documentation.html#connect>Kafka Connect API</a> to build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications so they can integrate with Kafka. For example, a connector to a relational database like PostgreSQL might capture every change to a set of tables. However, in practice, you typically don&rsquo;t need to implement your own connectors because the Kafka community already provides hundreds of ready-to-use connectors.</li></ul><h2 id=where-to-go-from-here>Where to go from here<a class=td-heading-self-link href=#where-to-go-from-here aria-label="Heading self-link"></a></h2><ul><li>To get hands-on experience with Kafka, follow the <a href=/quickstart>Quickstart</a>.</li><li>To understand Kafka in more detail, read the <a href=/>Documentation</a>. You also have your choice of <a href=/books-and-papers>Kafka books and academic papers</a>.</li><li>Browse through the <a href=/powered-by>Use Cases</a> to learn how other users in our world-wide community are getting value out of Kafka.</li><li>Join a <a href=/events>local Kafka meetup group</a> and <a href=https://kafka-summit.org/past-events/>watch talks from Kafka Summit</a>, the main conference of the Kafka community.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-87be86f466ccf95b3e25ecfa713a9ae9>2 - Use Cases</h1><p>Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see <a href=https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/>this blog post</a>.</p><h2 id=messaging>Messaging<a class=td-heading-self-link href=#messaging aria-label="Heading self-link"></a></h2><p>Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</p><p>In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.</p><p>In this domain Kafka is comparable to traditional messaging systems such as <a href=https://activemq.apache.org>ActiveMQ</a> or <a href=https://www.rabbitmq.com>RabbitMQ</a>.</p><h2 id=website-activity-tracking>Website Activity Tracking<a class=td-heading-self-link href=#website-activity-tracking aria-label="Heading self-link"></a></h2><p>The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.</p><p>Activity tracking is often very high volume as many activity messages are generated for each user page view.</p><h2 id=metrics>Metrics<a class=td-heading-self-link href=#metrics aria-label="Heading self-link"></a></h2><p>Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.</p><h2 id=log-aggregation>Log Aggregation<a class=td-heading-self-link href=#log-aggregation aria-label="Heading self-link"></a></h2><p>Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.</p><h2 id=stream-processing>Stream Processing<a class=td-heading-self-link href=#stream-processing aria-label="Heading self-link"></a></h2><p>Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an &ldquo;articles&rdquo; topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called <a href=/streams>Kafka Streams</a> is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include <a href=https://storm.apache.org/>Apache Storm</a> and <a href=https://samza.apache.org/>Apache Samza</a>.</p><h2 id=event-sourcing>Event Sourcing<a class=td-heading-self-link href=#event-sourcing aria-label="Heading self-link"></a></h2><p><a href=https://martinfowler.com/eaaDev/EventSourcing.html>Event sourcing</a> is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka&rsquo;s support for very large stored log data makes it an excellent backend for an application built in this style.</p><h2 id=commit-log>Commit Log<a class=td-heading-self-link href=#commit-log aria-label="Heading self-link"></a></h2><p>Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The <a href=/documentation.html#compaction>log compaction</a> feature in Kafka helps support this usage. In this usage Kafka is similar to <a href=https://bookkeeper.apache.org/>Apache BookKeeper</a> project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-808eec61f3f1629c7f824e92a5d5d2fb>3 - Quick Start</h1><h2 id=step-1-get-kafka>Step 1: Get Kafka<a class=td-heading-self-link href=#step-1-get-kafka aria-label="Heading self-link"></a></h2><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/4.0.0/kafka_2.13-4.0.0.tgz">Download</a> the latest Kafka release and extract it:</p><pre><code>$ tar -xzf kafka_2.13-4.0.0.tgz
$ cd kafka_2.13-4.0.0
</code></pre><h2 id=step-2-start-the-kafka-environment>Step 2: Start the Kafka environment<a class=td-heading-self-link href=#step-2-start-the-kafka-environment aria-label="Heading self-link"></a></h2><p>NOTE: Your local environment must have Java 17+ installed.</p><p>Kafka can be run using local scripts and downloaded files or the docker image.</p><h3 id=using-downloaded-files>Using downloaded files<a class=td-heading-self-link href=#using-downloaded-files aria-label="Heading self-link"></a></h3><p>Generate a Cluster UUID</p><pre><code>$ KAFKA_CLUSTER_ID=&quot;$(bin/kafka-storage.sh random-uuid)&quot;
</code></pre><p>Format Log Directories</p><pre><code>$ bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties
</code></pre><p>Start the Kafka Server</p><pre><code>$ bin/kafka-server-start.sh config/server.properties
</code></pre><p>Once the Kafka server has successfully launched, you will have a basic Kafka environment running and ready to use.</p><h3 id=using-jvm-based-apache-kafka-docker-image>Using JVM Based Apache Kafka Docker Image<a class=td-heading-self-link href=#using-jvm-based-apache-kafka-docker-image aria-label="Heading self-link"></a></h3><p>Get the Docker image:</p><pre><code>$ docker pull apache/kafka:4.0.0
</code></pre><p>Start the Kafka Docker container:</p><pre><code>$ docker run -p 9092:9092 apache/kafka:4.0.0
</code></pre><h3 id=using-graalvm-based-native-apache-kafka-docker-image>Using GraalVM Based Native Apache Kafka Docker Image<a class=td-heading-self-link href=#using-graalvm-based-native-apache-kafka-docker-image aria-label="Heading self-link"></a></h3><p>Get the Docker image:</p><pre><code>$ docker pull apache/kafka-native:4.0.0
</code></pre><p>Start the Kafka Docker container:</p><pre><code>$ docker run -p 9092:9092 apache/kafka-native:4.0.0
</code></pre><h2 id=step-3-create-a-topic-to-store-your-events>Step 3: Create a topic to store your events<a class=td-heading-self-link href=#step-3-create-a-topic-to-store-your-events aria-label="Heading self-link"></a></h2><p>Kafka is a distributed <em>event streaming platform</em> that lets you read, write, store, and process <a href=/#messages><em>events</em></a> (also called <em>records</em> or <em>messages</em> in the documentation) across many machines.</p><p>Example events are payment transactions, geolocation updates from mobile phones, shipping orders, sensor measurements from IoT devices or medical equipment, and much more. These events are organized and stored in <a href=/#intro_concepts_and_terms><em>topics</em></a>. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder.</p><p>So before you can write your first events, you must create a topic. Open another terminal session and run:</p><pre><code>$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
</code></pre><p>All of Kafka&rsquo;s command line tools have additional options: run the <code>kafka-topics.sh</code> command without any arguments to display usage information. For example, it can also show you <a href=/#intro_concepts_and_terms>details such as the partition count</a> of the new topic:</p><pre><code>$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
Topic: quickstart-events        TopicId: NPmZHyhbR9y00wMglMH2sg PartitionCount: 1       ReplicationFactor: 1	Configs:
Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0
</code></pre><h2 id=step-4-write-some-events-into-the-topic>Step 4: Write some events into the topic<a class=td-heading-self-link href=#step-4-write-some-events-into-the-topic aria-label="Heading self-link"></a></h2><p>A Kafka client communicates with the Kafka brokers via the network for writing (or reading) events. Once received, the brokers will store the events in a durable and fault-tolerant manner for as long as you need—even forever.</p><p>Run the console producer client to write a few events into your topic. By default, each line you enter will result in a separate event being written to the topic.</p><pre><code>$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
&gt;This is my first event
&gt;This is my second event
</code></pre><p>You can stop the producer client with <code>Ctrl-C</code> at any time.</p><h2 id=step-5-read-the-events>Step 5: Read the events<a class=td-heading-self-link href=#step-5-read-the-events aria-label="Heading self-link"></a></h2><p>Open another terminal session and run the console consumer client to read the events you just created:</p><pre><code>$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
</code></pre><p>You can stop the consumer client with <code>Ctrl-C</code> at any time.</p><p>Feel free to experiment: for example, switch back to your producer terminal (previous step) to write additional events, and see how the events immediately show up in your consumer terminal.</p><p>Because events are durably stored in Kafka, they can be read as many times and by as many consumers as you want. You can easily verify this by opening yet another terminal session and re-running the previous command again.</p><h2 id=step-6-importexport-your-data-as-streams-of-events-with-kafka-connect>Step 6: Import/export your data as streams of events with Kafka Connect<a class=td-heading-self-link href=#step-6-importexport-your-data-as-streams-of-events-with-kafka-connect aria-label="Heading self-link"></a></h2><p>You probably have lots of data in existing systems like relational databases or traditional messaging systems, along with many applications that already use these systems. <a href=/#connect>Kafka Connect</a> allows you to continuously ingest data from external systems into Kafka, and vice versa. It is an extensible tool that runs <em>connectors</em> , which implement the custom logic for interacting with an external system. It is thus very easy to integrate existing systems with Kafka. To make this process even easier, there are hundreds of such connectors readily available.</p><p>In this quickstart we&rsquo;ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file.</p><p>First, make sure to add <code>connect-file-4.0.0.jar</code> to the <code>plugin.path</code> property in the Connect worker&rsquo;s configuration. For the purpose of this quickstart we&rsquo;ll use a relative path and consider the connectors&rsquo; package as an uber jar, which works when the quickstart commands are run from the installation directory. However, it&rsquo;s worth noting that for production deployments using absolute paths is always preferable. See <a href=/#connectconfigs_plugin.path>plugin.path</a> for a detailed description of how to set this config.</p><p>Edit the <code>config/connect-standalone.properties</code> file, add or change the <code>plugin.path</code> configuration property match the following, and save the file:</p><pre><code>$ echo &quot;plugin.path=libs/connect-file-4.0.0.jar&quot; &gt;&gt; config/connect-standalone.properties
</code></pre><p>Then, start by creating some seed data to test with:</p><pre><code>$ echo -e &quot;foo
bar&quot; &gt; test.txt
</code></pre><p>Or on Windows:</p><pre><code>$ echo foo &gt; test.txt
$ echo bar &gt;&gt; test.txt
</code></pre><p>Next, we&rsquo;ll start two connectors running in <em>standalone</em> mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p><pre><code>$ bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
</code></pre><p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file.</p><p>During startup you&rsquo;ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from <code>test.txt</code> and producing them to the topic <code>connect-test</code>, and the sink connector should start reading messages from the topic <code>connect-test</code> and write them to the file <code>test.sink.txt</code>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p><pre><code>$ more test.sink.txt
foo
bar
</code></pre><p>Note that the data is being stored in the Kafka topic <code>connect-test</code>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p><pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;foo&quot;}
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;bar&quot;}
…
</code></pre><p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p><pre><code>$ echo &quot;Another line&quot; &gt;&gt; test.txt
</code></pre><p>You should see the line appear in the console consumer output and in the sink file.</p><h2 id=step-7-process-your-events-with-kafka-streams>Step 7: Process your events with Kafka Streams<a class=td-heading-self-link href=#step-7-process-your-events-with-kafka-streams aria-label="Heading self-link"></a></h2><p>Once your data is stored in Kafka as events, you can process the data with the <a href=/streams>Kafka Streams</a> client library for Java/Scala. It allows you to implement mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka topics. Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&rsquo;s server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, and distributed. The library supports exactly-once processing, stateful operations and aggregations, windowing, joins, processing based on event-time, and much more.</p><p>To give you a first taste, here&rsquo;s how one would implement the popular <code>WordCount</code> algorithm:</p><pre><code>KStream&lt;String, String&gt; textLines = builder.stream(&quot;quickstart-events&quot;);

KTable&lt;String, Long&gt; wordCounts = textLines
            .flatMapValues(line -&gt; Arrays.asList(line.toLowerCase().split(&quot; &quot;)))
            .groupBy((keyIgnored, word) -&gt; word)
            .count();

wordCounts.toStream().to(&quot;output-topic&quot;, Produced.with(Serdes.String(), Serdes.Long()));
</code></pre><p>The <a href=/streams/quickstart>Kafka Streams demo</a> and the <a href=/40/streams/tutorial/>app development tutorial</a> demonstrate how to code and run such a streaming application from start to finish.</p><h2 id=step-8-terminate-the-kafka-environment>Step 8: Terminate the Kafka environment<a class=td-heading-self-link href=#step-8-terminate-the-kafka-environment aria-label="Heading self-link"></a></h2><p>Now that you reached the end of the quickstart, feel free to tear down the Kafka environment—or continue playing around.</p><ol><li>Stop the producer and consumer clients with <code>Ctrl-C</code>, if you haven&rsquo;t done so already.</li><li>Stop the Kafka broker with <code>Ctrl-C</code>.</li></ol><p>If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:</p><pre><code>$ rm -rf /tmp/kafka-logs /tmp/kraft-combined-logs
</code></pre><h2 id=congratulations>Congratulations!<a class=td-heading-self-link href=#congratulations aria-label="Heading self-link"></a></h2><p>You have successfully finished the Apache Kafka quickstart.</p><p>To learn more, we suggest the following next steps:</p><ul><li>Read through the brief <a href=/intro>Introduction</a> to learn how Kafka works at a high level, its main concepts, and how it compares to other technologies. To understand Kafka in more detail, head over to the <a href=/>Documentation</a>.</li><li>Browse through the <a href=/powered-by>Use Cases</a> to learn how other users in our world-wide community are getting value out of Kafka.</li><li>Join a <a href=/events>local Kafka meetup group</a> and <a href=https://kafka-summit.org/past-events/>watch talks from Kafka Summit</a>, the main conference of the Kafka community.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a0632c888d774a65bfc52087a226533b>4 - Ecosystem</h1><p>There are a plethora of tools that integrate with Kafka outside the main distribution. The <a href=https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem>ecosystem page</a> lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-94bef8fa786f550271be06febb7d60a8>5 - Upgrading</h1><h2 id=upgrading-to-400>Upgrading to 4.0.0<a class=td-heading-self-link href=#upgrading-to-400 aria-label="Heading self-link"></a></h2><h3 id=upgrading-clients-to-400>Upgrading Clients to 4.0.0<a class=td-heading-self-link href=#upgrading-clients-to-400 aria-label="Heading self-link"></a></h3><p><strong>For a rolling upgrade:</strong></p><ol><li>Upgrade the clients one at a time: shut down the client, update the code, and restart it.</li><li>Clients (including Streams and Connect) must be on version 2.1 or higher before upgrading to 4.0. Many deprecated APIs were removed in Kafka 4.0. For more information about the compatibility, please refer to the <a href=/40/compatibility.html>compatibility matrix</a> or <a href=https://cwiki.apache.org/confluence/x/y4kgF>KIP-1124</a>.</li></ol><h3 id=upgrading-servers-to-400-from-any-version-33x-through-39x>Upgrading Servers to 4.0.0 from any version 3.3.x through 3.9.x<a class=td-heading-self-link href=#upgrading-servers-to-400-from-any-version-33x-through-39x aria-label="Heading self-link"></a></h3><p>Note: Apache Kafka 4.0 only supports KRaft mode - ZooKeeper mode has been removed. As such, <strong>broker upgrades to 4.0.0 (and higher) require KRaft mode and the software and metadata versions must be at least 3.3.x</strong> (the first version when KRaft mode was deemed production ready). For clusters in KRaft mode with versions older than 3.3.x, we recommend upgrading to 3.9.x before upgrading to 4.0.x. Clusters in ZooKeeper mode have to be <a href=/40/documentation.html#kraft_zk_migration>migrated to KRaft mode</a> before they can be upgraded to 4.0.x.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, finalize the upgrade by running <code>bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --release-version 4.0</code></li><li>Note that cluster metadata downgrade is not supported in this version since it has metadata changes. Every <a href=https://github.com/apache/kafka/blob/trunk/server-common/src/main/java/org/apache/kafka/server/common/MetadataVersion.java>MetadataVersion</a> has a boolean parameter that indicates if there are metadata changes (i.e. <code>IBP_4_0_IV1(23, "4.0", "IV1", true)</code> means this version has metadata changes). Given your current and target versions, a downgrade is only possible if there are no metadata changes in the versions between.</li></ol><h3 id=notable-changes-in-400>Notable changes in 4.0.0<a class=td-heading-self-link href=#notable-changes-in-400 aria-label="Heading self-link"></a></h3><ul><li>Old protocol API versions have been removed. Users should ensure brokers are version 2.1 or higher before upgrading Java clients (including Connect and Kafka Streams which use the clients internally) to 4.0. Similarly, users should ensure their Java clients (including Connect and Kafka Streams) version is 2.1 or higher before upgrading brokers to 4.0. Finally, care also needs to be taken when it comes to kafka clients that are not part of Apache Kafka, please see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-896%3A+Remove+old+client+protocol+API+versions+in+Kafka+4.0>KIP-896</a> for the details.</li><li>Apache Kafka 4.0 only supports KRaft mode - ZooKeeper mode has been removed. About version upgrade, check <a href=/40/documentation.html#upgrade_4_0_0>Upgrading to 4.0.0 from any version 3.3.x through 3.9.x</a> for more info.</li><li>Apache Kafka 4.0 ships with a brand-new group coordinator implementation (See <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217387038#KIP848:TheNextGenerationoftheConsumerRebalanceProtocol-GroupCoordinator">here</a>). Functionally speaking, it implements all the same APIs. There are reasonable defaults, but the behavior of the new group coordinator can be tuned by setting the configurations with prefix <code>group.coordinator</code>.</li><li>The Next Generation of the Consumer Rebalance Protocol (<a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol>KIP-848</a>) is now Generally Available (GA) in Apache Kafka 4.0. The protocol is automatically enabled on the server when the upgrade to 4.0 is finalized. Note that once the new protocol is used by consumer groups, the cluster can only downgrade to version 3.4.1 or newer. Check <a href=/40/documentation.html#consumer_rebalance_protocol>here</a> for details.</li><li>Transactions Server Side Defense (<a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-890%3A+Transactions+Server-Side+Defense>KIP-890</a>) brings a strengthened transactional protocol to Apache Kafka 4.0. The new and improved transactional protocol is enabled when the upgrade to 4.0 is finalized. When using 4.0 producer clients, the producer epoch is bumped on every transaction to ensure every transaction includes the intended messages and duplicates are not written as part of the next transaction. Downgrading the protocol is safe. For more information check <a href=/40/documentation.html#transaction_protocol>here</a></li><li>Eligible Leader Replicas (<a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-966%3A+Eligible+Leader+Replicas>KIP-966 Part 1</a>) enhances the replication protocol for the Apache Kafka 4.0. Now the KRaft controller keeps track of the data partition replicas that are not included in ISR but are safe to be elected as leader without data loss. Such replicas are stored in the partition metadata as the <code>Eligible Leader Replicas</code>(ELR). For more information check <a href=/40/documentation.html#eligible_leader_replicas>here</a></li><li>Since Apache Kafka 4.0.0, we have added a system property (&ldquo;org.apache.kafka.sasl.oauthbearer.allowed.urls&rdquo;) to set the allowed URLs as SASL OAUTHBEARER token or jwks endpoints. By default, the value is an empty list. Users should explicitly set the allowed list if necessary.</li><li>A number of deprecated classes, methods, configurations and tools have been removed.<ul><li><strong>Common</strong><ul><li>The <code>metrics.jmx.blacklist</code> and <code>metrics.jmx.whitelist</code> configurations were removed from the <code>org.apache.kafka.common.metrics.JmxReporter</code> Please use <code>metrics.jmx.exclude</code> and <code>metrics.jmx.include</code> respectively instead.</li><li>The <code>auto.include.jmx.reporter</code> configuration was removed. The <code>metric.reporters</code> configuration is now set to <code>org.apache.kafka.common.metrics.JmxReporter</code> by default.</li><li>The constructor <code>org.apache.kafka.common.metrics.JmxReporter</code> with string argument was removed. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-606%3A+Add+Metadata+Context+to+MetricsReporter>KIP-606</a> for details.</li><li>The <code>bufferpool-wait-time-total</code>, <code>io-waittime-total</code>, and <code>iotime-total</code> metrics were removed. Please use <code>bufferpool-wait-time-ns-total</code>, <code>io-wait-time-ns-total</code>, and <code>io-time-ns-total</code> metrics as replacements, respectively.</li><li>The <code>kafka.common.requests.DescribeLogDirsResponse.LogDirInfo</code> class was removed. Please use the <code>kafka.clients.admin.DescribeLogDirsResult.descriptions()</code> class and <code>kafka.clients.admin.DescribeLogDirsResult.allDescriptions()</code>instead.</li><li>The <code>kafka.common.requests.DescribeLogDirsResponse.ReplicaInfo</code> class was removed. Please use the <code>kafka.clients.admin.DescribeLogDirsResult.descriptions()</code> class and <code>kafka.clients.admin.DescribeLogDirsResult.allDescriptions()</code>instead.</li><li>The <code>org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler</code> class was removed. Please use the <code>org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginCallbackHandler</code> class instead.</li><li>The <code>org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler</code> class was removed. Please use the <code>org.apache.kafka.common.security.oauthbearer.OAuthBearerValidatorCallbackHandler</code> class instead.</li><li>The <code>org.apache.kafka.common.errors.NotLeaderForPartitionException</code> class was removed. The <code>org.apache.kafka.common.errors.NotLeaderOrFollowerException</code> is returned if a request could not be processed because the broker is not the leader or follower for a topic partition.</li><li>The <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code> and <code>org.apache.kafka.clients.producer.UniformStickyPartitioner</code> class was removed.</li><li>The <code>log.message.format.version</code> and <code>message.format.version</code> configs were removed.</li><li>The function <code>onNewBatch</code> in <code>org.apache.kafka.clients.producer.Partitioner</code> class was removed.</li><li>The default properties files for KRaft mode are no longer stored in the separate <code>config/kraft</code> directory since Zookeeper has been removed. These files have been consolidated with other configuration files. Now all configuration files are in <code>config</code> directory.</li><li>The valid format for <code>--bootstrap-server</code> only supports comma-separated value, such as <code>host1:port1,host2:port2,...</code>. Providing other formats, like space-separated bootstrap servers (e.g., <code>host1:port1 host2:port2 host3:port3</code>), will result in an exception, even though this was allowed in Apache Kafka versions prior to 4.0.</li></ul></li><li><strong>Broker</strong><ul><li>The <code>delegation.token.master.key</code> configuration was removed. Please use <code>delegation.token.secret.key</code> instead.</li><li>The <code>offsets.commit.required.acks</code> configuration was removed. See <a href=https://cwiki.apache.org/confluence/x/9YobEg>KIP-1041</a> for details.</li><li>The <code>log.message.timestamp.difference.max.ms</code> configuration was removed. Please use <code>log.message.timestamp.before.max.ms</code> and <code>log.message.timestamp.after.max.ms</code> instead. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-937%3A+Improve+Message+Timestamp+Validation>KIP-937</a> for details.</li><li>The <code>remote.log.manager.copier.thread.pool.size</code> configuration default value was changed to 10 from -1. Values of -1 are no longer valid. A minimum of 1 or higher is valid. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li><li>The <code>remote.log.manager.expiration.thread.pool.size</code> configuration default value was changed to 10 from -1. Values of -1 are no longer valid. A minimum of 1 or higher is valid. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li><li>The <code>remote.log.manager.thread.pool.size</code> configuration default value was changed to 2 from 10. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li><li>The minimum <code>segment.bytes/log.segment.bytes</code> has changed from 14 bytes to 1MB. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li></ul></li><li><strong>MirrorMaker</strong><ul><li>The original MirrorMaker (MM1) and related classes were removed. Please use the Connect-based MirrorMaker (MM2), as described in the <a href=/40/#georeplication>Geo-Replication section.</a>.</li><li>The <code>use.incremental.alter.configs</code> configuration was removed from <code>MirrorSourceConnector</code>. The modified behavior is identical to the previous <code>required</code> configuration, therefore users should ensure that brokers in the target cluster are at least running 2.3.0.</li><li>The <code>add.source.alias.to.metrics</code> configuration was removed from <code>MirrorSourceConnector</code>. The source cluster alias is now always added to the metrics.</li><li>The <code>config.properties.blacklist</code> was removed from the <code>org.apache.kafka.connect.mirror.MirrorSourceConfig</code> Please use <code>config.properties.exclude</code> instead.</li><li>The <code>topics.blacklist</code> was removed from the <code>org.apache.kafka.connect.mirror.MirrorSourceConfig</code> Please use <code>topics.exclude</code> instead.</li><li>The <code>groups.blacklist</code> was removed from the <code>org.apache.kafka.connect.mirror.MirrorSourceConfig</code> Please use <code>groups.exclude</code> instead.</li></ul></li><li><strong>Tools</strong><ul><li>The <code>kafka.common.MessageReader</code> class was removed. Please use the <a href=/40/javadoc/org/apache/kafka/tools/api/RecordReader.html><code>org.apache.kafka.tools.api.RecordReader</code></a> interface to build custom readers for the <code>kafka-console-producer</code> tool.</li><li>The <code>kafka.tools.DefaultMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.DefaultMessageFormatter</code> class instead.</li><li>The <code>kafka.tools.LoggingMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.LoggingMessageFormatter</code> class instead.</li><li>The <code>kafka.tools.NoOpMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.NoOpMessageFormatter</code> class instead.</li><li>The <code>--whitelist</code> option was removed from the <code>kafka-console-consumer</code> command line tool. Please use <code>--include</code> instead.</li><li>Redirections from the old tools packages have been removed: <code>kafka.admin.FeatureCommand</code>, <code>kafka.tools.ClusterTool</code>, <code>kafka.tools.EndToEndLatency</code>, <code>kafka.tools.StateChangeLogMerger</code>, <code>kafka.tools.StreamsResetter</code>, <code>kafka.tools.JmxTool</code>.</li><li>The <code>--authorizer</code>, <code>--authorizer-properties</code>, and <code>--zk-tls-config-file</code> options were removed from the <code>kafka-acls</code> command line tool. Please use <code>--bootstrap-server</code> or <code>--bootstrap-controller</code> instead.</li><li>The <code>kafka.serializer.Decoder</code> trait was removed, please use the <a href=/40/javadoc/org/apache/kafka/tools/api/Decoder.html><code>org.apache.kafka.tools.api.Decoder</code></a> interface to build custom decoders for the <code>kafka-dump-log</code> tool.</li><li>The <code>kafka.coordinator.group.OffsetsMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.OffsetsMessageFormatter</code> class instead.</li><li>The <code>kafka.coordinator.group.GroupMetadataMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.GroupMetadataMessageFormatter</code> class instead.</li><li>The <code>kafka.coordinator.transaction.TransactionLogMessageFormatter</code> class was removed. Please use the <code>org.apache.kafka.tools.consumer.TransactionLogMessageFormatter</code> class instead.</li><li>The <code>--topic-white-list</code> option was removed from the <code>kafka-replica-verification</code> command line tool. Please use <code>--topics-include</code> instead.</li><li>The <code>--broker-list</code> option was removed from the <code>kafka-verifiable-consumer</code> command line tool. Please use <code>--bootstrap-server</code> instead.</li><li>kafka-configs.sh now uses incrementalAlterConfigs API to alter broker configurations instead of the deprecated alterConfigs API, and it will fall directly if the broker doesn&rsquo;t support incrementalAlterConfigs API, which means the broker version is prior to 2.3.x. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1011%3A+Use+incrementalAlterConfigs+when+updating+broker+configs+by+kafka-configs.sh>KIP-1011</a> for more details.</li><li>The <code>kafka.admin.ZkSecurityMigrator</code> tool was removed.</li></ul></li><li><strong>Connect</strong><ul><li>The <code>whitelist</code> and <code>blacklist</code> configurations were removed from the <code>org.apache.kafka.connect.transforms.ReplaceField</code> transformation. Please use <code>include</code> and <code>exclude</code> respectively instead.</li><li>The <code>onPartitionsRevoked(Collection&lt;TopicPartition>)</code> and <code>onPartitionsAssigned(Collection&lt;TopicPartition>)</code> methods were removed from <code>SinkTask</code>.</li><li>The <code>commitRecord(SourceRecord)</code> method was removed from <code>SourceTask</code>.</li></ul></li><li><strong>Consumer</strong><ul><li>The <code>poll(long)</code> method was removed from the consumer. Please use <code>poll(Duration)</code> instead. Note that there is a difference in behavior between the two methods. The <code>poll(Duration)</code> method does not block beyond the timeout awaiting partition assignment, whereas the earlier <code>poll(long)</code> method used to wait beyond the timeout.</li><li>The <code>committed(TopicPartition)</code> and <code>committed(TopicPartition, Duration)</code> methods were removed from the consumer. Please use <code>committed(Set&amp;ltTopicPartition;>)</code> and <code>committed(Set&amp;ltTopicPartition;>, Duration)</code> instead.</li><li>The <code>setException(KafkaException)</code> method was removed from the <code>org.apache.kafka.clients.consumer.MockConsumer</code>. Please use <code>setPollException(KafkaException)</code> instead.</li></ul></li><li><strong>Producer</strong><ul><li>The <code>enable.idempotence</code> configuration will no longer automatically fall back when the <code>max.in.flight.requests.per.connection</code> value exceeds 5.</li><li>The deprecated <code>sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata>, String)</code> method has been removed from the Producer API.</li><li>The default <code>linger.ms</code> changed from 0 to 5 in Apache Kafka 4.0 as the efficiency gains from larger batches typically result in similar or lower producer latency despite the increased linger.</li></ul></li><li><strong>Admin client</strong><ul><li>The <code>alterConfigs</code> method was removed from the <code>org.apache.kafka.clients.admin.Admin</code>. Please use <code>incrementalAlterConfigs</code> instead.</li><li>The <code>org.apache.kafka.common.ConsumerGroupState</code> enumeration and related methods have been deprecated. Please use <code>GroupState</code> instead which applies to all types of group.</li><li>The <code>Admin.describeConsumerGroups</code> method used to return a <code>ConsumerGroupDescription</code> in state <code>DEAD</code> if the group ID was not found. In Apache Kafka 4.0, the <code>GroupIdNotFoundException</code> is thrown instead as part of the support for new types of group.</li><li>The <code>org.apache.kafka.clients.admin.DeleteTopicsResult.values()</code> method was removed. Please use <code>org.apache.kafka.clients.admin.DeleteTopicsResult.topicNameValues()</code> instead.</li><li>The <code>org.apache.kafka.clients.admin.TopicListing.TopicListing(String, boolean)</code> method was removed. Please use <code>org.apache.kafka.clients.admin.TopicListing.TopicListing(String, Uuid, boolean)</code> instead.</li><li>The <code>org.apache.kafka.clients.admin.ListConsumerGroupOffsetsOptions.topicPartitions(List&lt;TopicPartition>)</code> method was removed. Please use <code>org.apache.kafka.clients.admin.Admin.listConsumerGroupOffsets(Map&lt;String, ListConsumerGroupOffsetsSpec>, ListConsumerGroupOffsetsOptions)</code> instead.</li><li>The deprecated <code>dryRun</code> methods were removed from the <code>org.apache.kafka.clients.admin.UpdateFeaturesOptions</code>. Please use <code>validateOnly</code> instead.</li><li>The constructor <code>org.apache.kafka.clients.admin.FeatureUpdate</code> with short and boolean arguments was removed. Please use the constructor that accepts short and the specified UpgradeType enum instead.</li><li>The <code>allowDowngrade</code> method was removed from the <code>org.apache.kafka.clients.admin.FeatureUpdate</code>.</li><li>The <code>org.apache.kafka.clients.admin.DescribeTopicsResult.DescribeTopicsResult(Map&lt;String, KafkaFuture&lt;TopicDescription>>)</code> method was removed. Please use <code>org.apache.kafka.clients.admin.DescribeTopicsResult.DescribeTopicsResult(Map&lt;Uuid, KafkaFuture&lt;TopicDescription>>, Map&lt;String, KafkaFuture&lt;TopicDescription>>)</code> instead.</li><li>The <code>values()</code> method was removed from the <code>org.apache.kafka.clients.admin.DescribeTopicsResult</code>. Please use <code>topicNameValues()</code> instead.</li><li>The <code>all()</code> method was removed from the <code>org.apache.kafka.clients.admin.DescribeTopicsResult</code>. Please use <code>allTopicNames()</code> instead.</li></ul></li><li><strong>Kafka Streams</strong><ul><li>All public API, deprecated in Apache Kafka 3.6 or an earlier release, have been removed, with the exception of <code>JoinWindows.of()</code> and <code>JoinWindows#grace()</code>. See <a href=https://issues.apache.org/jira/browse/KAFKA-17531>KAFKA-17531</a> for details.</li><li>The most important changes are highlighted in the <a href=/40/streams/upgrade-guide/#streams_api_changes_400>Kafka Streams upgrade guide</a>.</li><li>For a full list of changes, see <a href=https://issues.apache.org/jira/browse/KAFKA-12822>KAFKA-12822</a>.</li><li>If you are using <code>KStream#transformValues()</code> which was removed with Apache Kafka 4.0.0 release, and you need to rewrite your program to use <code>KStreams#processValues()</code> instead, pay close attention to the <a href=/40/streams/developer-guide/dsl-api/#transformers-removal-and-migration-to-processors>migration guide</a>.</li></ul></li></ul></li><li>Other changes:<ul><li>The minimum Java version required by clients and Kafka Streams applications has been increased from Java 8 to Java 11 while brokers, connect and tools now require Java 17. See <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181308223">KIP-750</a> and <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=284789510">KIP-1013</a> for more details.</li><li>Java 23 support has been added in Apache Kafka 4.0</li><li>Scala 2.12 support has been removed in Apache Kafka 4.0 See <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181308218">KIP-751</a> for more details</li><li>Logging framework has been migrated from Log4j to Log4j2. Users can use the log4j-transform-cli tool to automatically convert their existing Log4j configuration files to Log4j2 format. See <a href=https://logging.staged.apache.org/log4j/transform/cli.html#log4j-transform-cli>log4j-transform-cli</a> for more details. Log4j2 provides limited compatibility for Log4j configurations. See <a href=https://logging.apache.org/log4j/2.x/migrate-from-log4j1.html#ConfigurationCompatibility>Use Log4j 1 to Log4j 2 bridge</a> for more information,</li><li>KafkaLog4jAppender has been removed, users should migrate to the log4j2 appender See <a href=https://logging.apache.org/log4j/2.x/manual/appenders.html#KafkaAppender>KafkaAppender</a> for more details</li><li>The <code>--delete-config</code> option in the <code>kafka-topics</code> command line tool has been deprecated.</li><li>For implementors of RemoteLogMetadataManager (RLMM), a new API <code>nextSegmentWithTxnIndex</code> is introduced in RLMM to allow the implementation to return the next segment metadata with a transaction index. This API is used when the consumers are enabled with isolation level as READ_COMMITTED. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1058:+Txn+consumer+exerts+pressure+on+remote+storage+when+collecting+aborted+transactions>KIP-1058</a> for more details.</li><li>The criteria for identifying internal topics in ReplicationPolicy and DefaultReplicationPolicy have been updated to enable the replication of topics that appear to be internal but aren&rsquo;t truly internal to Kafka and Mirror Maker 2. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1074%3A+Allow+the+replication+of+user+internal+topics>KIP-1074</a> for more details.</li><li>KIP-714 is now enabled for Kafka Streams via <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1076%3A++Metrics+for+client+applications+KIP-714+extension>KIP-1076</a>. This allows to not only collect the metric of the internally used clients of a Kafka Streams appliction via a broker-side plugin, but also to collect the <a href=/40/#kafka_streams_monitoring>metrics</a> of the Kafka Streams runtime itself.</li><li>The default value of &rsquo;num.recovery.threads.per.data.dir&rsquo; has been changed from 1 to 2. The impact of this is faster recovery post unclean shutdown at the expense of extra IO cycles. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li><li>The default value of &lsquo;message.timestamp.after.max.ms&rsquo; has been changed from Long.Max to 1 hour. The impact of this messages with a timestamp of more than 1 hour in the future will be rejected when message.timestamp.type=CreateTime is set. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1030%3A+Change+constraints+and+default+values+for+various+configurations>KIP-1030</a></li><li>Introduced in KIP-890, the <code>TransactionAbortableException</code> enhances error handling within transactional operations by clearly indicating scenarios where transactions should be aborted due to errors. It is important for applications to properly manage both <code>TimeoutException</code> and <code>TransactionAbortableException</code> when working with transaction producers.<ul><li><strong>TimeoutException:</strong> This exception indicates that a transactional operation has timed out. Given the risk of message duplication that can arise from retrying operations after a timeout (potentially violating exactly-once semantics), applications should treat timeouts as reasons to abort the ongoing transaction.</li><li><strong>TransactionAbortableException:</strong> Specifically introduced to signal errors that should lead to transaction abortion, ensuring this exception is properly handled is critical for maintaining the integrity of transactional processing.</li><li>To ensure seamless operation and compatibility with future Kafka versions, developers are encouraged to update their error-handling logic to treat both exceptions as triggers for aborting transactions. This approach is pivotal for preserving exactly-once semantics.</li><li>See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-890%3A+Transactions+Server-Side+Defense>KIP-890</a> and <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-1050%3A+Consistent+error+handling+for+Transactions>KIP-1050</a> for more details</li></ul></li></ul></li></ul><h2 id=upgrading-to-390-and-older-versions>Upgrading to 3.9.0 and older versions<a class=td-heading-self-link href=#upgrading-to-390-and-older-versions aria-label="Heading self-link"></a></h2><p>See <a href=/39/#upgrade>Upgrading From Previous Versions</a> in the 3.9 documentation.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-454ace89e9084b5fe25ece7be47eddfc>6 - KRaft vs ZooKeeper</h1><h1 id=differences-between-kraft-mode-and-zookeeper-mode>Differences Between KRaft mode and ZooKeeper mode<a class=td-heading-self-link href=#differences-between-kraft-mode-and-zookeeper-mode aria-label="Heading self-link"></a></h1><h1 id=removed-zookeeper-features>Removed ZooKeeper Features<a class=td-heading-self-link href=#removed-zookeeper-features aria-label="Heading self-link"></a></h1><p>This section documents differences in behavior between KRaft mode and ZooKeeper mode. Specifically, several configurations, metrics and features have changed or are no longer required in KRaft mode. To migrate an existing cluster from ZooKeeper mode to KRaft mode, please refer to the <a href=/39/documentation.html#kraft_zk_migration>ZooKeeper to KRaft Migration</a> section.</p><h1 id=configurations>Configurations<a class=td-heading-self-link href=#configurations aria-label="Heading self-link"></a></h1><ul><li><p>Removed password encoder-related configurations. These configurations were used in ZooKeeper mode to define the key and backup key for encrypting sensitive data (e.g., passwords), specify the algorithm and key generation method for password encryption (e.g., AES, RSA), and control the key length and encryption strength.</p><ul><li><code>password.encoder.secret</code></li><li><code>password.encoder.old.secret</code></li><li><code>password.encoder.keyfactory.algorithm</code></li><li><code>password.encoder.cipher.algorithm</code></li><li><code>password.encoder.key.length</code></li><li><code>password.encoder.iterations</code></li></ul></li></ul><p>In KRaft mode, Kafka stores sensitive data in records, and the data is not encrypted in Kafka.</p><ul><li>Removed <code>control.plane.listener.name</code>. Kafka relies on ZooKeeper to manage metadata, but some internal operations (e.g., communication between controllers (a.k.a., broker controller) and brokers) still require Kafka’s internal control plane for coordination.</li></ul><p>In KRaft mode, Kafka eliminates its dependency on ZooKeeper, and the control plane functionality is fully integrated into Kafka itself. The process roles are clearly separated: brokers handle data-related requests, while the controllers (a.k.a., quorum controller) manages metadata-related requests. The controllers use the Raft protocol for internal communication, which operates differently from the ZooKeeper model. Use the following parameters to configure the control plane listener:</p><pre><code>* `controller.listener.names`
* `listeners`
* `listener.security.protocol.map`
</code></pre><ul><li><p>Removed graceful broker shutdowns-related configurations. These configurations were used in ZooKeeper mode to define the maximum number of retries and the retry backoff time for controlled shutdowns. It can reduce the risk of unplanned leader changes and data inconsistencies.</p><ul><li><code>controlled.shutdown.max.retries</code></li><li><code>controlled.shutdown.retry.backoff.ms</code></li></ul></li></ul><p>In KRaft mode, Kafka uses the Raft protocol to manage metadata. The broker shutdown process differs from ZooKeeper mode as it is managed by the quorum-based controller. The shutdown process is more reliable and efficient due to automated leader transfers and metadata updates handled by the controller.</p><ul><li><p>Removed the broker id generation-related configurations. These configurations were used in ZooKeeper mode to specify the broker id auto generation and control the broker id generation process.</p><ul><li><code>reserved.broker.max.id</code></li><li><code>broker.id.generation.enable</code></li></ul></li></ul><p>Kafka uses the node id in KRaft mode to identify servers.</p><pre><code>* `node.id`
</code></pre><ul><li><p>Removed broker protocol version-related configurations. These configurations were used in ZooKeeper mode to define communication protocol version between brokers. In KRaft mode, Kafka uses <code>metadata.version</code> to control the feature level of the cluster, which can be managed using <code>bin/kafka-features.sh</code>.</p><ul><li><code>inter.broker.protocol.version</code></li></ul></li><li><p>Removed dynamic configurations which relied on ZooKeeper. In KRaft mode, to change these configurations, you need to restart the broker/controller.</p><ul><li><code>advertised.listeners</code></li></ul></li><li><p>Removed the leader imbalance configuration used only in ZooKeeper. <code>leader.imbalance.per.broker.percentage</code> was used to limit the preferred leader election frequency in ZooKeeper.</p><ul><li><code>leader.imbalance.per.broker.percentage</code></li></ul></li><li><p>Removed ZooKeeper related configurations.</p><ul><li><code>zookeeper.connect</code></li><li><code>zookeeper.session.timeout.ms</code></li><li><code>zookeeper.connection.timeout.ms</code></li><li><code>zookeeper.set.acl</code></li><li><code>zookeeper.max.in.flight.requests</code></li><li><code>zookeeper.ssl.client.enable</code></li><li><code>zookeeper.clientCnxnSocket</code></li><li><code>zookeeper.ssl.keystore.location</code></li><li><code>zookeeper.ssl.keystore.password</code></li><li><code>zookeeper.ssl.keystore.type</code></li><li><code>zookeeper.ssl.truststore.location</code></li><li><code>zookeeper.ssl.truststore.password</code></li><li><code>zookeeper.ssl.truststore.type</code></li><li><code>zookeeper.ssl.protocol</code></li><li><code>zookeeper.ssl.enabled.protocols</code></li><li><code>zookeeper.ssl.cipher.suites</code></li><li><code>zookeeper.ssl.endpoint.identification.algorithm</code></li><li><code>zookeeper.ssl.crl.enable</code></li><li><code>zookeeper.ssl.ocsp.enable</code></li></ul></li></ul><h1 id=dynamic-log-levels>Dynamic Log Levels<a class=td-heading-self-link href=#dynamic-log-levels aria-label="Heading self-link"></a></h1><ul><li><p>The dynamic log levels feature allows you to change the log4j settings of a running broker or controller process without restarting it. The command-line syntax for setting dynamic log levels on brokers has not changed in KRaft mode. Here is an example of setting the log level on a broker:</p><pre><code>./bin/kafka-configs.sh --bootstrap-server localhost:9092 \
--entity-type broker-loggers \
--entity-name 1 \
--alter \
--add-config org.apache.kafka.raft.KafkaNetworkChannel=TRACE
</code></pre></li><li><p>When setting dynamic log levels on the controllers, the <code>--bootstrap-controller</code> flag must be used. Here is an example of setting the log level ona controller:</p><pre><code>./bin/kafka-configs.sh --bootstrap-controller localhost:9093 \
--entity-type broker-loggers \
--entity-name 1 \
--alter \
--add-config org.apache.kafka.raft.KafkaNetworkChannel=TRACE
</code></pre></li></ul><p>Note that the entity-type must be specified as <code>broker-loggers</code>, even though we are changing a controller&rsquo;s log level rather than a broker&rsquo;s log level.</p><ul><li>When changing the log level of a combined node, which has both broker and controller roles, either &ndash;bootstrap-servers or &ndash;bootstrap-controllers may be used. Combined nodes have only a single set of log levels; there are not different log levels for the broker and controller parts of the process.</li></ul><h1 id=dynamic-controller-configurations>Dynamic Controller Configurations<a class=td-heading-self-link href=#dynamic-controller-configurations aria-label="Heading self-link"></a></h1><ul><li><p>Some Kafka configurations can be changed dynamically, without restarting the process. The command-line syntax for setting dynamic log levels on brokers has not changed in KRaft mode. Here is an example of setting the number of IO threads on a broker:</p><pre><code>./bin/kafka-configs.sh --bootstrap-server localhost:9092 \
--entity-type brokers \
--entity-name 1 \
--alter \
--add-config num.io.threads=5
</code></pre></li><li><p>Controllers will apply all applicable cluster-level dynamic configurations. For example, the following command-line will change the <code>max.connections</code> setting on all of the brokers and all of the controllers in the cluster:</p><pre><code>./bin/kafka-configs.sh --bootstrap-server localhost:9092 \
--entity-type brokers \
--entity-default \
--alter \
--add-config max.connections=10000
</code></pre></li></ul><p>It is not currently possible to apply a dynamic configuration on only a single controller.</p><h1 id=metrics>Metrics<a class=td-heading-self-link href=#metrics aria-label="Heading self-link"></a></h1><ul><li><p>Removed the following metrics related to ZooKeeper. <code>ControlPlaneNetworkProcessorAvgIdlePercent</code> is to monitor the average fraction of time the network processors are idle. The other <code>ControlPlaneExpiredConnectionsKilledCount</code> is to monitor the total number of connections disconnected, across all processors.</p><ul><li><code>ControlPlaneNetworkProcessorAvgIdlePercent</code></li><li><code>ControlPlaneExpiredConnectionsKilledCount</code></li></ul></li></ul><p>In KRaft mode, Kafka also provides metrics to monitor the network processors and expired connections. Use the following metrics to monitor the network processors and expired connections:</p><pre><code>* `NetworkProcessorAvgIdlePercent`
* `ExpiredConnectionsKilledCount`
</code></pre><ul><li><p>Removed the metrics which are only used in ZooKeeper mode.</p><ul><li><code>kafka.controller:type=ControllerChannelManager,name=QueueSize</code></li><li><code>kafka.controller:type=ControllerChannelManager,name=RequestRateAndQueueTimeMs</code></li><li><code>kafka.controller:type=ControllerEventManager,name=EventQueueSize</code></li><li><code>kafka.controller:type=ControllerEventManager,name=EventQueueTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=AutoLeaderBalanceRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=ControlledShutdownRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=ControllerChangeRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=ControllerShutdownRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=IdleRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=IsrChangeRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=LeaderAndIsrResponseReceivedRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=ListPartitionReassignmentRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=LogDirChangeRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=ManualLeaderBalanceRateAndTimeMs</code></li><li><code>kafka.controller:type=KafkaController,name=MigratingZkBrokerCount</code></li><li><code>kafka.controller:type=ControllerStats,name=PartitionReassignmentRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=TopicChangeRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=TopicDeletionRateAndTimeMs</code></li><li><code>kafka.controller:type=KafkaController,name=TopicsIneligibleToDeleteCount</code></li><li><code>kafka.controller:type=ControllerStats,name=TopicUncleanLeaderElectionEnableRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=UncleanLeaderElectionEnableRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec</code></li><li><code>kafka.controller:type=ControllerStats,name=UpdateFeaturesRateAndTimeMs</code></li><li><code>kafka.controller:type=ControllerStats,name=UpdateMetadataResponseReceivedRateAndTimeMs</code></li><li><code>kafka.controller:type=KafkaController,name=ActiveBrokerCount</code></li><li><code>kafka.controller:type=KafkaController,name=ActiveControllerCount</code></li><li><code>kafka.controller:type=KafkaController,name=ControllerState</code></li><li><code>kafka.controller:type=KafkaController,name=FencedBrokerCount</code></li><li><code>kafka.controller:type=KafkaController,name=GlobalPartitionCount</code></li><li><code>kafka.controller:type=KafkaController,name=GlobalTopicCount</code></li><li><code>kafka.controller:type=KafkaController,name=OfflinePartitionsCount</code></li><li><code>kafka.controller:type=KafkaController,name=PreferredReplicaImbalanceCount</code></li><li><code>kafka.controller:type=KafkaController,name=ReplicasIneligibleToDeleteCount</code></li><li><code>kafka.controller:type=KafkaController,name=ReplicasToDeleteCount</code></li><li><code>kafka.controller:type=KafkaController,name=TopicsToDeleteCount</code></li><li><code>kafka.controller:type=KafkaController,name=ZkMigrationState</code></li><li><code>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=ElectLeader</code></li><li><code>kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=topic</code></li><li><code>kafka.server:type=DelayedOperationPurgatory,name=NumDelayedOperations,delayedOperation=ElectLeader</code></li><li><code>kafka.server:type=DelayedOperationPurgatory,name=NumDelayedOperations,delayedOperation=topic</code></li><li><code>kafka.server:type=SessionExpireListener,name=SessionState</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperAuthFailuresPerSec</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperDisconnectsPerSec</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperExpiresPerSec</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperReadOnlyConnectsPerSec</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperSaslAuthenticationsPerSec</code></li><li><code>kafka.server:type=SessionExpireListener,name=ZooKeeperSyncConnectsPerSec</code></li><li><code>kafka.server:type=ZooKeeperClientMetrics,name=ZooKeeperRequestLatencyMs</code></li></ul></li></ul><h1 id=behavioral-change-reference>Behavioral Change Reference<a class=td-heading-self-link href=#behavioral-change-reference aria-label="Heading self-link"></a></h1><p>This document catalogs the functional and operational differences between ZooKeeper mode and KRaft mode.</p><ul><li><strong>Configuration Value Size Limitation</strong> : KRaft mode restricts configuration values to a maximum size of <code>Short.MAX_VALUE</code>, which prevents using the append operation to create larger configuration values.</li><li><strong>Policy Class Deployment</strong> : In KRaft mode, the <code>CreateTopicPolicy</code> and <code>AlterConfigPolicy</code> plugins run on the controller instead of the broker. This requires users to deploy the policy class JAR files on the controller and configure the parameters (<code>create.topic.policy.class.name</code> and <code>alter.config.policy.class.name</code>) on the controller.</li></ul><p>Note: If migrating from ZooKeeper mode, ensure policy JARs are moved from brokers to controllers.</p><ul><li><strong>Custom implementations of<code>KafkaPrincipalBuilder</code></strong>: In KRaft mode, custom implementations of <code>KafkaPrincipalBuilder</code> must also implement <code>KafkaPrincipalSerde</code>; otherwise brokers will not be able to forward requests to the controller.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd44e44b3bd848d14840f2f3c80dcfb3>7 - Compatibility</h1><h1 id=compatibility>Compatibility<a class=td-heading-self-link href=#compatibility aria-label="Heading self-link"></a></h1><p>With the release of Kafka 4.0, significant changes have been introduced that impact compatibility across various components. To assist users in planning upgrades and ensuring seamless interoperability, a comprehensive compatibility matrix has been prepared.</p><h1 id=jdk-compatibility-across-kafka-versions>JDK Compatibility Across Kafka Versions<a class=td-heading-self-link href=#jdk-compatibility-across-kafka-versions aria-label="Heading self-link"></a></h1><table><thead><tr><th>Module</th><th>Kafka Version</th><th>Java 11</th><th>Java 17</th><th>Java 23</th></tr></thead><tbody><tr><td>Clients</td><td>4.0.0</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>Streams</td><td>4.0.0</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>Connect</td><td>4.0.0</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>Server</td><td>4.0.0</td><td>❌</td><td>✅</td><td>✅</td></tr></tbody></table><p><strong>Note: Java 8 is removed in Kafka 4.0 and is no longer supported.</strong></p><h1 id=server-compatibility>Server Compatibility<a class=td-heading-self-link href=#server-compatibility aria-label="Heading self-link"></a></h1><table><thead><tr><th>KRaft Cluster Version</th><th>Compatibility 4.0 Server (dynamic voter)</th><th>Compatibility 4.0 Server (static voter)</th></tr></thead><tbody><tr><td>before 3.2.x</td><td>❌</td><td>❌</td></tr><tr><td>3.3.x</td><td>❌</td><td>✅</td></tr><tr><td>3.4.x</td><td>❌</td><td>✅</td></tr><tr><td>3.5.x</td><td>❌</td><td>✅</td></tr><tr><td>3.6.x</td><td>❌</td><td>✅</td></tr><tr><td>3.7.x</td><td>❌</td><td>✅</td></tr><tr><td>3.8.x</td><td>❌</td><td>✅</td></tr><tr><td>3.9.x</td><td>✅</td><td>✅</td></tr><tr><td>4.0.x</td><td>✅</td><td>✅</td></tr></tbody></table><p><strong>Note: Can’t upgrade server from static voter to dynamic voter, see<a href=https://issues.apache.org/jira/browse/KAFKA-16538>KAFKA-16538</a>.</strong></p><h2 id=clientbroker-forward-compatibility>Client/Broker Forward Compatibility<a class=td-heading-self-link href=#clientbroker-forward-compatibility aria-label="Heading self-link"></a></h2><table><thead><tr><th>Kafka Version</th><th>Module</th><th>Compatibility with Kafka 4.0</th><th>Key Differences/Limitations</th></tr></thead><tbody><tr><td>0.x, 1.x, 2.0</td><td>Client</td><td>❌ Not Compatible</td><td>Pre-0.10.x protocols are fully removed in Kafka 4.0 (<a href=https://cwiki.apache.org/confluence/x/K5sODg>KIP-896</a>).</td></tr><tr><td>Streams</td><td>❌ Not Compatible</td><td>Pre-0.10.x protocols are fully removed in Kafka 4.0 (<a href=https://cwiki.apache.org/confluence/x/K5sODg>KIP-896</a>).</td><td></td></tr><tr><td>Connect</td><td>❌ Not Compatible</td><td>Pre-0.10.x protocols are fully removed in Kafka 4.0 (<a href=https://cwiki.apache.org/confluence/x/K5sODg>KIP-896</a>).</td><td></td></tr><tr><td>2.1 ~ 2.8</td><td>Client</td><td>⚠️ Partially Compatible</td><td>More details in the <a href=/40/documentation.html#upgrade_400_notable_consumer>Consumer</a>, <a href=/40/documentation.html#upgrade_400_notable_producer>Producer</a>, and <a href=/40/documentation.html#upgrade_400_notable_admin_client>Admin Client</a> section.</td></tr><tr><td>Streams</td><td>⚠️ Limited Compatibility</td><td>More details in the <a href=/40/documentation.html#upgrade_400_notable_kafka_streams>Kafka Streams</a> section.</td><td></td></tr><tr><td>Connect</td><td>⚠️ Limited Compatibility</td><td>More details in the <a href=/40/documentation.html#upgrade_400_notable_connect>Connect</a> section.</td><td></td></tr><tr><td>3.x</td><td>Client</td><td>✅ Fully Compatible</td><td></td></tr><tr><td>Streams</td><td>✅ Fully Compatible</td><td></td><td></td></tr><tr><td>Connect</td><td>✅ Fully Compatible</td><td></td><td></td></tr></tbody></table><p>Note: Starting with Kafka 4.0, the <code>--zookeeper</code> option in AdminClient commands has been removed. Users must use the <code>--bootstrap-server</code> option to interact with the Kafka cluster. This change aligns with the transition to KRaft mode.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e2ace19dfe1cc49207a18f13b8bf083f>8 - Docker</h1><h2 id=jvm-based-apache-kafka-docker-image>JVM Based Apache Kafka Docker Image<a class=td-heading-self-link href=#jvm-based-apache-kafka-docker-image aria-label="Heading self-link"></a></h2><p><a href=https://www.docker.com/>Docker</a> is a popular container runtime. Docker images for the JVM based Apache Kafka can be found on <a href=https://hub.docker.com/r/apache/kafka>Docker Hub</a> and are available from version 3.7.0.</p><p>Docker image can be pulled from Docker Hub using the following command:</p><pre><code>$ docker pull apache/kafka:4.0.0
</code></pre><p>If you want to fetch the latest version of the Docker image use following command:</p><pre><code>$ docker pull apache/kafka:latest
</code></pre><p>To start the Kafka container using this Docker image with default configs and on default port 9092:</p><pre><code>$ docker run -p 9092:9092 apache/kafka:4.0.0
</code></pre><h2 id=graalvm-based-native-apache-kafka-docker-image>GraalVM Based Native Apache Kafka Docker Image<a class=td-heading-self-link href=#graalvm-based-native-apache-kafka-docker-image aria-label="Heading self-link"></a></h2><p>Docker images for the GraalVM Based Native Apache Kafka can be found on <a href=https://hub.docker.com/r/apache/kafka-native>Docker Hub</a> and are available from version 3.8.0.<br>NOTE: This image is experimental and intended for local development and testing purposes only; it is not recommended for production use.</p><p>Docker image can be pulled from Docker Hub using the following command:</p><pre><code>$ docker pull apache/kafka-native:4.0.0
</code></pre><p>If you want to fetch the latest version of the Docker image use following command:</p><pre><code>$ docker pull apache/kafka-native:latest
</code></pre><p>To start the Kafka container using this Docker image with default configs and on default port 9092:</p><pre><code>$ docker run -p 9092:9092 apache/kafka-native:4.0.0
</code></pre><h2 id=usage-guide>Usage guide<a class=td-heading-self-link href=#usage-guide aria-label="Heading self-link"></a></h2><p>Detailed instructions for using the Docker image are mentioned <a href=https://github.com/apache/kafka/blob/trunk/docker/examples/README.md>here</a>.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>