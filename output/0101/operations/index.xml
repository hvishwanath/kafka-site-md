<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Operations on</title><link>https://example.kafka-site-md.dev/0101/operations/</link><description>Recent content in Operations on</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://example.kafka-site-md.dev/0101/operations/index.xml" rel="self" type="application/rss+xml"/><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/0101/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/0101/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/0101/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.retention.hours=168 log.flush.interval.ms=10000 log.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/0101/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/0101/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/0101/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
We do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/0101/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.8, which is the one ZkClient 0.9 uses. ZkClient is the client layer Kafka uses to interact with ZooKeeper.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item></channel></rss>