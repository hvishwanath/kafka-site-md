<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Implementation on</title><link>https://example.kafka-site-md.dev/0101/implementation/</link><description>Recent content in Implementation on</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://example.kafka-site-md.dev/0101/implementation/index.xml" rel="self" type="application/rss+xml"/><item><title>API Design</title><link>https://example.kafka-site-md.dev/0101/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/0101/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/0101/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header, a variable length opaque key byte array and a variable length opaque value byte array. The header contains the following fields:
A CRC32 checksum to detect corruption or truncation. A format version. An attributes identifier A timestamp Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/0101/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/message-format/</guid><description>Message Format /** * 1. 4 byte CRC32 of the message * 2. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes, value is 0 or 1 * 3. 1 byte &amp;quot;attributes&amp;quot; identifier to allow annotations on the message independent of the version * bit 0 ~ 2 : Compression codec. * 0 : no compression * 1 : gzip * 2 : snappy * 3 : lz4 * bit 3 : Timestamp type * 0 : create time * 1 : log append time * bit 4 ~ 7 : reserved * 4.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/0101/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/0101/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item></channel></rss>