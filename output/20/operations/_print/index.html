<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://example.kafka-site-md.dev/20/operations/><link rel=alternate type=application/rss+xml href=https://example.kafka-site-md.dev/20/operations/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Operations | </title><meta name=description content="Apache Kafka"><meta property="og:title" content="Operations"><meta property="og:description" content="Apache Kafka"><meta property="og:type" content="website"><meta property="og:url" content="https://example.kafka-site-md.dev/20/operations/"><meta itemprop=name content="Operations"><meta itemprop=description content="Apache Kafka"><meta name=twitter:card content="summary"><meta name=twitter:title content="Operations"><meta name=twitter:description content="Apache Kafka"><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/41/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/41/>4.1</a></li><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e58a36913f949563db0a14b5eaf8f6a5.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/20/operations/>Return to the regular view of this page</a>.</p></div><h1 class=title>Operations</h1><ul><li>1: <a href=#pg-1120ce5dcf6df97634304e71259cc3da>Basic Kafka Operations</a></li><li>2: <a href=#pg-35318f45f4c5db57d169b76b3606152c>Datacenters</a></li><li>3: <a href=#pg-b7e2ae5eace6446201846ed5e1242d94>Kafka Configuration</a></li><li>4: <a href=#pg-2efa8a0c55fd9c040969c12b7f2b901d>Java Version</a></li><li>5: <a href=#pg-8990fdef2eebf110a9ab90bb5637d9fa>Hardware and OS</a></li><li>6: <a href=#pg-e4a18d4403d4aeed7eb0b312b9e33ae4>Monitoring</a></li><li>7: <a href=#pg-932f586fb8e0eef80921a948e18121cc>ZooKeeper</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-1120ce5dcf6df97634304e71259cc3da>1 - Basic Kafka Operations</h1><div class=lead>Basic Kafka Operations</div><h1 id=basic-kafka-operations>Basic Kafka Operations<a class=td-heading-self-link href=#basic-kafka-operations aria-label="Heading self-link"></a></h1><p>This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the <code>bin/</code> directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.</p><h2 id=adding-and-removing-topics>Adding and removing topics<a class=td-heading-self-link href=#adding-and-removing-topics aria-label="Heading self-link"></a></h2><p>You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic. If topics are auto-created then you may want to tune the default topic configurations used for auto-created topics.</p><p>Topics are added and modified using the topic tool:</p><pre><code>  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name
        --partitions 20 --replication-factor 3 --config x=y
</code></pre><p>The replication factor controls how many servers will replicate each message that is written. If you have a replication factor of 3 then up to 2 servers can fail before you will lose access to your data. We recommend you use a replication factor of 2 or 3 so that you can transparently bounce machines without interrupting data consumption.</p><p>The partition count controls how many logs the topic will be sharded into. There are several impacts of the partition count. First each partition must fit entirely on a single server. So if you have 20 partitions the full data set (and read and write load) will be handled by no more than 20 servers (not counting replicas). Finally the partition count impacts the maximum parallelism of your consumers. This is discussed in greater detail in the concepts section.</p><p>Each sharded partition log is placed into its own folder under the Kafka log directory. The name of such folders consists of the topic name, appended by a dash (-) and the partition id. Since a typical folder name can not be over 255 characters long, there will be a limitation on the length of topic names. We assume the number of partitions will not ever be above 100,000. Therefore, topic names cannot be longer than 249 characters. This leaves just enough room in the folder name for a dash and a potentially 5 digit long partition id.</p><p>The configurations added on the command line override the default settings the server has for things like the length of time data should be retained. The complete set of per-topic configurations is documented here.</p><h2 id=modifying-topics>Modifying topics<a class=td-heading-self-link href=#modifying-topics aria-label="Heading self-link"></a></h2><p>You can change the configuration or partitioning of a topic using the same topic tool.</p><p>To add partitions you can do</p><pre><code>  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name
        --partitions 40
</code></pre><p>Be aware that one use case for partitions is to semantically partition data, and adding partitions doesn&rsquo;t change the partitioning of existing data so this may disturb consumers if they rely on that partition. That is if data is partitioned by <code>hash(key) % number_of_partitions</code> then this partitioning will potentially be shuffled by adding partitions but Kafka will not attempt to automatically redistribute data in any way.</p><p>To add configs:</p><pre><code>  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --add-config x=y
</code></pre><p>To remove a config:</p><pre><code>  &gt; bin/kafka-configs.sh --zookeeper zk_host:port/chroot --entity-type topics --entity-name my_topic_name --alter --delete-config x
</code></pre><p>And finally deleting a topic:</p><pre><code>  &gt; bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name
</code></pre><p>Kafka does not currently support reducing the number of partitions for a topic.</p><p>Instructions for changing the replication factor of a topic can be found here.</p><h2 id=graceful-shutdown>Graceful shutdown<a class=td-heading-self-link href=#graceful-shutdown aria-label="Heading self-link"></a></h2><p>The Kafka cluster will automatically detect any broker shutdown or failure and elect new leaders for the partitions on that machine. This will occur whether a server fails or it is brought down intentionally for maintenance or configuration changes. For the latter cases Kafka supports a more graceful mechanism for stopping a server than just killing it. When a server is stopped gracefully it has two optimizations it will take advantage of:</p><ol><li><p>It will sync all its logs to disk to avoid needing to do any log recovery when it restarts (i.e. validating the checksum for all messages in the tail of the log). Log recovery takes time so this speeds up intentional restarts.</p></li><li><p>It will migrate any partitions the server is the leader for to other replicas prior to shutting down. This will make the leadership transfer faster and minimize the time each partition is unavailable to a few milliseconds.
Syncing the logs will happen automatically whenever the server is stopped other than by a hard kill, but the controlled leadership migration requires using a special setting:</p><pre><code> controlled.shutdown.enable=true
</code></pre></li></ol><p>Note that controlled shutdown will only succeed if <em>all</em> the partitions hosted on the broker have replicas (i.e. the replication factor is greater than 1 <em>and</em> at least one of these replicas is alive). This is generally what you want since shutting down the last replica would make that topic partition unavailable.</p><h2 id=balancing-leadership>Balancing leadership<a class=td-heading-self-link href=#balancing-leadership aria-label="Heading self-link"></a></h2><p>Whenever a broker stops or crashes leadership for that broker&rsquo;s partitions transfers to other replicas. This means that by default when the broker is restarted it will only be a follower for all its partitions, meaning it will not be used for client reads and writes.</p><p>To avoid this imbalance, Kafka has a notion of preferred replicas. If the list of replicas for a partition is 1,5,9 then node 1 is preferred as the leader to either node 5 or 9 because it is earlier in the replica list. You can have the Kafka cluster try to restore leadership to the restored replicas by running the command:</p><pre><code>  &gt; bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot
</code></pre><p>Since running this command can be tedious you can also configure Kafka to do this automatically by setting the following configuration:</p><pre><code>      auto.leader.rebalance.enable=true
</code></pre><h2 id=balancing-replicas-across-racks>Balancing Replicas Across Racks<a class=td-heading-self-link href=#balancing-replicas-across-racks aria-label="Heading self-link"></a></h2><p>The rack awareness feature spreads replicas of the same partition across different racks. This extends the guarantees Kafka provides for broker-failure to cover rack-failure, limiting the risk of data loss should all the brokers on a rack fail at once. The feature can also be applied to other broker groupings such as availability zones in EC2.</p><p>You can specify that a broker belongs to a particular rack by adding a property to the broker config:</p><pre><code>   broker.rack=my-rack-id
</code></pre><p>When a topic is created, modified or replicas are redistributed, the rack constraint will be honoured, ensuring replicas span as many racks as they can (a partition will span min(#racks, replication-factor) different racks).</p><p>The algorithm used to assign replicas to brokers ensures that the number of leaders per broker will be constant, regardless of how brokers are distributed across racks. This ensures balanced throughput.</p><p>However if racks are assigned different numbers of brokers, the assignment of replicas will not be even. Racks with fewer brokers will get more replicas, meaning they will use more storage and put more resources into replication. Hence it is sensible to configure an equal number of brokers per rack.</p><h2 id=mirroring-data-between-clusters>Mirroring data between clusters<a class=td-heading-self-link href=#mirroring-data-between-clusters aria-label="Heading self-link"></a></h2><p>We refer to the process of replicating data <em>between</em> Kafka clusters &ldquo;mirroring&rdquo; to avoid confusion with the replication that happens amongst the nodes in a single cluster. Kafka comes with a tool for mirroring data between Kafka clusters. The tool consumes from a source cluster and produces to a destination cluster. A common use case for this kind of mirroring is to provide a replica in another datacenter. This scenario will be discussed in more detail in the next section.</p><p>You can run many such mirroring processes to increase throughput and for fault-tolerance (if one process dies, the others will take overs the additional load).</p><p>Data will be read from topics in the source cluster and written to a topic with the same name in the destination cluster. In fact the mirror maker is little more than a Kafka consumer and producer hooked together.</p><p>The source and destination clusters are completely independent entities: they can have different numbers of partitions and the offsets will not be the same. For this reason the mirror cluster is not really intended as a fault-tolerance mechanism (as the consumer position will be different); for that we recommend using normal in-cluster replication. The mirror maker process will, however, retain and use the message key for partitioning so order is preserved on a per-key basis.</p><p>Here is an example showing how to mirror a single topic (named <em>my-topic</em>) from an input cluster:</p><pre><code>  &gt; bin/kafka-mirror-maker.sh
        --consumer.config consumer.properties
        --producer.config producer.properties --whitelist my-topic
</code></pre><p>Note that we specify the list of topics with the <code>--whitelist</code> option. This option allows any regular expression using <a href=http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html>Java-style regular expressions</a>. So you could mirror two topics named <em>A</em> and <em>B</em> using <code>--whitelist 'A|B'</code>. Or you could mirror <em>all</em> topics using <code>--whitelist '*'</code>. Make sure to quote any regular expression to ensure the shell doesn&rsquo;t try to expand it as a file path. For convenience we allow the use of &lsquo;,&rsquo; instead of &lsquo;|&rsquo; to specify a list of topics.</p><p>Sometimes it is easier to say what it is that you <em>don&rsquo;t</em> want. Instead of using <code>--whitelist</code> to say what you want to mirror you can use <code>--blacklist</code> to say what to exclude. This also takes a regular expression argument. However, <code>--blacklist</code> is not supported when the new consumer has been enabled (i.e. when <code>bootstrap.servers</code> has been defined in the consumer configuration).</p><p>Combining mirroring with the configuration <code>auto.create.topics.enable=true</code> makes it possible to have a replica cluster that will automatically create and replicate all data in a source cluster even as new topics are added.</p><h2 id=checking-consumer-position>Checking consumer position<a class=td-heading-self-link href=#checking-consumer-position aria-label="Heading self-link"></a></h2><p>Sometimes it&rsquo;s useful to see the position of your consumers. We have a tool that will show the position of all consumers in a consumer group as well as how far behind the end of the log they are. To run this tool on a consumer group named <em>my-group</em> consuming a topic named <em>my-topic</em> would look like this:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

  Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
  my-topic                       0          2               4               2          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       1          2               3               1          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
  my-topic                       2          2               3               1          consumer-2-42c1abd4-e3b2-425d-a8bb-e1ea49b29bb2   /127.0.0.1                     consumer-2
</code></pre><p>This tool also works with ZooKeeper-based consumers:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --describe --group my-group

  Note: This will only show information about consumers that use ZooKeeper (not those using the Java consumer API).

  TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID
  my-topic                       0          2               4               2          my-group_consumer-1
  my-topic                       1          2               3               1          my-group_consumer-1
  my-topic                       2          2               3               1          my-group_consumer-2
</code></pre><h2 id=managing-consumer-groups>Managing Consumer Groups<a class=td-heading-self-link href=#managing-consumer-groups aria-label="Heading self-link"></a></h2><p>With the ConsumerGroupCommand tool, we can list, describe, or delete consumer groups. When using the <a href=http://kafka.apache.org/documentation.html#newconsumerapi>new consumer API</a> (where the broker handles coordination of partition handling and rebalance), the group can be deleted manually, or automatically when the last committed offset for that group expires. Manual deletion works only if the group does not have any active members. For example, to list all consumer groups across all topics:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

  test-consumer-group
</code></pre><p>To view offsets, as mentioned earlier, we &ldquo;describe&rdquo; the consumer group like this:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

  TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                    HOST            CLIENT-ID
  topic3          0          241019          395308          154289          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
  topic2          1          520678          803288          282610          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
  topic3          1          241018          398817          157799          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
  topic1          0          854144          855809          1665            consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
  topic2          0          460537          803290          342753          consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
  topic3          2          243655          398812          155157          consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4
</code></pre><p>There are a number of additional &ldquo;describe&rdquo; options that can be used to provide more detailed information about a consumer group that uses the new consumer API:</p><ul><li><p>--members: This option provides the list of all active members in the consumer group.</p><pre><code>      &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members

  CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS
  consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2
  consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1
  consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3
  consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0
</code></pre></li><li><p>--members &ndash;verbose: On top of the information reported by the &ldquo;&ndash;members&rdquo; options above, this option also provides the partitions assigned to each member.</p><pre><code>      &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members --verbose

  CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS     ASSIGNMENT
  consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2               topic1(0), topic2(0)
  consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1               topic3(2)
  consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3               topic2(1), topic3(0,1)
  consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0               -
</code></pre></li><li><p>--offsets: This is the default describe option and provides the same output as the &ldquo;&ndash;describe&rdquo; option.</p></li><li><p>--state: This option provides useful group-level information.</p><pre><code>      &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --state

  COORDINATOR (ID)          ASSIGNMENT-STRATEGY       STATE                #MEMBERS
  localhost:9092 (0)        range                     Stable               4
</code></pre></li></ul><p>To manually delete one or multiple consumer groups, the &ldquo;&ndash;delete&rdquo; option can be used:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group my-group --group my-other-group

  Note: This will not show information about old Zookeeper-based consumers.
  Deletion of requested consumer groups ('my-group', 'my-other-group') was successful.
</code></pre><p>If you are using the old high-level consumer and storing the group metadata in ZooKeeper (i.e. <code>offsets.storage=zookeeper</code>), pass <code>--zookeeper</code> instead of <code>bootstrap-server</code>:</p><pre><code>  &gt; bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list
</code></pre><h2 id=expanding-your-cluster>Expanding your cluster<a class=td-heading-self-link href=#expanding-your-cluster aria-label="Heading self-link"></a></h2><p>Adding servers to a Kafka cluster is easy, just assign them a unique broker id and start up Kafka on your new servers. However these new servers will not automatically be assigned any data partitions, so unless partitions are moved to them they won&rsquo;t be doing any work until new topics are created. So usually when you add machines to your cluster you will want to migrate some existing data to these machines.</p><p>The process of migrating data is manually initiated but fully automated. Under the covers what happens is that Kafka will add the new server as a follower of the partition it is migrating and allow it to fully replicate the existing data in that partition. When the new server has fully replicated the contents of this partition and joined the in-sync replica one of the existing replicas will delete their partition&rsquo;s data.</p><p>The partition reassignment tool can be used to move partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. The partition reassignment tool does not have the capability to automatically study the data distribution in a Kafka cluster and move partitions around to attain an even load distribution. As such, the admin has to figure out which topics or partitions should be moved around.</p><p>The partition reassignment tool can run in 3 mutually exclusive modes:</p><ul><li>--generate: In this mode, given a list of topics and a list of brokers, the tool generates a candidate reassignment to move all partitions of the specified topics to the new brokers. This option merely provides a convenient way to generate a partition reassignment plan given a list of topics and target brokers.</li><li>--execute: In this mode, the tool kicks off the reassignment of partitions based on the user provided reassignment plan. (using the &ndash;reassignment-json-file option). This can either be a custom reassignment plan hand crafted by the admin or provided by using the &ndash;generate option</li><li>--verify: In this mode, the tool verifies the status of the reassignment for all partitions listed during the last &ndash;execute. The status can be either of successfully completed, failed or in progress</li></ul><h3 id=automatically-migrating-data-to-new-machines>Automatically migrating data to new machines<a class=td-heading-self-link href=#automatically-migrating-data-to-new-machines aria-label="Heading self-link"></a></h3><p>The partition reassignment tool can be used to move some topics off of the current set of brokers to the newly added brokers. This is typically useful while expanding an existing cluster since it is easier to move entire topics to the new set of brokers, than moving one partition at a time. When used to do this, the user should provide a list of topics that should be moved to the new set of brokers and a target list of new brokers. The tool then evenly distributes all partitions for the given list of topics across the new set of brokers. During this move, the replication factor of the topic is kept constant. Effectively the replicas for all partitions for the input list of topics are moved from the old set of brokers to the newly added brokers.</p><p>For instance, the following example will move all partitions for topics foo1,foo2 to the new set of brokers 5,6. At the end of this move, all partitions for topics foo1 and foo2 will <em>only</em> exist on brokers 5,6.</p><p>Since the tool accepts the input list of topics as a json file, you first need to identify the topics you want to move and create the json file as follows:</p><pre><code>  &gt; cat topics-to-move.json
  {&quot;topics&quot;: [{&quot;topic&quot;: &quot;foo1&quot;},
              {&quot;topic&quot;: &quot;foo2&quot;}],
  &quot;version&quot;:1
  }
</code></pre><p>Once the json file is ready, use the partition reassignment tool to generate a candidate assignment:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list &quot;5,6&quot; --generate
  Current partition replica assignment

  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]}]
  }

  Proposed partition reassignment configuration

  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]}]
  }
</code></pre><p>The tool generates a candidate assignment that will move all partitions from topics foo1,foo2 to brokers 5,6. Note, however, that at this point, the partition movement has not started, it merely tells you the current assignment and the proposed new assignment. The current assignment should be saved in case you want to rollback to it. The new assignment should be saved in a json file (e.g. expand-cluster-reassignment.json) to be input to the tool with the &ndash;execute option as follows:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
  Current partition replica assignment

  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]}]
  }

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]}]
  }
</code></pre><p>Finally, the &ndash;verify option can be used with the tool to check the status of the partition reassignment. Note that the same expand-cluster-reassignment.json (used with the &ndash;execute option) should be used with the &ndash;verify option:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo1,1] is in progress
  Reassignment of partition [foo1,2] is in progress
  Reassignment of partition [foo2,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
  Reassignment of partition [foo2,2] completed successfully
</code></pre><h3 id=custom-partition-assignment-and-migration>Custom partition assignment and migration<a class=td-heading-self-link href=#custom-partition-assignment-and-migration aria-label="Heading self-link"></a></h3><p>The partition reassignment tool can also be used to selectively move replicas of a partition to a specific set of brokers. When used in this manner, it is assumed that the user knows the reassignment plan and does not require the tool to generate a candidate reassignment, effectively skipping the &ndash;generate step and moving straight to the &ndash;execute step</p><p>For instance, the following example moves partition 0 of topic foo1 to brokers 5,6 and partition 1 of topic foo2 to brokers 2,3:</p><p>The first step is to hand craft the custom reassignment plan in a json file:</p><pre><code>  &gt; cat custom-reassignment.json
  {&quot;version&quot;:1,&quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},{&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]}]}
</code></pre><p>Then, use the json file with the &ndash;execute option to start the reassignment process:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
  Current partition replica assignment

  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,4]}]
  }

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]},
                {&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]}]
  }
</code></pre><p>The &ndash;verify option can be used with the tool to check the status of the partition reassignment. Note that the same expand-cluster-reassignment.json (used with the &ndash;execute option) should be used with the &ndash;verify option:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo1,0] completed successfully
  Reassignment of partition [foo2,1] completed successfully
</code></pre><h2 id=decommissioning-brokers>Decommissioning brokers<a class=td-heading-self-link href=#decommissioning-brokers aria-label="Heading self-link"></a></h2><p>The partition reassignment tool does not have the ability to automatically generate a reassignment plan for decommissioning brokers yet. As such, the admin has to come up with a reassignment plan to move the replica for all partitions hosted on the broker to be decommissioned, to the rest of the brokers. This can be relatively tedious as the reassignment needs to ensure that all the replicas are not moved from the decommissioned broker to only one other broker. To make this process effortless, we plan to add tooling support for decommissioning brokers in the future.</p><h2 id=increasing-replication-factor>Increasing replication factor<a class=td-heading-self-link href=#increasing-replication-factor aria-label="Heading self-link"></a></h2><p>Increasing the replication factor of an existing partition is easy. Just specify the extra replicas in the custom reassignment json file and use it with the &ndash;execute option to increase the replication factor of the specified partitions.</p><p>For instance, the following example increases the replication factor of partition 0 of topic foo from 1 to 3. Before increasing the replication factor, the partition&rsquo;s only replica existed on broker 5. As part of increasing the replication factor, we will add more replicas on brokers 6 and 7.</p><p>The first step is to hand craft the custom reassignment plan in a json file:</p><pre><code>  &gt; cat increase-replication-factor.json
  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6,7]}]}
</code></pre><p>Then, use the json file with the &ndash;execute option to start the reassignment process:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
  Current partition replica assignment

  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5]}]}

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started reassignment of partitions
  {&quot;version&quot;:1,
  &quot;partitions&quot;:[{&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6,7]}]}
</code></pre><p>The &ndash;verify option can be used with the tool to check the status of the partition reassignment. Note that the same increase-replication-factor.json (used with the &ndash;execute option) should be used with the &ndash;verify option:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify
  Status of partition reassignment:
  Reassignment of partition [foo,0] completed successfully
</code></pre><p>You can also verify the increase in replication factor with the kafka-topics tool:</p><pre><code>  &gt; bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe
  Topic:foo	PartitionCount:1	ReplicationFactor:3	Configs:
    Topic: foo	Partition: 0	Leader: 5	Replicas: 5,6,7	Isr: 5,6,7
</code></pre><h2 id=limiting-bandwidth-usage-during-data-migration>Limiting Bandwidth Usage during Data Migration<a class=td-heading-self-link href=#limiting-bandwidth-usage-during-data-migration aria-label="Heading self-link"></a></h2><p>Kafka lets you apply a throttle to replication traffic, setting an upper bound on the bandwidth used to move replicas from machine to machine. This is useful when rebalancing a cluster, bootstrapping a new broker or adding or removing brokers, as it limits the impact these data-intensive operations will have on users.</p><p>There are two interfaces that can be used to engage a throttle. The simplest, and safest, is to apply a throttle when invoking the kafka-reassign-partitions.sh, but kafka-configs.sh can also be used to view and alter the throttle values directly.</p><p>So for example, if you were to execute a rebalance, with the below command, it would move partitions at no more than 50MB/s.</p><pre><code>$ bin/kafka-reassign-partitions.sh --zookeeper myhost:2181--execute --reassignment-json-file bigger-cluster.json —throttle 50000000
</code></pre><p>When you execute this script you will see the throttle engage:</p><pre><code>  The throttle limit was set to 50000000 B/s
  Successfully started reassignment of partitions.
</code></pre><p>Should you wish to alter the throttle, during a rebalance, say to increase the throughput so it completes quicker, you can do this by re-running the execute command passing the same reassignment-json-file:</p><pre><code>$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --execute --reassignment-json-file bigger-cluster.json --throttle 700000000
  There is an existing assignment running.
  The throttle limit was set to 700000000 B/s
</code></pre><p>Once the rebalance completes the administrator can check the status of the rebalance using the &ndash;verify option. If the rebalance has completed, the throttle will be removed via the &ndash;verify command. It is important that administrators remove the throttle in a timely manner once rebalancing completes by running the command with the &ndash;verify option. Failure to do so could cause regular replication traffic to be throttled.</p><p>When the &ndash;verify option is executed, and the reassignment has completed, the script will confirm that the throttle was removed:</p><pre><code>  &gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --verify --reassignment-json-file bigger-cluster.json
  Status of partition reassignment:
  Reassignment of partition [my-topic,1] completed successfully
  Reassignment of partition [mytopic,0] completed successfully
  Throttle was removed.
</code></pre><p>The administrator can also validate the assigned configs using the kafka-configs.sh. There are two pairs of throttle configuration used to manage the throttling process. The throttle value itself. This is configured, at a broker level, using the dynamic properties:</p><pre><code>leader.replication.throttled.rate
  follower.replication.throttled.rate
</code></pre><p>There is also an enumerated set of throttled replicas:</p><pre><code>leader.replication.throttled.replicas
  follower.replication.throttled.replicas
</code></pre><p>Which are configured per topic. All four config values are automatically assigned by kafka-reassign-partitions.sh (discussed below).</p><p>To view the throttle limit configuration:</p><pre><code>  &gt; bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type brokers
  Configs for brokers '2' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
  Configs for brokers '1' are leader.replication.throttled.rate=700000000,follower.replication.throttled.rate=700000000
</code></pre><p>This shows the throttle applied to both leader and follower side of the replication protocol. By default both sides are assigned the same throttled throughput value.</p><p>To view the list of throttled replicas:</p><pre><code>  &gt; bin/kafka-configs.sh --describe --zookeeper localhost:2181 --entity-type topics
  Configs for topic 'my-topic' are leader.replication.throttled.replicas=1:102,0:101,
      follower.replication.throttled.replicas=1:101,0:102
</code></pre><p>Here we see the leader throttle is applied to partition 1 on broker 102 and partition 0 on broker 101. Likewise the follower throttle is applied to partition 1 on broker 101 and partition 0 on broker 102.</p><p>By default kafka-reassign-partitions.sh will apply the leader throttle to all replicas that exist before the rebalance, any one of which might be leader. It will apply the follower throttle to all move destinations. So if there is a partition with replicas on brokers 101,102, being reassigned to 102,103, a leader throttle, for that partition, would be applied to 101,102 and a follower throttle would be applied to 103 only.</p><p>If required, you can also use the &ndash;alter switch on kafka-configs.sh to alter the throttle configurations manually.</p><h3 id=safe-usage-of-throttled-replication>Safe usage of throttled replication<a class=td-heading-self-link href=#safe-usage-of-throttled-replication aria-label="Heading self-link"></a></h3><p>Some care should be taken when using throttled replication. In particular:</p><p><em>(1) Throttle Removal:</em></p><p>The throttle should be removed in a timely manner once reassignment completes (by running kafka-reassign-partitions —verify).</p><p><em>(2) Ensuring Progress:</em></p><p>If the throttle is set too low, in comparison to the incoming write rate, it is possible for replication to not make progress. This occurs when:</p><pre><code>max(BytesInPerSec) &gt; throttle
</code></pre><p>Where BytesInPerSec is the metric that monitors the write throughput of producers into each broker.</p><p>The administrator can monitor whether replication is making progress, during the rebalance, using the metric:</p><pre><code>kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)
</code></pre><p>The lag should constantly decrease during replication. If the metric does not decrease the administrator should increase the throttle throughput as described above.</p><h2 id=setting-quotas>Setting quotas<a class=td-heading-self-link href=#setting-quotas aria-label="Heading self-link"></a></h2><p>Quotas overrides and defaults may be configured at (user, client-id), user or client-id levels as described here. By default, clients receive an unlimited quota. It is possible to set custom quotas for each (user, client-id), user or client-id group.</p><p>Configure custom quota for (user=user1, client-id=clientA):</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Updated config for entity: user-principal 'user1', client-id 'clientA'.
</code></pre><p>Configure custom quota for user=user1:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1
  Updated config for entity: user-principal 'user1'.
</code></pre><p>Configure custom quota for client-id=clientA:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-name clientA
  Updated config for entity: client-id 'clientA'.
</code></pre><p>It is possible to set default quotas for each (user, client-id), user or client-id group by specifying <em>--entity-default</em> option instead of <em>--entity-name</em>.</p><p>Configure default client-id quota for user=userA:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-name user1 --entity-type clients --entity-default
  Updated config for entity: user-principal 'user1', default client-id.
</code></pre><p>Configure default quota for user:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type users --entity-default
  Updated config for entity: default user-principal.
</code></pre><p>Configure default quota for client-id:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200' --entity-type clients --entity-default
  Updated config for entity: default client-id.
</code></pre><p>Here&rsquo;s how to describe the quota for a given (user, client-id):</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1 --entity-type clients --entity-name clientA
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre><p>Describe quota for a given user:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-name user1
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre><p>Describe quota for a given client-id:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type clients --entity-name clientA
  Configs for client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre><p>If entity name is not specified, all entities of the specified type are described. For example, describe all users:</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users
  Configs for user-principal 'user1' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for default user-principal are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre><p>Similarly for (user, client):</p><pre><code>  &gt; bin/kafka-configs.sh  --zookeeper localhost:2181 --describe --entity-type users --entity-type clients
  Configs for user-principal 'user1', default client-id are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
  Configs for user-principal 'user1', client-id 'clientA' are producer_byte_rate=1024,consumer_byte_rate=2048,request_percentage=200
</code></pre><p>It is possible to set default quotas that apply to all client-ids by setting these configs on the brokers. These properties are applied only if quota overrides or defaults are not configured in Zookeeper. By default, each client-id receives an unlimited quota. The following sets the default quota per producer and consumer client-id to 10MB/sec.</p><pre><code>    quota.producer.default=10485760
    quota.consumer.default=10485760
</code></pre><p>Note that these properties are being deprecated and may be removed in a future release. Defaults configured using kafka-configs.sh take precedence over these properties.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-35318f45f4c5db57d169b76b3606152c>2 - Datacenters</h1><div class=lead>Datacenters</div><h1 id=datacenters>Datacenters<a class=td-heading-self-link href=#datacenters aria-label="Heading self-link"></a></h1><p>Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).</p><p>This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally. This allows each facility to stand alone and operate even if the inter-datacenter links are unavailable: when this occurs the mirroring falls behind until the link is restored at which time it catches up.</p><p>For applications that need a global view of all data you can use mirroring to provide clusters which have aggregate data mirrored from the local clusters in <em>all</em> datacenters. These aggregate clusters are used for reads by applications that require the full data set.</p><p>This is not the only possible deployment pattern. It is possible to read from or write to a remote Kafka cluster over the WAN, though obviously this will add whatever latency is required to get the cluster.</p><p>Kafka naturally batches data in both the producer and consumer so it can achieve high-throughput even over a high-latency connection. To allow this though it may be necessary to increase the TCP socket buffer sizes for the producer, consumer, and broker using the <code>socket.send.buffer.bytes</code> and <code>socket.receive.buffer.bytes</code> configurations. The appropriate way to set this is documented <a href=http://en.wikipedia.org/wiki/Bandwidth-delay_product>here</a>.</p><p>It is generally <em>not</em> advisable to run a <em>single</em> Kafka cluster that spans multiple datacenters over a high-latency link. This will incur very high replication latency both for Kafka writes and ZooKeeper writes, and neither Kafka nor ZooKeeper will remain available in all locations if the network between locations is unavailable.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b7e2ae5eace6446201846ed5e1242d94>3 - Kafka Configuration</h1><div class=lead>Kafka Configuration</div><h1 id=kafka-configuration>Kafka Configuration<a class=td-heading-self-link href=#kafka-configuration aria-label="Heading self-link"></a></h1><h2 id=important-client-configurations>Important Client Configurations<a class=td-heading-self-link href=#important-client-configurations aria-label="Heading self-link"></a></h2><p>The most important old Scala producer configurations control</p><ul><li>acks</li><li>compression</li><li>sync vs async production</li><li>batch size (for async producers)</li></ul><p>The most important new Java producer configurations control</p><ul><li>acks</li><li>compression</li><li>batch size</li></ul><p>The most important consumer configuration is the fetch size.</p><p>All configurations are documented in the configuration section.</p><h2 id=a-production-server-config>A Production Server Config<a class=td-heading-self-link href=#a-production-server-config aria-label="Heading self-link"></a></h2><p>Here is an example production server configuration:</p><pre><code>  # ZooKeeper
  zookeeper.connect=[list of ZooKeeper servers]

  # Log configuration
  num.partitions=8
  default.replication.factor=3
  log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).]

  # Other configurations
  broker.id=[An integer. Start with 0 and increment by 1 for each new broker.]
  listeners=[list of listeners]
  auto.create.topics.enable=false
  min.insync.replicas=2
  queued.max.requests=[number of concurrent requests]
</code></pre><p>Our client configuration varies a fair amount between different use cases.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2efa8a0c55fd9c040969c12b7f2b901d>4 - Java Version</h1><div class=lead>Java Version</div><h1 id=java-version>Java Version<a class=td-heading-self-link href=#java-version aria-label="Heading self-link"></a></h1><p>From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&rsquo;s tuning looks like this:</p><pre><code>  -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC
  -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
  -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80
</code></pre><p>For reference, here are the stats on one of LinkedIn&rsquo;s busiest clusters (at peak):</p><ul><li>60 brokers</li><li>50k partitions (replication factor 2)</li><li>800k messages/sec in</li><li>300 MB/sec inbound, 1 GB/sec+ outbound</li></ul><p>The tuning looks fairly aggressive, but all of the brokers in that cluster have a 90% GC pause time of about 21ms, and they&rsquo;re doing less than 1 young GC per second.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8990fdef2eebf110a9ab90bb5637d9fa>5 - Hardware and OS</h1><div class=lead>Hardware and OS</div><h1 id=hardware-and-os>Hardware and OS<a class=td-heading-self-link href=#hardware-and-os aria-label="Heading self-link"></a></h1><p>We are using dual quad-core Intel Xeon machines with 24GB of memory.</p><p>You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.</p><p>The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better. Depending on how you configure flush behavior you may or may not benefit from more expensive disks (if you force flush often then higher RPM SAS drives may be better).</p><h2 id=os>OS<a class=td-heading-self-link href=#os aria-label="Heading self-link"></a></h2><p>Kafka should run well on any unix system and has been tested on Linux and Solaris.</p><p>We have seen a few issues running on Windows and Windows is not currently a well supported platform though we would be happy to change that.</p><p>It is unlikely to require much OS-level tuning, but there are two potentially important OS-level configurations:</p><ul><li>File descriptor limits: Kafka uses file descriptors for log segments and open connections. If a broker hosts many partitions, consider that the broker needs at least (number_of_partitions)*(partition_size/segment_size) to track all log segments in addition to the number of connections the broker makes. We recommend at least 100000 allowed file descriptors for the broker processes as a starting point.</li><li>Max socket buffer size: can be increased to enable high-performance data transfer between data centers as <a href=http://www.psc.edu/index.php/networking/641-tcp-tune>described here</a>.</li></ul><h2 id=disks-and-filesystem>Disks and Filesystem<a class=td-heading-self-link href=#disks-and-filesystem aria-label="Heading self-link"></a></h2><p>We recommend using multiple drives to get good throughput and not sharing the same drives used for Kafka data with application logs or other OS filesystem activity to ensure good latency. You can either RAID these drives together into a single volume or format and mount each drive as its own directory. Since Kafka has replication the redundancy provided by RAID can also be provided at the application level. This choice has several tradeoffs.</p><p>If you configure multiple data directories partitions will be assigned round-robin to data directories. Each partition will be entirely in one of the data directories. If data is not well balanced among partitions this can lead to load imbalance between disks.</p><p>RAID can potentially do better at balancing load between disks (although it doesn&rsquo;t always seem to) because it balances load at a lower level. The primary downside of RAID is that it is usually a big performance hit for write throughput and reduces the available disk space.</p><p>Another potential benefit of RAID is the ability to tolerate disk failures. However our experience has been that rebuilding the RAID array is so I/O intensive that it effectively disables the server, so this does not provide much real availability improvement.</p><h2 id=application-vs-os-flush-management>Application vs. OS Flush Management<a class=td-heading-self-link href=#application-vs-os-flush-management aria-label="Heading self-link"></a></h2><p>Kafka always immediately writes all data to the filesystem and supports the ability to configure the flush policy that controls when data is forced out of the OS cache and onto disk using the flush. This flush policy can be controlled to force data to disk after a period of time or after a certain number of messages has been written. There are several choices in this configuration.</p><p>Kafka must eventually call fsync to know that data was flushed. When recovering from a crash for any log segment not known to be fsync&rsquo;d Kafka will check the integrity of each message by checking its CRC and also rebuild the accompanying offset index file as part of the recovery process executed on startup.</p><p>Note that durability in Kafka does not require syncing data to disk, as a failed node will always recover from its replicas.</p><p>We recommend using the default flush settings which disable application fsync entirely. This means relying on the background flush done by the OS and Kafka&rsquo;s own background flush. This provides the best of all worlds for most uses: no knobs to tune, great throughput and latency, and full recovery guarantees. We generally feel that the guarantees provided by replication are stronger than sync to local disk, however the paranoid still may prefer having both and application level fsync policies are still supported.</p><p>The drawback of using application level flush settings is that it is less efficient in its disk usage pattern (it gives the OS less leeway to re-order writes) and it can introduce latency as fsync in most Linux filesystems blocks writes to the file whereas the background flushing does much more granular page-level locking.</p><p>In general you don&rsquo;t need to do any low-level tuning of the filesystem, but in the next few sections we will go over some of this in case it is useful.</p><h2 id=understanding-linux-os-flush-behavior>Understanding Linux OS Flush Behavior<a class=td-heading-self-link href=#understanding-linux-os-flush-behavior aria-label="Heading self-link"></a></h2><p>In Linux, data written to the filesystem is maintained in <a href=http://en.wikipedia.org/wiki/Page_cache>pagecache</a> until it must be written out to disk (due to an application-level fsync or the OS&rsquo;s own flush policy). The flushing of data is done by a set of background threads called pdflush (or in post 2.6.32 kernels &ldquo;flusher threads&rdquo;).</p><p>Pdflush has a configurable policy that controls how much dirty data can be maintained in cache and for how long before it must be written back to disk. This policy is described <a href=http://web.archive.org/web/20160518040713/http://www.westnet.com/~gsmith/content/linux-pdflush.htm>here</a>. When Pdflush cannot keep up with the rate of data being written it will eventually cause the writing process to block incurring latency in the writes to slow down the accumulation of data.</p><p>You can see the current state of OS memory usage by doing</p><pre><code> &gt; cat /proc/meminfo 
</code></pre><p>The meaning of these values are described in the link above.</p><p>Using pagecache has several advantages over an in-process cache for storing data that will be written out to disk:</p><ul><li>The I/O scheduler will batch together consecutive small writes into bigger physical writes which improves throughput.</li><li>The I/O scheduler will attempt to re-sequence writes to minimize movement of the disk head which improves throughput.</li><li>It automatically uses all the free memory on the machine</li></ul><h2 id=filesystem-selection>Filesystem Selection<a class=td-heading-self-link href=#filesystem-selection aria-label="Heading self-link"></a></h2><p>Kafka uses regular files on disk, and as such it has no hard dependency on a specific filesystem. The two filesystems which have the most usage, however, are EXT4 and XFS. Historically, EXT4 has had more usage, but recent improvements to the XFS filesystem have shown it to have better performance characteristics for Kafka&rsquo;s workload with no compromise in stability.</p><p>Comparison testing was performed on a cluster with significant message loads, using a variety of filesystem creation and mount options. The primary metric in Kafka that was monitored was the &ldquo;Request Local Time&rdquo;, indicating the amount of time append operations were taking. XFS resulted in much better local times (160ms vs. 250ms+ for the best EXT4 configuration), as well as lower average wait times. The XFS performance also showed less variability in disk performance.</p><h3 id=general-filesystem-notes>General Filesystem Notes<a class=td-heading-self-link href=#general-filesystem-notes aria-label="Heading self-link"></a></h3><p>For any filesystem used for data directories, on Linux systems, the following options are recommended to be used at mount time:</p><ul><li>noatime: This option disables updating of a file&rsquo;s atime (last access time) attribute when the file is read. This can eliminate a significant number of filesystem writes, especially in the case of bootstrapping consumers. Kafka does not rely on the atime attributes at all, so it is safe to disable this.</li></ul><h3 id=xfs-notes>XFS Notes<a class=td-heading-self-link href=#xfs-notes aria-label="Heading self-link"></a></h3><p>The XFS filesystem has a significant amount of auto-tuning in place, so it does not require any change in the default settings, either at filesystem creation time or at mount. The only tuning parameters worth considering are:</p><ul><li>largeio: This affects the preferred I/O size reported by the stat call. While this can allow for higher performance on larger disk writes, in practice it had minimal or no effect on performance.</li><li>nobarrier: For underlying devices that have battery-backed cache, this option can provide a little more performance by disabling periodic write flushes. However, if the underlying device is well-behaved, it will report to the filesystem that it does not require flushes, and this option will have no effect.</li></ul><h3 id=ext4-notes>EXT4 Notes<a class=td-heading-self-link href=#ext4-notes aria-label="Heading self-link"></a></h3><p>EXT4 is a serviceable choice of filesystem for the Kafka data directories, however getting the most performance out of it will require adjusting several mount options. In addition, these options are generally unsafe in a failure scenario, and will result in much more data loss and corruption. For a single broker failure, this is not much of a concern as the disk can be wiped and the replicas rebuilt from the cluster. In a multiple-failure scenario, such as a power outage, this can mean underlying filesystem (and therefore data) corruption that is not easily recoverable. The following options can be adjusted:</p><ul><li>data=writeback: Ext4 defaults to data=ordered which puts a strong order on some writes. Kafka does not require this ordering as it does very paranoid data recovery on all unflushed log. This setting removes the ordering constraint and seems to significantly reduce latency.</li><li>Disabling journaling: Journaling is a tradeoff: it makes reboots faster after server crashes but it introduces a great deal of additional locking which adds variance to write performance. Those who don&rsquo;t care about reboot time and want to reduce a major source of write latency spikes can turn off journaling entirely.</li><li>commit=num_secs: This tunes the frequency with which ext4 commits to its metadata journal. Setting this to a lower value reduces the loss of unflushed data during a crash. Setting this to a higher value will improve throughput.</li><li>nobh: This setting controls additional ordering guarantees when using data=writeback mode. This should be safe with Kafka as we do not depend on write ordering and improves throughput and latency.</li><li>delalloc: Delayed allocation means that the filesystem avoid allocating any blocks until the physical write occurs. This allows ext4 to allocate a large extent instead of smaller pages and helps ensure the data is written sequentially. This feature is great for throughput. It does seem to involve some locking in the filesystem which adds a bit of latency variance.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e4a18d4403d4aeed7eb0b312b9e33ae4>6 - Monitoring</h1><div class=lead>Monitoring</div><h1 id=monitoring>Monitoring<a class=td-heading-self-link href=#monitoring aria-label="Heading self-link"></a></h1><p>Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.</p><p>All Kafka rate metrics have a corresponding cumulative count metric with suffix <code>-total</code>. For example, <code>records-consumed-rate</code> has a corresponding metric named <code>records-consumed-total</code>.</p><p>The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.</p><table><thead><tr><th>We do graphing and alerting on the following metrics: Description</th><th>Mbean name</th><th>Normal value</th></tr></thead><tbody><tr><td>Message in rate</td><td>kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec</td><td></td></tr><tr><td>Byte in rate from clients</td><td>kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec</td><td></td></tr><tr><td>Byte in rate from other brokers</td><td>kafka.server:type=BrokerTopicMetrics,name=ReplicationBytesInPerSec</td><td></td></tr><tr><td>Request rate</td><td>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce</td><td>FetchConsumer</td></tr><tr><td>Error rate</td><td>kafka.network:type=RequestMetrics,name=ErrorsPerSec,request=([-.\w]+),error=([-.\w]+)</td><td>Number of errors in responses counted per-request-type, per-error-code. If a response contains multiple errors, all are counted. error=NONE indicates successful responses.</td></tr><tr><td>Request size in bytes</td><td>kafka.network:type=RequestMetrics,name=RequestBytes,request=([-.\w]+)</td><td>Size of requests for each request type.</td></tr><tr><td>Temporary memory size in bytes</td><td>kafka.network:type=RequestMetrics,name=TemporaryMemoryBytes,request={Produce</td><td>Fetch}</td></tr><tr><td>Message conversion time</td><td>kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request={Produce</td><td>Fetch}</td></tr><tr><td>Message conversion rate</td><td>kafka.server:type=BrokerTopicMetrics,name={Produce</td><td>Fetch}MessageConversionsPerSec,topic=([-.\w]+)</td></tr><tr><td>Byte out rate to clients</td><td>kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec</td><td></td></tr><tr><td>Byte out rate to other brokers</td><td>kafka.server:type=BrokerTopicMetrics,name=ReplicationBytesOutPerSec</td><td></td></tr><tr><td>Log flush rate and time</td><td>kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs</td><td></td></tr></tbody></table><h1 id=of-under-replicated-partitions-isr--all-replicas--kafkaservertypereplicamanagernameunderreplicatedpartitions--0>of under replicated partitions (|ISR| &lt; |all replicas|) | kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions | 0<a class=td-heading-self-link href=#of-under-replicated-partitions-isr--all-replicas--kafkaservertypereplicamanagernameunderreplicatedpartitions--0 aria-label="Heading self-link"></a></h1><h1 id=of-under-minisr-partitions-isr--mininsyncreplicas--kafkaservertypereplicamanagernameunderminisrpartitioncount--0>of under minIsr partitions (|ISR| &lt; min.insync.replicas) | kafka.server:type=ReplicaManager,name=UnderMinIsrPartitionCount | 0<a class=td-heading-self-link href=#of-under-minisr-partitions-isr--mininsyncreplicas--kafkaservertypereplicamanagernameunderminisrpartitioncount--0 aria-label="Heading self-link"></a></h1><h1 id=of-offline-log-directories--kafkalogtypelogmanagernameofflinelogdirectorycount--0>of offline log directories | kafka.log:type=LogManager,name=OfflineLogDirectoryCount | 0<a class=td-heading-self-link href=#of-offline-log-directories--kafkalogtypelogmanagernameofflinelogdirectorycount--0 aria-label="Heading self-link"></a></h1><p>Is controller active on broker | kafka.controller:type=KafkaController,name=ActiveControllerCount | only one broker in the cluster should have 1<br>Leader election rate | kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs | non-zero when there are broker failures<br>Unclean leader election rate | kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec | 0<br>Partition counts | kafka.server:type=ReplicaManager,name=PartitionCount | mostly even across brokers<br>Leader replica counts | kafka.server:type=ReplicaManager,name=LeaderCount | mostly even across brokers<br>ISR shrink rate | kafka.server:type=ReplicaManager,name=IsrShrinksPerSec | If a broker goes down, ISR for some of the partitions will shrink. When that broker is up again, ISR will be expanded once the replicas are fully caught up. Other than that, the expected value for both ISR shrink rate and expansion rate is 0.<br>ISR expansion rate | kafka.server:type=ReplicaManager,name=IsrExpandsPerSec | See above<br>Max lag in messages btw follower and leader replicas | kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica | lag should be proportional to the maximum batch size of a produce request.<br>Lag in messages per follower replica | kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+) | lag should be proportional to the maximum batch size of a produce request.<br>Requests waiting in the producer purgatory | kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce | non-zero if ack=-1 is used<br>Requests waiting in the fetch purgatory | kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fetch | size depends on fetch.wait.max.ms in the consumer<br>Request total time | kafka.network:type=RequestMetrics,name=TotalTimeMs,request={Produce|FetchConsumer|FetchFollower} | broken into queue, local, remote and response send time<br>Time the request waits in the request queue | kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request={Produce|FetchConsumer|FetchFollower} |<br>Time the request is processed at the leader | kafka.network:type=RequestMetrics,name=LocalTimeMs,request={Produce|FetchConsumer|FetchFollower} |<br>Time the request waits for the follower | kafka.network:type=RequestMetrics,name=RemoteTimeMs,request={Produce|FetchConsumer|FetchFollower} | non-zero for produce requests when ack=-1<br>Time the request waits in the response queue | kafka.network:type=RequestMetrics,name=ResponseQueueTimeMs,request={Produce|FetchConsumer|FetchFollower} |<br>Time to send the response | kafka.network:type=RequestMetrics,name=ResponseSendTimeMs,request={Produce|FetchConsumer|FetchFollower} |<br>Number of messages the consumer lags behind the producer by. Published by the consumer, not broker. | <em>Old consumer:</em> kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=([-.\w]+) <em>New consumer:</em> kafka.consumer:type=consumer-fetch-manager-metrics,client-id={client-id} Attribute: records-lag-max |<br>The average fraction of time the network processors are idle | kafka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent | between 0 and 1, ideally > 0.3<br>The average fraction of time the request handler threads are idle | kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent | between 0 and 1, ideally > 0.3<br>Bandwidth quota metrics per (user, client-id), user or client-id | kafka.server:type={Produce|Fetch},user=([-.\w]+),client-id=([-.\w]+) | Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0. byte-rate indicates the data produce/consume rate of the client in bytes/sec. For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.<br>Request quota metrics per (user, client-id), user or client-id | kafka.server:type=Request,user=([-.\w]+),client-id=([-.\w]+) | Two attributes. throttle-time indicates the amount of time in ms the client was throttled. Ideally = 0. request-time indicates the percentage of time spent in broker network and I/O threads to process requests from client group. For (user, client-id) quotas, both user and client-id are specified. If per-client-id quota is applied to the client, user is not specified. If per-user quota is applied, client-id is not specified.<br>Requests exempt from throttling | kafka.server:type=Request | exempt-throttle-time indicates the percentage of time spent in broker network and I/O threads to process requests that are exempt from throttling.<br>ZooKeeper client request latency | kafka.server:type=ZooKeeperClientMetrics,name=ZooKeeperRequestLatencyMs | Latency in millseconds for ZooKeeper requests from broker.<br>ZooKeeper connection status | kafka.server:type=SessionExpireListener,name=SessionState | Connection status of broker&rsquo;s ZooKeeper session which may be one of Disconnected|SyncConnected|AuthFailed|ConnectedReadOnly|SaslAuthenticated|Expired.</p><h2 id=common-monitoring-metrics-for-producerconsumerconnectstreams>Common monitoring metrics for producer/consumer/connect/streams<a class=td-heading-self-link href=#common-monitoring-metrics-for-producerconsumerconnectstreams aria-label="Heading self-link"></a></h2><table><thead><tr><th>The following metrics are available on producer/consumer/connector/streams instances. For specific metrics, please see following sections. Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>connection-close-rate</td><td>Connections closed per second in the window.</td><td>kafka.[producer</td></tr><tr><td>connection-creation-rate</td><td>New connections established per second in the window.</td><td>kafka.[producer</td></tr><tr><td>network-io-rate</td><td>The average number of network operations (reads or writes) on all connections per second.</td><td>kafka.[producer</td></tr><tr><td>outgoing-byte-rate</td><td>The average number of outgoing bytes sent per second to all servers.</td><td>kafka.[producer</td></tr><tr><td>request-rate</td><td>The average number of requests sent per second.</td><td>kafka.[producer</td></tr><tr><td>request-size-avg</td><td>The average size of all requests in the window.</td><td>kafka.[producer</td></tr><tr><td>request-size-max</td><td>The maximum size of any request sent in the window.</td><td>kafka.[producer</td></tr><tr><td>incoming-byte-rate</td><td>Bytes/second read off all sockets.</td><td>kafka.[producer</td></tr><tr><td>response-rate</td><td>Responses received sent per second.</td><td>kafka.[producer</td></tr><tr><td>select-rate</td><td>Number of times the I/O layer checked for new I/O to perform per second.</td><td>kafka.[producer</td></tr><tr><td>io-wait-time-ns-avg</td><td>The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds.</td><td>kafka.[producer</td></tr><tr><td>io-wait-ratio</td><td>The fraction of time the I/O thread spent waiting.</td><td>kafka.[producer</td></tr><tr><td>io-time-ns-avg</td><td>The average length of time for I/O per select call in nanoseconds.</td><td>kafka.[producer</td></tr><tr><td>io-ratio</td><td>The fraction of time the I/O thread spent doing I/O.</td><td>kafka.[producer</td></tr><tr><td>connection-count</td><td>The current number of active connections.</td><td>kafka.[producer</td></tr><tr><td>successful-authentication-rate</td><td>Connections that were successfully authenticated using SASL or SSL.</td><td>kafka.[producer</td></tr><tr><td>failed-authentication-rate</td><td>Connections that failed authentication.</td><td>kafka.[producer</td></tr></tbody></table><h2 id=common-per-broker-metrics-for-producerconsumerconnectstreams>Common Per-broker metrics for producer/consumer/connect/streams<a class=td-heading-self-link href=#common-per-broker-metrics-for-producerconsumerconnectstreams aria-label="Heading self-link"></a></h2><table><thead><tr><th>The following metrics are available on producer/consumer/connector/streams instances. For specific metrics, please see following sections. Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>outgoing-byte-rate</td><td>The average number of outgoing bytes sent per second for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>request-rate</td><td>The average number of requests sent per second for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>request-size-avg</td><td>The average size of all requests in the window for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>request-size-max</td><td>The maximum size of any request sent in the window for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>incoming-byte-rate</td><td>The average number of responses received per second for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>request-latency-avg</td><td>The average request latency in ms for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>request-latency-max</td><td>The maximum request latency in ms for a node.</td><td>kafka.producer:type=[consumer</td></tr><tr><td>response-rate</td><td>Responses received sent per second for a node.</td><td>kafka.producer:type=[consumer</td></tr></tbody></table><h2 id=producer-monitoring>Producer monitoring<a class=td-heading-self-link href=#producer-monitoring aria-label="Heading self-link"></a></h2><table><thead><tr><th>The following metrics are available on producer instances. Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>waiting-threads</td><td>The number of user threads blocked waiting for buffer memory to enqueue their records.</td><td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td></tr><tr><td>buffer-total-bytes</td><td>The maximum amount of buffer memory the client can use (whether or not it is currently used).</td><td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td></tr><tr><td>buffer-available-bytes</td><td>The total amount of buffer memory that is not being used (either unallocated or in the free list).</td><td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td></tr><tr><td>bufferpool-wait-time</td><td>The fraction of time an appender waits for space allocation.</td><td>kafka.producer:type=producer-metrics,client-id=([-.\w]+)</td></tr></tbody></table><h3 id=producer-sender-metrics>Producer Sender Metrics<a class=td-heading-self-link href=#producer-sender-metrics aria-label="Heading self-link"></a></h3><table class=data-table><tbody><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.producer:type=producer-metrics,client-id="{client-id}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>batch-size-avg</td><td>The average number of bytes sent per partition per-request.</td></tr><tr><td></td><td>batch-size-max</td><td>The max number of bytes sent per partition per-request.</td></tr><tr><td></td><td>batch-split-rate</td><td>The average number of batch splits per second</td></tr><tr><td></td><td>batch-split-total</td><td>The total number of batch splits</td></tr><tr><td></td><td>compression-rate-avg</td><td>The average compression rate of record batches.</td></tr><tr><td></td><td>metadata-age</td><td>The age in seconds of the current producer metadata being used.</td></tr><tr><td></td><td>produce-throttle-time-avg</td><td>The average time in ms a request was throttled by a broker</td></tr><tr><td></td><td>produce-throttle-time-max</td><td>The maximum time in ms a request was throttled by a broker</td></tr><tr><td></td><td>record-error-rate</td><td>The average per-second number of record sends that resulted in errors</td></tr><tr><td></td><td>record-error-total</td><td>The total number of record sends that resulted in errors</td></tr><tr><td></td><td>record-queue-time-avg</td><td>The average time in ms record batches spent in the send buffer.</td></tr><tr><td></td><td>record-queue-time-max</td><td>The maximum time in ms record batches spent in the send buffer.</td></tr><tr><td></td><td>record-retry-rate</td><td>The average per-second number of retried record sends</td></tr><tr><td></td><td>record-retry-total</td><td>The total number of retried record sends</td></tr><tr><td></td><td>record-send-rate</td><td>The average number of records sent per second.</td></tr><tr><td></td><td>record-send-total</td><td>The total number of records sent.</td></tr><tr><td></td><td>record-size-avg</td><td>The average record size</td></tr><tr><td></td><td>record-size-max</td><td>The maximum record size</td></tr><tr><td></td><td>records-per-request-avg</td><td>The average number of records per request.</td></tr><tr><td></td><td>request-latency-avg</td><td>The average request latency in ms</td></tr><tr><td></td><td>request-latency-max</td><td>The maximum request latency in ms</td></tr><tr><td></td><td>requests-in-flight</td><td>The current number of in-flight requests awaiting a response.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.producer:type=producer-topic-metrics,client-id="{client-id}",topic="{topic}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>byte-rate</td><td>The average number of bytes sent per second for a topic.</td></tr><tr><td></td><td>byte-total</td><td>The total number of bytes sent for a topic.</td></tr><tr><td></td><td>compression-rate</td><td>The average compression rate of record batches for a topic.</td></tr><tr><td></td><td>record-error-rate</td><td>The average per-second number of record sends that resulted in errors for a topic</td></tr><tr><td></td><td>record-error-total</td><td>The total number of record sends that resulted in errors for a topic</td></tr><tr><td></td><td>record-retry-rate</td><td>The average per-second number of retried record sends for a topic</td></tr><tr><td></td><td>record-retry-total</td><td>The total number of retried record sends for a topic</td></tr><tr><td></td><td>record-send-rate</td><td>The average number of records sent per second for a topic.</td></tr><tr><td></td><td>record-send-total</td><td>The total number of records sent for a topic.</td></tr></tbody></table><h2 id=new-consumer-monitoring>New consumer monitoring<a class=td-heading-self-link href=#new-consumer-monitoring aria-label="Heading self-link"></a></h2><p>The following metrics are available on new consumer instances.</p><h3 id=consumer-group-metrics>Consumer Group Metrics<a class=td-heading-self-link href=#consumer-group-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>commit-latency-avg</td><td>The average time taken for a commit request</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>commit-latency-max</td><td>The max time taken for a commit request</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>commit-rate</td><td>The number of commit calls per second</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>assigned-partitions</td><td>The number of partitions currently assigned to this consumer</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>heartbeat-response-time-max</td><td>The max time taken to receive a response to a heartbeat request</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>heartbeat-rate</td><td>The average number of heartbeats per second</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>join-time-avg</td><td>The average time taken for a group rejoin</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>join-time-max</td><td>The max time taken for a group rejoin</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>join-rate</td><td>The number of group joins per second</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>sync-time-avg</td><td>The average time taken for a group sync</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>sync-time-max</td><td>The max time taken for a group sync</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>sync-rate</td><td>The number of group syncs per second</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr><tr><td>last-heartbeat-seconds-ago</td><td>The number of seconds since the last controller heartbeat</td><td>kafka.consumer:type=consumer-coordinator-metrics,client-id=([-.\w]+)</td></tr></tbody></table><h3 id=consumer-fetch-metrics>Consumer Fetch Metrics<a class=td-heading-self-link href=#consumer-fetch-metrics aria-label="Heading self-link"></a></h3><table class=data-table><tbody><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.consumer:type=consumer-fetch-manager-metrics,client-id="{client-id}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>bytes-consumed-rate</td><td>The average number of bytes consumed per second</td></tr><tr><td></td><td>bytes-consumed-total</td><td>The total number of bytes consumed</td></tr><tr><td></td><td>fetch-latency-avg</td><td>The average time taken for a fetch request.</td></tr><tr><td></td><td>fetch-latency-max</td><td>The max time taken for any fetch request.</td></tr><tr><td></td><td>fetch-rate</td><td>The number of fetch requests per second.</td></tr><tr><td></td><td>fetch-size-avg</td><td>The average number of bytes fetched per request</td></tr><tr><td></td><td>fetch-size-max</td><td>The maximum number of bytes fetched per request</td></tr><tr><td></td><td>fetch-throttle-time-avg</td><td>The average throttle time in ms</td></tr><tr><td></td><td>fetch-throttle-time-max</td><td>The maximum throttle time in ms</td></tr><tr><td></td><td>fetch-total</td><td>The total number of fetch requests.</td></tr><tr><td></td><td>records-consumed-rate</td><td>The average number of records consumed per second</td></tr><tr><td></td><td>records-consumed-total</td><td>The total number of records consumed</td></tr><tr><td></td><td>records-lag-max</td><td>The maximum lag in terms of number of records for any partition in this window</td></tr><tr><td></td><td>records-lead-min</td><td>The minimum lead in terms of number of records for any partition in this window</td></tr><tr><td></td><td>records-per-request-avg</td><td>The average number of records in each request</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.consumer:type=consumer-fetch-manager-metrics,client-id="{client-id}",topic="{topic}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>bytes-consumed-rate</td><td>The average number of bytes consumed per second for a topic</td></tr><tr><td></td><td>bytes-consumed-total</td><td>The total number of bytes consumed for a topic</td></tr><tr><td></td><td>fetch-size-avg</td><td>The average number of bytes fetched per request for a topic</td></tr><tr><td></td><td>fetch-size-max</td><td>The maximum number of bytes fetched per request for a topic</td></tr><tr><td></td><td>records-consumed-rate</td><td>The average number of records consumed per second for a topic</td></tr><tr><td></td><td>records-consumed-total</td><td>The total number of records consumed for a topic</td></tr><tr><td></td><td>records-per-request-avg</td><td>The average number of records in each request for a topic</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.consumer:type=consumer-fetch-manager-metrics,partition="{partition}",topic="{topic}",client-id="{client-id}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>records-lag</td><td>The latest lag of the partition</td></tr><tr><td></td><td>records-lag-avg</td><td>The average lag of the partition</td></tr><tr><td></td><td>records-lag-max</td><td>The max lag of the partition</td></tr><tr><td></td><td>records-lead</td><td>The latest lead of the partition</td></tr><tr><td></td><td>records-lead-avg</td><td>The average lead of the partition</td></tr><tr><td></td><td>records-lead-min</td><td>The min lead of the partition</td></tr></tbody></table><h2 id=connect-monitoring>Connect Monitoring<a class=td-heading-self-link href=#connect-monitoring aria-label="Heading self-link"></a></h2><p>A Connect worker process contains all the producer and consumer metrics as well as metrics specific to Connect. The worker process itself has a number of metrics, while each connector and task have additional metrics.<table class=data-table><tbody><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=connect-worker-metrics</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>connector-count</td><td>The number of connectors run in this worker.</td></tr><tr><td></td><td>connector-startup-attempts-total</td><td>The total number of connector startups that this worker has attempted.</td></tr><tr><td></td><td>connector-startup-failure-percentage</td><td>The average percentage of this worker's connectors starts that failed.</td></tr><tr><td></td><td>connector-startup-failure-total</td><td>The total number of connector starts that failed.</td></tr><tr><td></td><td>connector-startup-success-percentage</td><td>The average percentage of this worker's connectors starts that succeeded.</td></tr><tr><td></td><td>connector-startup-success-total</td><td>The total number of connector starts that succeeded.</td></tr><tr><td></td><td>task-count</td><td>The number of tasks run in this worker.</td></tr><tr><td></td><td>task-startup-attempts-total</td><td>The total number of task startups that this worker has attempted.</td></tr><tr><td></td><td>task-startup-failure-percentage</td><td>The average percentage of this worker's tasks starts that failed.</td></tr><tr><td></td><td>task-startup-failure-total</td><td>The total number of task starts that failed.</td></tr><tr><td></td><td>task-startup-success-percentage</td><td>The average percentage of this worker's tasks starts that succeeded.</td></tr><tr><td></td><td>task-startup-success-total</td><td>The total number of task starts that succeeded.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=connect-worker-rebalance-metrics</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>completed-rebalances-total</td><td>The total number of rebalances completed by this worker.</td></tr><tr><td></td><td>epoch</td><td>The epoch or generation number of this worker.</td></tr><tr><td></td><td>leader-name</td><td>The name of the group leader.</td></tr><tr><td></td><td>rebalance-avg-time-ms</td><td>The average time in milliseconds spent by this worker to rebalance.</td></tr><tr><td></td><td>rebalance-max-time-ms</td><td>The maximum time in milliseconds spent by this worker to rebalance.</td></tr><tr><td></td><td>rebalancing</td><td>Whether this worker is currently rebalancing.</td></tr><tr><td></td><td>time-since-last-rebalance-ms</td><td>The time in milliseconds since this worker completed the most recent rebalance.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=connector-metrics,connector="{connector}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>connector-class</td><td>The name of the connector class.</td></tr><tr><td></td><td>connector-type</td><td>The type of the connector. One of 'source' or 'sink'.</td></tr><tr><td></td><td>connector-version</td><td>The version of the connector class, as reported by the connector.</td></tr><tr><td></td><td>status</td><td>The status of the connector. One of 'unassigned', 'running', 'paused', 'failed', or 'destroyed'.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=connector-task-metrics,connector="{connector}",task="{task}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>batch-size-avg</td><td>The average size of the batches processed by the connector.</td></tr><tr><td></td><td>batch-size-max</td><td>The maximum size of the batches processed by the connector.</td></tr><tr><td></td><td>offset-commit-avg-time-ms</td><td>The average time in milliseconds taken by this task to commit offsets.</td></tr><tr><td></td><td>offset-commit-failure-percentage</td><td>The average percentage of this task's offset commit attempts that failed.</td></tr><tr><td></td><td>offset-commit-max-time-ms</td><td>The maximum time in milliseconds taken by this task to commit offsets.</td></tr><tr><td></td><td>offset-commit-success-percentage</td><td>The average percentage of this task's offset commit attempts that succeeded.</td></tr><tr><td></td><td>pause-ratio</td><td>The fraction of time this task has spent in the pause state.</td></tr><tr><td></td><td>running-ratio</td><td>The fraction of time this task has spent in the running state.</td></tr><tr><td></td><td>status</td><td>The status of the connector task. One of 'unassigned', 'running', 'paused', 'failed', or 'destroyed'.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=sink-task-metrics,connector="{connector}",task="{task}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>offset-commit-completion-rate</td><td>The average per-second number of offset commit completions that were completed successfully.</td></tr><tr><td></td><td>offset-commit-completion-total</td><td>The total number of offset commit completions that were completed successfully.</td></tr><tr><td></td><td>offset-commit-seq-no</td><td>The current sequence number for offset commits.</td></tr><tr><td></td><td>offset-commit-skip-rate</td><td>The average per-second number of offset commit completions that were received too late and skipped/ignored.</td></tr><tr><td></td><td>offset-commit-skip-total</td><td>The total number of offset commit completions that were received too late and skipped/ignored.</td></tr><tr><td></td><td>partition-count</td><td>The number of topic partitions assigned to this task belonging to the named sink connector in this worker.</td></tr><tr><td></td><td>put-batch-avg-time-ms</td><td>The average time taken by this task to put a batch of sinks records.</td></tr><tr><td></td><td>put-batch-max-time-ms</td><td>The maximum time taken by this task to put a batch of sinks records.</td></tr><tr><td></td><td>sink-record-active-count</td><td>The number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td></tr><tr><td></td><td>sink-record-active-count-avg</td><td>The average number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td></tr><tr><td></td><td>sink-record-active-count-max</td><td>The maximum number of records that have been read from Kafka but not yet completely committed/flushed/acknowledged by the sink task.</td></tr><tr><td></td><td>sink-record-lag-max</td><td>The maximum lag in terms of number of records that the sink task is behind the consumer's position for any topic partitions.</td></tr><tr><td></td><td>sink-record-read-rate</td><td>The average per-second number of records read from Kafka for this task belonging to the named sink connector in this worker. This is before transformations are applied.</td></tr><tr><td></td><td>sink-record-read-total</td><td>The total number of records read from Kafka by this task belonging to the named sink connector in this worker, since the task was last restarted.</td></tr><tr><td></td><td>sink-record-send-rate</td><td>The average per-second number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.</td></tr><tr><td></td><td>sink-record-send-total</td><td>The total number of records output from the transformations and sent/put to this task belonging to the named sink connector in this worker, since the task was last restarted.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=source-task-metrics,connector="{connector}",task="{task}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>poll-batch-avg-time-ms</td><td>The average time in milliseconds taken by this task to poll for a batch of source records.</td></tr><tr><td></td><td>poll-batch-max-time-ms</td><td>The maximum time in milliseconds taken by this task to poll for a batch of source records.</td></tr><tr><td></td><td>source-record-active-count</td><td>The number of records that have been produced by this task but not yet completely written to Kafka.</td></tr><tr><td></td><td>source-record-active-count-avg</td><td>The average number of records that have been produced by this task but not yet completely written to Kafka.</td></tr><tr><td></td><td>source-record-active-count-max</td><td>The maximum number of records that have been produced by this task but not yet completely written to Kafka.</td></tr><tr><td></td><td>source-record-poll-rate</td><td>The average per-second number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.</td></tr><tr><td></td><td>source-record-poll-total</td><td>The total number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.</td></tr><tr><td></td><td>source-record-write-rate</td><td>The average per-second number of records output from the transformations and written to Kafka for this task belonging to the named source connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.</td></tr><tr><td></td><td>source-record-write-total</td><td>The number of records output from the transformations and written to Kafka for this task belonging to the named source connector in this worker, since the task was last restarted.</td></tr><tr><td colspan=3 class=mbeanName style=background-color:#ccc;font-weight:700>kafka.connect:type=task-error-metrics,connector="{connector}",task="{task}"</td></tr><tr><th style=width:90px></th><th>Attribute name</th><th>Description</th></tr><tr><td></td><td>deadletterqueue-produce-failures</td><td>The number of failed writes to the dead letter queue.</td></tr><tr><td></td><td>deadletterqueue-produce-requests</td><td>The number of attempted writes to the dead letter queue.</td></tr><tr><td></td><td>last-error-timestamp</td><td>The epoch timestamp when this task last encountered an error.</td></tr><tr><td></td><td>total-errors-logged</td><td>The number of errors that were logged.</td></tr><tr><td></td><td>total-record-errors</td><td>The number of record processing errors in this task.</td></tr><tr><td></td><td>total-record-failures</td><td>The number of record processing failures in this task.</td></tr><tr><td></td><td>total-records-skipped</td><td>The number of records skipped due to errors.</td></tr><tr><td></td><td>total-retries</td><td>The number of operations retried.</td></tr></tbody></table></p><h2 id=streams-monitoring>Streams Monitoring<a class=td-heading-self-link href=#streams-monitoring aria-label="Heading self-link"></a></h2><p>A Kafka Streams instance contains all the producer and consumer metrics as well as additional metrics specific to streams. By default Kafka Streams has metrics with two recording levels: debug and info. The debug level records all metrics, while the info level records only the thread-level metrics.</p><p>Note that the metrics have a 3-layer hierarchy. At the top level there are per-thread metrics. Each thread has tasks, with their own metrics. Each task has a number of processor nodes, with their own metrics. Each task also has a number of state stores and record caches, all with their own metrics.</p><p>Use the following configuration option to specify which metrics you want collected:</p><pre><code>metrics.recording.level=&quot;info&quot;
</code></pre><h3 id=thread-metrics>Thread Metrics<a class=td-heading-self-link href=#thread-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>All the following metrics have a recording level of <code>info</code>: Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>commit-latency-avg</td><td>The average execution time in ms for committing, across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>commit-latency-max</td><td>The maximum execution time in ms for committing across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>poll-latency-avg</td><td>The average execution time in ms for polling, across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>poll-latency-max</td><td>The maximum execution time in ms for polling across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>process-latency-avg</td><td>The average execution time in ms for processing, across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>process-latency-max</td><td>The maximum execution time in ms for processing across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>punctuate-latency-avg</td><td>The average execution time in ms for punctuating, across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>punctuate-latency-max</td><td>The maximum execution time in ms for punctuating across all running tasks of this thread.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>commit-rate</td><td>The average number of commits per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>commit-total</td><td>The total number of commit calls across all tasks.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>poll-rate</td><td>The average number of polls per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>poll-total</td><td>The total number of poll calls across all tasks.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>process-rate</td><td>The average number of process calls per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>process-total</td><td>The total number of process calls across all tasks.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>punctuate-rate</td><td>The average number of punctuates per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>punctuate-total</td><td>The total number of punctuate calls across all tasks.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>task-created-rate</td><td>The average number of newly created tasks per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>task-created-total</td><td>The total number of tasks created.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>task-closed-rate</td><td>The average number of tasks closed per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>task-closed-total</td><td>The total number of tasks closed.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>skipped-records-rate</td><td>The average number of skipped records per second.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr><tr><td>skipped-records-total</td><td>The total number of skipped records.</td><td>kafka.streams:type=stream-metrics,client-id=([-.\w]+)</td></tr></tbody></table><h3 id=task-metrics>Task Metrics<a class=td-heading-self-link href=#task-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>All the following metrics have a recording level of <code>debug</code>: Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>commit-latency-avg</td><td>The average commit time in ns for this task.</td><td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td></tr><tr><td>commit-latency-max</td><td>The maximum commit time in ns for this task.</td><td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td></tr><tr><td>commit-rate</td><td>The average number of commit calls per second.</td><td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td></tr><tr><td>commit-total</td><td>The total number of commit calls.</td><td>kafka.streams:type=stream-task-metrics,client-id=([-.\w]+),task-id=([-.\w]+)</td></tr></tbody></table><h3 id=processor-node-metrics>Processor Node Metrics<a class=td-heading-self-link href=#processor-node-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>All the following metrics have a recording level of <code>debug</code>: Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>process-latency-avg</td><td>The average process execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>process-latency-max</td><td>The maximum process execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>punctuate-latency-avg</td><td>The average punctuate execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>punctuate-latency-max</td><td>The maximum punctuate execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>create-latency-avg</td><td>The average create execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>create-latency-max</td><td>The maximum create execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>destroy-latency-avg</td><td>The average destroy execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>destroy-latency-max</td><td>The maximum destroy execution time in ns.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>process-rate</td><td>The average number of process operations per second.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>process-total</td><td>The total number of process operations called.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>punctuate-rate</td><td>The average number of punctuate operations per second.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>punctuate-total</td><td>The total number of punctuate operations called.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>create-rate</td><td>The average number of create operations per second.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>create-total</td><td>The total number of create operations called.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>destroy-rate</td><td>The average number of destroy operations per second.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>destroy-total</td><td>The total number of destroy operations called.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>forward-rate</td><td>The average rate of records being forwarded downstream, from source nodes only, per second.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr><tr><td>forward-total</td><td>The total number of of records being forwarded downstream, from source nodes only.</td><td>kafka.streams:type=stream-processor-node-metrics,client-id=([-.\w]+),task-id=([-.\w]+),processor-node-id=([-.\w]+)</td></tr></tbody></table><h3 id=state-store-metrics>State Store Metrics<a class=td-heading-self-link href=#state-store-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>All the following metrics have a recording level of <code>debug</code>. Note that the <code>store-scope</code> value is specified in <code>StoreSupplier#metricsScope()</code> for user&rsquo;s customized state stores; for built-in state stores, currently we have <code>in-memory-state</code>, <code>in-memory-lru-state</code>, <code>rocksdb-state</code> (for RocksDB backed key-value store), <code>rocksdb-window-state</code> (for RocksDB backed window store) and <code>rocksdb-session-state</code> (for RocksDB backed session store). Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>put-latency-avg</td><td>The average put execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-latency-max</td><td>The maximum put execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-if-absent-latency-avg</td><td>The average put-if-absent execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-if-absent-latency-max</td><td>The maximum put-if-absent execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>get-latency-avg</td><td>The average get execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>get-latency-max</td><td>The maximum get execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>delete-latency-avg</td><td>The average delete execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>delete-latency-max</td><td>The maximum delete execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-all-latency-avg</td><td>The average put-all execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-all-latency-max</td><td>The maximum put-all execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>all-latency-avg</td><td>The average all operation execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>all-latency-max</td><td>The maximum all operation execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>range-latency-avg</td><td>The average range execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>range-latency-max</td><td>The maximum range execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>flush-latency-avg</td><td>The average flush execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>flush-latency-max</td><td>The maximum flush execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>restore-latency-avg</td><td>The average restore execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>restore-latency-max</td><td>The maximum restore execution time in ns.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-rate</td><td>The average put rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-total</td><td>The total number of put calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-if-absent-rate</td><td>The average put-if-absent rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-if-absent-total</td><td>The total number of put-if-absent calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>get-rate</td><td>The average get rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>get-total</td><td>The total number of get calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>delete-rate</td><td>The average delete rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>delete-total</td><td>The total number of delete calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-all-rate</td><td>The average put-all rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>put-all-total</td><td>The total number of put-all calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>all-rate</td><td>The average all operation rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>all-total</td><td>The total number of all operation calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>range-rate</td><td>The average range rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>range-total</td><td>The total number of range calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>flush-rate</td><td>The average flush rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>flush-total</td><td>The total number of flush calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>restore-rate</td><td>The average restore rate for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr><tr><td>restore-total</td><td>The total number of restore calls for this store.</td><td>kafka.streams:type=stream-[store-scope]-metrics,client-id=([-.\w]+),task-id=([-.\w]+),[store-scope]-id=([-.\w]+)</td></tr></tbody></table><h3 id=record-cache-metrics>Record Cache Metrics<a class=td-heading-self-link href=#record-cache-metrics aria-label="Heading self-link"></a></h3><table><thead><tr><th>All the following metrics have a recording level of <code>debug</code>: Metric/Attribute name</th><th>Description</th><th>Mbean name</th></tr></thead><tbody><tr><td>hitRatio-avg</td><td>The average cache hit ratio defined as the ratio of cache read hits over the total cache read requests.</td><td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td></tr><tr><td>hitRatio-min</td><td>The mininum cache hit ratio.</td><td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td></tr><tr><td>hitRatio-max</td><td>The maximum cache hit ratio.</td><td>kafka.streams:type=stream-record-cache-metrics,client-id=([-.\w]+),task-id=([-.\w]+),record-cache-id=([-.\w]+)</td></tr></tbody></table><h2 id=others>Others<a class=td-heading-self-link href=#others aria-label="Heading self-link"></a></h2><p>We recommend monitoring GC time and other stats and various server stats such as CPU utilization, I/O service time, etc. On the client side, we recommend monitoring the message/byte rate (global and per topic), request rate/size/time, and on the consumer side, max lag in messages among all partitions and min fetch request rate. For a consumer to keep up, max lag needs to be less than a threshold and min fetch rate needs to be larger than 0.</p><h2 id=audit>Audit<a class=td-heading-self-link href=#audit aria-label="Heading self-link"></a></h2><p>The final alerting we do is on the correctness of the data delivery. We audit that every message that is sent is consumed by all consumers and measure the lag for this to occur. For important topics we alert if a certain completeness is not achieved in a certain time period. The details of this are discussed in KAFKA-260.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-932f586fb8e0eef80921a948e18121cc>7 - ZooKeeper</h1><div class=lead>ZooKeeper</div><h1 id=zookeeper>ZooKeeper<a class=td-heading-self-link href=#zookeeper aria-label="Heading self-link"></a></h1><h2 id=stable-version>Stable version<a class=td-heading-self-link href=#stable-version aria-label="Heading self-link"></a></h2><p>The current stable branch is 3.4 and the latest release of that branch is 3.4.9.</p><h2 id=operationalizing-zookeeper>Operationalizing ZooKeeper<a class=td-heading-self-link href=#operationalizing-zookeeper aria-label="Heading self-link"></a></h2><p>Operationally, we do the following for a healthy ZooKeeper installation:</p><ul><li>Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively. If you have a small deployment, then using 3 servers is acceptable, but keep in mind that you&rsquo;ll only be able to tolerate 1 server down in this case.</li><li>I/O segregation: if you do a lot of write type traffic you&rsquo;ll almost definitely want the transaction logs on a dedicated disk group. Writes to the transaction log are synchronous (but batched for performance), and consequently, concurrent writes can significantly affect performance. ZooKeeper snapshots can be one such a source of concurrent writes, and ideally should be written on a disk group separate from the transaction log. Snapshots are written to disk asynchronously, so it is typically ok to share with the operating system and message log files. You can configure a server to use a separate disk group with the dataLogDir parameter.</li><li>Application segregation: Unless you really understand the application patterns of other apps that you want to install on the same box, it can be a good idea to run ZooKeeper in isolation (though this can be a balancing act with the capabilities of the hardware).</li><li>Use care with virtualization: It can work, depending on your cluster layout and read/write patterns and SLAs, but the tiny overheads introduced by the virtualization layer can add up and throw off ZooKeeper, as it can be very time sensitive</li><li>ZooKeeper configuration: It&rsquo;s java, make sure you give it &rsquo;enough&rsquo; heap space (We usually run them with 3-5G, but that&rsquo;s mostly due to the data set size we have here). Unfortunately we don&rsquo;t have a good formula for it, but keep in mind that allowing for more ZooKeeper state means that snapshots can become large, and large snapshots affect recovery time. In fact, if the snapshot becomes too large (a few gigabytes), then you may need to increase the initLimit parameter to give enough time for servers to recover and join the ensemble.</li><li>Monitoring: Both JMX and the 4 letter words (4lw) commands are very useful, they do overlap in some cases (and in those cases we prefer the 4 letter commands, they seem more predictable, or at the very least, they work better with the LI monitoring infrastructure)</li><li>Don&rsquo;t overbuild the cluster: large clusters, especially in a write heavy usage pattern, means a lot of intracluster communication (quorums on the writes and subsequent cluster member updates), but don&rsquo;t underbuild it (and risk swamping the cluster). Having more servers adds to your read capacity.</li></ul><p>Overall, we try to keep the ZooKeeper system as small as will handle the load (plus standard growth capacity planning) and as simple as possible. We try not to do anything fancy with the configuration or application layout as compared to the official release as well as keep it as self contained as possible. For these reasons, we tend to skip the OS packaged versions, since it has a tendency to try to put things in the OS standard hierarchy, which can be &lsquo;messy&rsquo;, for want of a better way to word it.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>