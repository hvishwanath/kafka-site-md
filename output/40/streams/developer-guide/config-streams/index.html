<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Configuring a Streams Application | </title><meta name=description content='Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-first-streams-application"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-broker1:9092"); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.'><meta property="og:title" content="Configuring a Streams Application"><meta property="og:description" content='Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-first-streams-application"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-broker1:9092"); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.'><meta property="og:type" content="article"><meta property="og:url" content="https://example.kafka-site-md.dev/40/streams/developer-guide/config-streams/"><meta property="article:section" content="40"><meta property="article:modified_time" content="2025-03-28T16:24:11-07:00"><meta itemprop=name content="Configuring a Streams Application"><meta itemprop=description content='Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-first-streams-application"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-broker1:9092"); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.'><meta itemprop=dateModified content="2025-03-28T16:24:11-07:00"><meta itemprop=wordCount content="7033"><meta itemprop=keywords content="kafka,docs,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Configuring a Streams Application"><meta name=twitter:description content='Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-first-streams-application"); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka-broker1:9092"); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.'><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/41/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/41/>4.1</a></li><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e58a36913f949563db0a14b5eaf8f6a5.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><div id=content-mobile><form class="td-sidebar__search d-flex align-items-center"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e58a36913f949563db0a14b5eaf8f6a5.json data-offline-search-base-href=/ data-offline-search-max-results=10></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form></div><div id=content-desktop></div><nav class="td-sidebar-nav collapse td-sidebar-nav--search-disabled foldable-nav" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-40-li><a href=/40/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-40><span>AK 4.0.X</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40getting-started-li><input type=checkbox id=m-40getting-started-check>
<label for=m-40getting-started-check><a href=/40/getting-started/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40getting-started><span>Getting Started</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedintroduction-li><input type=checkbox id=m-40getting-startedintroduction-check>
<label for=m-40getting-startedintroduction-check><a href=/40/getting-started/introduction/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedintroduction><span>Introduction</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-starteduses-li><input type=checkbox id=m-40getting-starteduses-check>
<label for=m-40getting-starteduses-check><a href=/40/getting-started/uses/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-starteduses><span>Use Cases</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedquickstart-li><input type=checkbox id=m-40getting-startedquickstart-check>
<label for=m-40getting-startedquickstart-check><a href=/40/getting-started/quickstart/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedquickstart><span>Quick Start</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedecosystem-li><input type=checkbox id=m-40getting-startedecosystem-check>
<label for=m-40getting-startedecosystem-check><a href=/40/getting-started/ecosystem/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedecosystem><span>Ecosystem</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedupgrade-li><input type=checkbox id=m-40getting-startedupgrade-check>
<label for=m-40getting-startedupgrade-check><a href=/40/getting-started/upgrade/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedupgrade><span>Upgrading</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedzk2kraft-li><input type=checkbox id=m-40getting-startedzk2kraft-check>
<label for=m-40getting-startedzk2kraft-check><a href=/40/getting-started/zk2kraft/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedzk2kraft><span>KRaft vs ZooKeeper</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-startedcompatibility-li><input type=checkbox id=m-40getting-startedcompatibility-check>
<label for=m-40getting-startedcompatibility-check><a href=/40/getting-started/compatibility/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-startedcompatibility><span>Compatibility</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40getting-starteddocker-li><input type=checkbox id=m-40getting-starteddocker-check>
<label for=m-40getting-starteddocker-check><a href=/40/getting-started/docker/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40getting-starteddocker><span>Docker</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40apis-li><input type=checkbox id=m-40apis-check>
<label for=m-40apis-check><a href=/40/apis/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40apis><span>APIs</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40apisapi-li><input type=checkbox id=m-40apisapi-check>
<label for=m-40apisapi-check><a href=/40/apis/api/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40apisapi><span>API</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40configuration-li><input type=checkbox id=m-40configuration-check>
<label for=m-40configuration-check><a href=/40/configuration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40configuration><span>Configuration</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40configurationconfiguration-li><input type=checkbox id=m-40configurationconfiguration-check>
<label for=m-40configurationconfiguration-check><a href=/40/configuration/configuration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40configurationconfiguration><span>Configuration</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40design-li><input type=checkbox id=m-40design-check>
<label for=m-40design-check><a href=/40/design/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40design><span>Design</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40designdesign-li><input type=checkbox id=m-40designdesign-check>
<label for=m-40designdesign-check><a href=/40/design/design/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40designdesign><span>Design</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40designprotocol-li><input type=checkbox id=m-40designprotocol-check>
<label for=m-40designprotocol-check><a href=/40/design/protocol/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40designprotocol><span>Protocol</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40implementation-li><input type=checkbox id=m-40implementation-check>
<label for=m-40implementation-check><a href=/40/implementation/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40implementation><span>Implementation</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40implementationnetwork-layer-li><input type=checkbox id=m-40implementationnetwork-layer-check>
<label for=m-40implementationnetwork-layer-check><a href=/40/implementation/network-layer/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40implementationnetwork-layer><span>Network Layer</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40implementationmessages-li><input type=checkbox id=m-40implementationmessages-check>
<label for=m-40implementationmessages-check><a href=/40/implementation/messages/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40implementationmessages><span>Messages</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40implementationmessage-format-li><input type=checkbox id=m-40implementationmessage-format-check>
<label for=m-40implementationmessage-format-check><a href=/40/implementation/message-format/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40implementationmessage-format><span>Message Format</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40implementationlog-li><input type=checkbox id=m-40implementationlog-check>
<label for=m-40implementationlog-check><a href=/40/implementation/log/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40implementationlog><span>Log</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40implementationdistribution-li><input type=checkbox id=m-40implementationdistribution-check>
<label for=m-40implementationdistribution-check><a href=/40/implementation/distribution/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40implementationdistribution><span>Distribution</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40operations-li><input type=checkbox id=m-40operations-check>
<label for=m-40operations-check><a href=/40/operations/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40operations><span>Operations</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsbasic-kafka-operations-li><input type=checkbox id=m-40operationsbasic-kafka-operations-check>
<label for=m-40operationsbasic-kafka-operations-check><a href=/40/operations/basic-kafka-operations/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsbasic-kafka-operations><span>Basic Kafka Operations</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsdatacenters-li><input type=checkbox id=m-40operationsdatacenters-check>
<label for=m-40operationsdatacenters-check><a href=/40/operations/datacenters/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsdatacenters><span>Datacenters</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsgeo-replication-cross-cluster-data-mirroring-li><input type=checkbox id=m-40operationsgeo-replication-cross-cluster-data-mirroring-check>
<label for=m-40operationsgeo-replication-cross-cluster-data-mirroring-check><a href=/40/operations/geo-replication-cross-cluster-data-mirroring/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsgeo-replication-cross-cluster-data-mirroring><span>Geo-Replication (Cross-Cluster Data Mirroring)</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsmulti-tenancy-li><input type=checkbox id=m-40operationsmulti-tenancy-check>
<label for=m-40operationsmulti-tenancy-check><a href=/40/operations/multi-tenancy/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsmulti-tenancy><span>Multi-Tenancy</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsjava-version-li><input type=checkbox id=m-40operationsjava-version-check>
<label for=m-40operationsjava-version-check><a href=/40/operations/java-version/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsjava-version><span>Java Version</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationshardware-and-os-li><input type=checkbox id=m-40operationshardware-and-os-check>
<label for=m-40operationshardware-and-os-check><a href=/40/operations/hardware-and-os/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationshardware-and-os><span>Hardware and OS</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsmonitoring-li><input type=checkbox id=m-40operationsmonitoring-check>
<label for=m-40operationsmonitoring-check><a href=/40/operations/monitoring/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsmonitoring><span>Monitoring</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationskraft-li><input type=checkbox id=m-40operationskraft-check>
<label for=m-40operationskraft-check><a href=/40/operations/kraft/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationskraft><span>KRaft</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationstiered-storage-li><input type=checkbox id=m-40operationstiered-storage-check>
<label for=m-40operationstiered-storage-check><a href=/40/operations/tiered-storage/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationstiered-storage><span>Tiered Storage</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationsconsumer-rebalance-protocol-li><input type=checkbox id=m-40operationsconsumer-rebalance-protocol-check>
<label for=m-40operationsconsumer-rebalance-protocol-check><a href=/40/operations/consumer-rebalance-protocol/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationsconsumer-rebalance-protocol><span>Consumer Rebalance Protocol</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationstransaction-protocol-li><input type=checkbox id=m-40operationstransaction-protocol-check>
<label for=m-40operationstransaction-protocol-check><a href=/40/operations/transaction-protocol/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationstransaction-protocol><span>Transaction Protocol</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40operationseligible-leader-replicas-li><input type=checkbox id=m-40operationseligible-leader-replicas-check>
<label for=m-40operationseligible-leader-replicas-check><a href=/40/operations/eligible-leader-replicas/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40operationseligible-leader-replicas><span>Eligible Leader Replicas</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40security-li><input type=checkbox id=m-40security-check>
<label for=m-40security-check><a href=/40/security/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40security><span>Security</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securitysecurity-overview-li><input type=checkbox id=m-40securitysecurity-overview-check>
<label for=m-40securitysecurity-overview-check><a href=/40/security/security-overview/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securitysecurity-overview><span>Security Overview</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securitylistener-configuration-li><input type=checkbox id=m-40securitylistener-configuration-check>
<label for=m-40securitylistener-configuration-check><a href=/40/security/listener-configuration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securitylistener-configuration><span>Listener Configuration</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securityencryption-and-authentication-using-ssl-li><input type=checkbox id=m-40securityencryption-and-authentication-using-ssl-check>
<label for=m-40securityencryption-and-authentication-using-ssl-check><a href=/40/security/encryption-and-authentication-using-ssl/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securityencryption-and-authentication-using-ssl><span>Encryption and Authentication using SSL</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securityauthentication-using-sasl-li><input type=checkbox id=m-40securityauthentication-using-sasl-check>
<label for=m-40securityauthentication-using-sasl-check><a href=/40/security/authentication-using-sasl/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securityauthentication-using-sasl><span>Authentication using SASL</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securityauthorization-and-acls-li><input type=checkbox id=m-40securityauthorization-and-acls-check>
<label for=m-40securityauthorization-and-acls-check><a href=/40/security/authorization-and-acls/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securityauthorization-and-acls><span>Authorization and ACLs</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40securityincorporating-security-features-in-a-running-cluster-li><input type=checkbox id=m-40securityincorporating-security-features-in-a-running-cluster-check>
<label for=m-40securityincorporating-security-features-in-a-running-cluster-check><a href=/40/security/incorporating-security-features-in-a-running-cluster/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40securityincorporating-security-features-in-a-running-cluster><span>Incorporating Security Features in a Running Cluster</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-40kafka-connect-li><input type=checkbox id=m-40kafka-connect-check>
<label for=m-40kafka-connect-check><a href=/40/kafka-connect/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40kafka-connect><span>Kafka Connect</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40kafka-connectoverview-li><input type=checkbox id=m-40kafka-connectoverview-check>
<label for=m-40kafka-connectoverview-check><a href=/40/kafka-connect/overview/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40kafka-connectoverview><span>Overview</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40kafka-connectuser-guide-li><input type=checkbox id=m-40kafka-connectuser-guide-check>
<label for=m-40kafka-connectuser-guide-check><a href=/40/kafka-connect/user-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40kafka-connectuser-guide><span>User Guide</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40kafka-connectconnector-development-guide-li><input type=checkbox id=m-40kafka-connectconnector-development-guide-check>
<label for=m-40kafka-connectconnector-development-guide-check><a href=/40/kafka-connect/connector-development-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40kafka-connectconnector-development-guide><span>Connector Development Guide</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40kafka-connectadministration-li><input type=checkbox id=m-40kafka-connectadministration-check>
<label for=m-40kafka-connectadministration-check><a href=/40/kafka-connect/administration/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40kafka-connectadministration><span>Administration</span></a></label></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-40streams-li><input type=checkbox id=m-40streams-check checked>
<label for=m-40streams-check><a href=/40/streams/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40streams><span>Kafka Streams</span></a></label><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsintroduction-li><input type=checkbox id=m-40streamsintroduction-check>
<label for=m-40streamsintroduction-check><a href=/40/streams/introduction/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsintroduction><span>Introduction</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsquickstart-li><input type=checkbox id=m-40streamsquickstart-check>
<label for=m-40streamsquickstart-check><a href=/40/streams/quickstart/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsquickstart><span>Quick Start</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamstutorial-li><input type=checkbox id=m-40streamstutorial-check>
<label for=m-40streamstutorial-check><a href=/40/streams/tutorial/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamstutorial><span>Write a streams app</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamscore-concepts-li><input type=checkbox id=m-40streamscore-concepts-check>
<label for=m-40streamscore-concepts-check><a href=/40/streams/core-concepts/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamscore-concepts><span>Core Concepts</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsarchitecture-li><input type=checkbox id=m-40streamsarchitecture-check>
<label for=m-40streamsarchitecture-check><a href=/40/streams/architecture/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsarchitecture><span>Architecture</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsupgrade-guide-li><input type=checkbox id=m-40streamsupgrade-guide-check>
<label for=m-40streamsupgrade-guide-check><a href=/40/streams/upgrade-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsupgrade-guide><span>Upgrade Guide</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-40streamsdeveloper-guide-li><input type=checkbox id=m-40streamsdeveloper-guide-check checked>
<label for=m-40streamsdeveloper-guide-check><a href=/40/streams/developer-guide/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-40streamsdeveloper-guide><span>Streams Developer Guide</span></a></label><ul class="ul-3 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidewrite-streams-app-li><input type=checkbox id=m-40streamsdeveloper-guidewrite-streams-app-check>
<label for=m-40streamsdeveloper-guidewrite-streams-app-check><a href=/40/streams/developer-guide/write-streams-app/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidewrite-streams-app><span>Writing a Streams Application</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-40streamsdeveloper-guideconfig-streams-li><input type=checkbox id=m-40streamsdeveloper-guideconfig-streams-check checked>
<label for=m-40streamsdeveloper-guideconfig-streams-check><a href=/40/streams/developer-guide/config-streams/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guideconfig-streams><span class=td-sidebar-nav-active-item>Configuring a Streams Application</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidedsl-api-li><input type=checkbox id=m-40streamsdeveloper-guidedsl-api-check>
<label for=m-40streamsdeveloper-guidedsl-api-check><a href=/40/streams/developer-guide/dsl-api/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidedsl-api><span>Streams DSL</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guideprocessor-api-li><input type=checkbox id=m-40streamsdeveloper-guideprocessor-api-check>
<label for=m-40streamsdeveloper-guideprocessor-api-check><a href=/40/streams/developer-guide/processor-api/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guideprocessor-api><span>Processor API</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidedsl-topology-naming-li><input type=checkbox id=m-40streamsdeveloper-guidedsl-topology-naming-check>
<label for=m-40streamsdeveloper-guidedsl-topology-naming-check><a href=/40/streams/developer-guide/dsl-topology-naming/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidedsl-topology-naming><span>Naming Operators in a Streams DSL application</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidedatatypes-li><input type=checkbox id=m-40streamsdeveloper-guidedatatypes-check>
<label for=m-40streamsdeveloper-guidedatatypes-check><a href=/40/streams/developer-guide/datatypes/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidedatatypes><span>Data Types and Serialization</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidetesting-li><input type=checkbox id=m-40streamsdeveloper-guidetesting-check>
<label for=m-40streamsdeveloper-guidetesting-check><a href=/40/streams/developer-guide/testing/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidetesting><span>Testing a Streams Application</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guideinteractive-queries-li><input type=checkbox id=m-40streamsdeveloper-guideinteractive-queries-check>
<label for=m-40streamsdeveloper-guideinteractive-queries-check><a href=/40/streams/developer-guide/interactive-queries/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guideinteractive-queries><span>Interactive Queries</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidememory-mgmt-li><input type=checkbox id=m-40streamsdeveloper-guidememory-mgmt-check>
<label for=m-40streamsdeveloper-guidememory-mgmt-check><a href=/40/streams/developer-guide/memory-mgmt/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidememory-mgmt><span>Memory Management</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guiderunning-app-li><input type=checkbox id=m-40streamsdeveloper-guiderunning-app-check>
<label for=m-40streamsdeveloper-guiderunning-app-check><a href=/40/streams/developer-guide/running-app/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guiderunning-app><span>Running Streams Applications</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidemanage-topics-li><input type=checkbox id=m-40streamsdeveloper-guidemanage-topics-check>
<label for=m-40streamsdeveloper-guidemanage-topics-check><a href=/40/streams/developer-guide/manage-topics/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidemanage-topics><span>Managing Streams Application Topics</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guidesecurity-li><input type=checkbox id=m-40streamsdeveloper-guidesecurity-check>
<label for=m-40streamsdeveloper-guidesecurity-check><a href=/40/streams/developer-guide/security/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guidesecurity><span>Streams Security</span></a></label></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-40streamsdeveloper-guideapp-reset-tool-li><input type=checkbox id=m-40streamsdeveloper-guideapp-reset-tool-check>
<label for=m-40streamsdeveloper-guideapp-reset-tool-check><a href=/40/streams/developer-guide/app-reset-tool/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-40streamsdeveloper-guideapp-reset-tool><span>Application Reset Tool</span></a></label></li></ul></li></ul></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"><a href=https://github.com/apache/kafka-site//tree/markdown/content/en/40/streams/developer-guide/config-streams.md class="td-page-meta--view td-page-meta__view" target=_blank rel=noopener><i class="fa-solid fa-file-lines fa-fw"></i> View page source</a>
<a href=https://github.com/apache/kafka-site//edit/markdown/content/en/40/streams/developer-guide/config-streams.md class="td-page-meta--edit td-page-meta__edit" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Edit this page</a>
<a href="https://github.com/apache/kafka-site//new/markdown/content/en/40/streams/developer-guide?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class="td-page-meta--child td-page-meta__child" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Create child page</a>
<a href="https://github.com/apache/kafka-site//issues/new?title=Configuring%20a%20Streams%20Application" class="td-page-meta--issue td-page-meta__issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create documentation issue</a>
<a id=print href=/40/streams/developer-guide/_print/><i class="fa-solid fa-print fa-fw"></i> Print entire section</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#applicationid>application.id</a></li><li><a href=#bootstrapservers>bootstrap.servers</a></li></ul><ul><li><a href=#acks>acks</a></li><li><a href=#replicationfactor>replication.factor</a></li><li><a href=#mininsyncreplicas>min.insync.replicas</a></li><li><a href=#numstandbyreplicas>num.standby.replicas</a></li></ul><ul><li><a href=#acceptablerecoverylag>acceptable.recovery.lag</a></li><li><a href=#deserializationexceptionhandler-deprecated-defaultdeserializationexceptionhandler>deserialization.exception.handler (deprecated: default.deserialization.exception.handler)</a></li><li><a href=#productionexceptionhandler-deprecated-defaultproductionexceptionhandler>production.exception.handler (deprecated: default.production.exception.handler)</a></li><li><a href=#defaulttimestampextractor>default.timestamp.extractor</a></li><li><a href=#defaultkeyserde>default.key.serde</a></li><li><a href=#defaultvalueserde>default.value.serde</a></li><li><a href=#rackawareassignmentnon_overlap_cost>rack.aware.assignment.non_overlap_cost</a></li><li><a href=#rackawareassignmentstrategy>rack.aware.assignment.strategy</a></li><li><a href=#rackawareassignmenttags>rack.aware.assignment.tags</a></li><li><a href=#rackawareassignmenttraffic_cost>rack.aware.assignment.traffic_cost</a></li><li><a href=#logsummaryintervalms>log.summary.interval.ms</a></li><li><a href=#enablemetricspush>enable.metrics.push</a></li><li><a href=#maxtaskidlems>max.task.idle.ms</a></li><li><a href=#maxwarmupreplicas>max.warmup.replicas</a></li><li><a href=#numstandbyreplicas-1>num.standby.replicas</a></li><li><a href=#numstreamthreads>num.stream.threads</a></li><li><a href=#probingrebalanceintervalms>probing.rebalance.interval.ms</a></li><li><a href=#processingexceptionhandler>processing.exception.handler</a></li><li><a href=#processingguarantee>processing.guarantee</a></li><li><a href=#processorwrapperclass>processor.wrapper.class</a></li><li><a href=#replicationfactor-1>replication.factor</a></li><li><a href=#rocksdbconfigsetter>rocksdb.config.setter</a><ul><li></li><li><a href=#kafka-consumers-producer-and-admin-client-configuration-parameters>Kafka consumers, producer and admin client configuration parameters</a></li><li><a href=#parameters-controlled-by-kafka-streams>Parameters controlled by Kafka Streams</a></li><li><a href=#clientid>client.id</a></li></ul></li></ul></nav></div><div class="taxonomy taxonomy-terms-cloud taxo-tags"><h5 class=taxonomy-title>Tag Cloud</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/apis/ data-taxonomy-term=apis><span class=taxonomy-label>Apis</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/configuration/ data-taxonomy-term=configuration><span class=taxonomy-label>Configuration</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/design/ data-taxonomy-term=design><span class=taxonomy-label>Design</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/developer-guide/ data-taxonomy-term=developer-guide><span class=taxonomy-label>Developer-Guide</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/docs/ data-taxonomy-term=docs><span class=taxonomy-label>Docs</span><span class=taxonomy-count>1781</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/getting-started/ data-taxonomy-term=getting-started><span class=taxonomy-label>Getting-Started</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/implementation/ data-taxonomy-term=implementation><span class=taxonomy-label>Implementation</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/kafka/ data-taxonomy-term=kafka><span class=taxonomy-label>Kafka</span><span class=taxonomy-count>1781</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/ops/ data-taxonomy-term=ops><span class=taxonomy-label>Ops</span><span class=taxonomy-count>32</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/security/ data-taxonomy-term=security><span class=taxonomy-label>Security</span><span class=taxonomy-count>64</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/streams/ data-taxonomy-term=streams><span class=taxonomy-label>Streams</span><span class=taxonomy-count>64</span></a></li></ul></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class="pageinfo pageinfo-warning"><p>You are viewing documentation for an older version (4.0) of Kafka.
For up-to-date documentation, see the
<a href=/41/ target=_blank>latest version</a>.</p></div><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/40/>AK 4.0.X</a></li><li class=breadcrumb-item><a href=/40/streams/>Kafka Streams</a></li><li class=breadcrumb-item><a href=/40/streams/developer-guide/>Streams Developer Guide</a></li><li class="breadcrumb-item active" aria-current=page>Configuring a Streams Application</li></ol></nav><div class=td-content><h1>Configuring a Streams Application</h1><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-tags"><h5 class=taxonomy-title>Tags:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/kafka/ data-taxonomy-term=kafka><span class=taxonomy-label>Kafka</span></a></li><li><a class=taxonomy-term href=https://example.kafka-site-md.dev/tags/docs/ data-taxonomy-term=docs><span class=taxonomy-label>Docs</span></a></li></ul></div></header><h1 id=configuring-a-streams-application>Configuring a Streams Application<a class=td-heading-self-link href=#configuring-a-streams-application aria-label="Heading self-link"></a></h1><p>Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a <code>java.util.Properties</code> instance.</p><ol><li><p>Create a <code>java.util.Properties</code> instance.</p></li><li><p>Set the parameters. For example:</p><p>import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;</p></li></ol><pre><code>Properties settings = new Properties();
// Set a few key parameters
settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;my-first-streams-application&quot;);
settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;kafka-broker1:9092&quot;);
// Any further settings
settings.put(... , ...);
</code></pre><h1 id=configuration-parameter-reference>Configuration parameter reference<a class=td-heading-self-link href=#configuration-parameter-reference aria-label="Heading self-link"></a></h1><p>This section contains the most common Streams configuration parameters. For a full reference, see the <a href=/40/javadoc/org/apache/kafka/streams/StreamsConfig.html>Streams</a> Javadocs.</p><ul><li>Required configuration parameters<ul><li>application.id</li><li>bootstrap.servers</li></ul></li><li>Recommended configuration parameters for resiliency<ul><li>acks</li><li>replication.factor</li><li>min.insync.replicas</li><li>num.standby.replicas</li></ul></li><li>Optional configuration parameters<ul><li>acceptable.recovery.lag</li><li>default.deserialization.exception.handler (deprecated since 4.0)</li><li>default.key.serde</li><li>default.production.exception.handler (deprecated since 4.0)</li><li>default.timestamp.extractor</li><li>default.value.serde</li><li>deserialization.exception.handler</li><li>enable.metrics.push</li><li>log.summary.interval.ms</li><li>max.task.idle.ms</li><li>max.warmup.replicas</li><li>num.standby.replicas</li><li>num.stream.threads</li><li>probing.rebalance.interval.ms</li><li>processing.exception.handler</li><li>processing.guarantee</li><li>processor.wrapper.class</li><li>production.exception.handler</li><li>rack.aware.assignment.non_overlap_cost</li><li>rack.aware.assignment.strategy</li><li>rack.aware.assignment.tags</li><li>rack.aware.assignment.traffic_cost</li><li>replication.factor</li><li>rocksdb.config.setter</li><li>state.dir</li><li>task.assignor.class</li><li>topology.optimization</li><li>windowed.inner.class.serde</li></ul></li><li>Kafka consumers and producer configuration parameters<ul><li>Naming</li><li>Default Values</li><li>Parameters controlled by Kafka Streams</li><li>enable.auto.commit</li></ul></li></ul><h1 id=required-configuration-parameters>Required configuration parameters<a class=td-heading-self-link href=#required-configuration-parameters aria-label="Heading self-link"></a></h1><p>Here are the required Streams configuration parameters.</p><table><thead><tr><th>Parameter Name</th><th>Importance</th><th>Description</th><th>Default Value</th></tr></thead><tbody><tr><td>application.id</td><td>Required</td><td>An identifier for the stream processing application. Must be unique within the Kafka cluster.</td><td>None</td></tr><tr><td>bootstrap.servers</td><td>Required</td><td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.</td><td>None</td></tr></tbody></table><h2 id=applicationid>application.id<a class=td-heading-self-link href=#applicationid aria-label="Heading self-link"></a></h2><blockquote><p>(Required) The application ID. Each stream processing application must have a unique ID. The same ID must be given to all instances of the application. It is recommended to use only alphanumeric characters, <code>.</code> (dot), <code>-</code> (hyphen), and <code>_</code> (underscore). Examples: <code>"hello_world"</code>, <code>"hello_world-v1.0.0"</code></p><p>This ID is used in the following places to isolate resources used by the application from others:</p><ul><li>As the default Kafka consumer and producer <code>client.id</code> prefix</li><li>As the Kafka consumer <code>group.id</code> for coordination</li><li>As the name of the subdirectory in the state directory (cf. <code>state.dir</code>)</li><li>As the prefix of internal Kafka topic names</li></ul></blockquote><blockquote><p>Tip:
When an application is updated, the <code>application.id</code> should be changed unless you want to reuse the existing data in internal topics and state stores. For example, you could embed the version information within <code>application.id</code>, as <code>my-app-v1.0.0</code> and <code>my-app-v1.0.2</code>.</p></blockquote><h2 id=bootstrapservers>bootstrap.servers<a class=td-heading-self-link href=#bootstrapservers aria-label="Heading self-link"></a></h2><blockquote><p>(Required) The Kafka bootstrap servers. This is the same <a href=/40/documentation.html#producerconfigs>setting</a> that is used by the underlying producer and consumer clients to connect to the Kafka cluster. Example: <code>"kafka-broker1:9092,kafka-broker2:9092"</code>.</p></blockquote><h1 id=recommended-configuration-parameters-for-resiliency>Recommended configuration parameters for resiliency<a class=td-heading-self-link href=#recommended-configuration-parameters-for-resiliency aria-label="Heading self-link"></a></h1><p>There are several Kafka and Kafka Streams configuration options that need to be configured explicitly for resiliency in face of broker failures:</p><table><thead><tr><th>Parameter Name</th><th>Corresponding Client</th><th>Default value</th><th>Consider setting to</th></tr></thead><tbody><tr><td>acks</td><td>Producer (for version &lt;=2.8)</td><td><code>acks="1")</code></td><td><code>acks="all"</code></td></tr><tr><td>replication.factor (for broker version 2.3 or older)</td><td>Streams</td><td><code>-1</code></td><td><code>3</code> (broker 2.4+: ensure broker config <code>default.replication.factor=3</code>)</td></tr><tr><td>min.insync.replicas</td><td>Broker</td><td><code>1</code></td><td><code>2</code></td></tr><tr><td>num.standby.replicas</td><td>Streams</td><td><code>0</code></td><td><code>1</code></td></tr></tbody></table><p>Increasing the replication factor to 3 ensures that the internal Kafka Streams topic can tolerate up to 2 broker failures. The tradeoff from moving to the default values to the recommended ones is that some performance and more storage space (3x with the replication factor of 3) are sacrificed for more resiliency.</p><h2 id=acks>acks<a class=td-heading-self-link href=#acks aria-label="Heading self-link"></a></h2><blockquote><p>The number of acknowledgments that the leader must have received before considering a request complete. This controls the durability of records that are sent. The possible values are:</p><ul><li><code>acks="0"</code> The producer does not wait for acknowledgment from the server and the record is immediately added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the producer won&rsquo;t generally know of any failures. The offset returned for each record will always be set to <code>-1</code>.</li><li><code>acks="1"</code> The leader writes the record to its local log and responds without waiting for full acknowledgement from all followers. If the leader immediately fails after acknowledging the record, but before the followers have replicated it, then the record will be lost.</li><li><code>acks="all"</code> (default since 3.0 release) The leader waits for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost if there is at least one in-sync replica alive. This is the strongest available guarantee.</li></ul></blockquote><blockquote><p>For more information, see the <a href=https://kafka.apache.org/#producerconfigs>Kafka Producer documentation</a>.</p></blockquote><h2 id=replicationfactor>replication.factor<a class=td-heading-self-link href=#replicationfactor aria-label="Heading self-link"></a></h2><blockquote><p>See the description here.</p></blockquote><h2 id=mininsyncreplicas>min.insync.replicas<a class=td-heading-self-link href=#mininsyncreplicas aria-label="Heading self-link"></a></h2><p>The minimum number of in-sync replicas available for replication if the producer is configured with <code>acks="all"</code> (see <a href=/40/#topicconfigs_min.insync.replicas>topic configs</a>).</p><h2 id=numstandbyreplicas>num.standby.replicas<a class=td-heading-self-link href=#numstandbyreplicas aria-label="Heading self-link"></a></h2><blockquote><p>See the description here.</p></blockquote><pre><code>Properties streamsSettings = new Properties();
// for broker version 2.3 or older
//streamsSettings.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
// for version 2.8 or older
//streamsSettings.put(StreamsConfig.producerPrefix(ProducerConfig.ACKS_CONFIG), &quot;all&quot;);
streamsSettings.put(StreamsConfig.topicPrefix(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG), 2);
streamsSettings.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
</code></pre><h1 id=optional-configuration-parameters>Optional configuration parameters<a class=td-heading-self-link href=#optional-configuration-parameters aria-label="Heading self-link"></a></h1><p>Here are the optional <a href=/40/javadoc/org/apache/kafka/streams/StreamsConfig.html>Streams</a> javadocs, sorted by level of importance:</p><blockquote><ul><li>High: These are parameters with a default value which is most likely not a good fit for production use. It&rsquo;s highly recommended to revisit these parameters for production usage.</li><li>Medium: The default values of these parameters should work for production for many cases, but it&rsquo;s not uncommon that they are changed, for example to tune performance.</li><li>Low: It should rarely be necessary to change the value for these parameters. It&rsquo;s only recommended to change them if there is a very specific issue you want to address.</li></ul></blockquote><table><thead><tr><th>Parameter Name</th><th>Importance</th><th>Description</th><th>Default Value</th></tr></thead><tbody><tr><td>acceptable.recovery.lag</td><td>Medium</td><td>The maximum acceptable lag (number of offsets to catch up) for an instance to be considered caught-up and ready for the active task.</td><td><code>10000</code></td></tr><tr><td>application.server</td><td>Low</td><td>A host:port pair pointing to an embedded user defined endpoint that can be used for discovering the locations of state stores within a single Kafka Streams application. The value of this must be different for each instance of the application.</td><td>the empty string</td></tr><tr><td>buffered.records.per.partition</td><td>Low</td><td>The maximum number of records to buffer per partition.</td><td><code>1000</code></td></tr><tr><td>statestore.cache.max.bytes</td><td>Medium</td><td>Maximum number of memory bytes to be used for record caches across all threads.</td><td><code>10485760</code></td></tr><tr><td>cache.max.bytes.buffering (Deprecated. Use statestore.cache.max.bytes instead.)</td><td>Medium</td><td>Maximum number of memory bytes to be used for record caches across all threads.</td><td><code>10485760</code></td></tr><tr><td>client.id</td><td>Medium</td><td>An ID string to pass to the server when making requests. (This setting is passed to the consumer/producer clients used internally by Kafka Streams.)</td><td>the empty string</td></tr><tr><td>commit.interval.ms</td><td>Low</td><td>The frequency in milliseconds with which to save the position (offsets in source topics) of tasks.</td><td><code>30000</code> (30 seconds)</td></tr><tr><td>default.deserialization.exception.handler (Deprecated. Use deserialization.exception.handler instead.)</td><td>Medium</td><td>Exception handling class that implements the <code>DeserializationExceptionHandler</code> interface.</td><td><code>LogAndContinueExceptionHandler</code></td></tr><tr><td>default.key.serde</td><td>Medium</td><td>Default serializer/deserializer class for record keys, implements the <code>Serde</code> interface. Must be set by the user or all serdes must be passed in explicitly (see also default.value.serde).</td><td><code>null</code></td></tr><tr><td>default.production.exception.handler (Deprecated. Use production.exception.handler instead.)</td><td>Medium</td><td>Exception handling class that implements the <code>ProductionExceptionHandler</code> interface.</td><td><code>DefaultProductionExceptionHandler</code></td></tr><tr><td>default.timestamp.extractor</td><td>Medium</td><td>Timestamp extractor class that implements the <code>TimestampExtractor</code> interface. See Timestamp Extractor</td><td><code>FailOnInvalidTimestamp</code></td></tr><tr><td>default.value.serde</td><td>Medium</td><td>Default serializer/deserializer class for record values, implements the <code>Serde</code> interface. Must be set by the user or all serdes must be passed in explicitly (see also default.key.serde).</td><td><code>null</code></td></tr><tr><td>default.dsl.store</td><td>Low</td><td>[DEPRECATED] The default state store type used by DSL operators. Deprecated in favor of <code>dsl.store.suppliers.class</code></td><td><code>"ROCKS_DB"</code></td></tr><tr><td>deserialization.exception.handler</td><td>Medium</td><td>Exception handling class that implements the <code>DeserializationExceptionHandler</code> interface.</td><td><code>LogAndContinueExceptionHandler</code></td></tr><tr><td>dsl.store.suppliers.class</td><td>Low</td><td>Defines a default state store implementation to be used by any stateful DSL operator that has not explicitly configured the store implementation type. Must implement the <code>org.apache.kafka.streams.state.DslStoreSuppliers</code> interface.</td><td><code>BuiltInDslStoreSuppliers.RocksDBDslStoreSuppliers</code></td></tr><tr><td>log.summary.interval.ms</td><td>Low</td><td>The output interval in milliseconds for logging summary information (disabled if negative).</td><td><code>120000</code> (2 minutes)</td></tr><tr><td>enable.metrics.push</td><td>Low</td><td>Whether to enable pushing of client metrics to the cluster, if the cluster has a client metrics subscription which matches this client.</td><td><code>true</code></td></tr><tr><td>max.task.idle.ms</td><td>Medium</td><td>This config controls whether joins and merges may produce out-of-order results. The config value is the maximum amount of time in milliseconds a stream task will stay idle when it is fully caught up on some (but not all) input partitions to wait for producers to send additional records and avoid potential out-of-order record processing across multiple input streams. The default (zero) does not wait for producers to send more records, but it does wait to fetch data that is already present on the brokers. This default means that for records that are already present on the brokers, Streams will process them in timestamp order. Set to -1 to disable idling entirely and process any locally available data, even though doing so may produce out-of-order processing.</td><td><code>0</code></td></tr><tr><td>max.warmup.replicas</td><td>Medium</td><td>The maximum number of warmup replicas (extra standbys beyond the configured num.standbys) that can be assigned at once.</td><td><code>2</code></td></tr><tr><td>metric.reporters</td><td>Low</td><td>A list of classes to use as metrics reporters.</td><td>the empty list</td></tr><tr><td>metrics.num.samples</td><td>Low</td><td>The number of samples maintained to compute metrics.</td><td><code>2</code></td></tr><tr><td>metrics.recording.level</td><td>Low</td><td>The highest recording level for metrics.</td><td><code>INFO</code></td></tr><tr><td>metrics.sample.window.ms</td><td>Low</td><td>The window of time in milliseconds a metrics sample is computed over.</td><td><code>30000</code> (30 seconds)</td></tr><tr><td>num.standby.replicas</td><td>High</td><td>The number of standby replicas for each task.</td><td><code>0</code></td></tr><tr><td>num.stream.threads</td><td>Medium</td><td>The number of threads to execute stream processing.</td><td><code>1</code></td></tr><tr><td>probing.rebalance.interval.ms</td><td>Low</td><td>The maximum time in milliseconds to wait before triggering a rebalance to probe for warmup replicas that have sufficiently caught up.</td><td><code>600000</code> (10 minutes)</td></tr><tr><td>processing.exception.handler</td><td>Medium</td><td>Exception handling class that implements the <code>ProcessingExceptionHandler</code> interface.</td><td><code>LogAndFailProcessingExceptionHandler</code></td></tr><tr><td>processing.guarantee</td><td>Medium</td><td>The processing mode. Can be either <code>"at_least_once"</code> or <code>"exactly_once_v2"</code> (for EOS version 2, requires broker version 2.5+). See Processing Guarantee..</td><td><code>"at_least_once"</code></td></tr><tr><td>processor.wrapper.class</td><td>Medium</td><td>A class or class name implementing the <code>ProcessorWrapper</code> interface. Must be passed in when creating the topology, and will not be applied unless passed in to the appropriate constructor as a TopologyConfig. You should use the <code>StreamsBuilder#new(TopologyConfig)</code> constructor for DSL applications, and the <code>Topology#new(TopologyConfig)</code> constructor for PAPI applications.</td><td></td></tr><tr><td>production.exception.handler</td><td>Medium</td><td>Exception handling class that implements the <code>ProductionExceptionHandler</code> interface.</td><td><code>DefaultProductionExceptionHandler</code></td></tr><tr><td>poll.ms</td><td>Low</td><td>The amount of time in milliseconds to block waiting for input.</td><td><code>100</code></td></tr><tr><td>rack.aware.assignment.strategy</td><td>Low</td><td>The strategy used for rack aware assignment. Acceptable value are <code>"none"</code> (default), <code>"min_traffic"</code>, and <code>"balance_suttopology"</code>. See Rack Aware Assignment Strategy.</td><td><code>"none"</code></td></tr><tr><td>List of tag keys used to distribute standby replicas across Kafka Streams clients. When configured, Kafka Streams will make a best-effort to distribute the standby tasks over clients with different tag values. See Rack Aware Assignment Tags. the empty list</td><td></td><td></td><td></td></tr><tr><td>rack.aware.assignment.non_overlap_cost</td><td>Low</td><td>Cost associated with moving tasks from existing assignment. See Rack Aware Assignment Non-Overlap-Cost.</td><td><code>null</code></td></tr><tr><td>rack.aware.assignment.non_overlap_cost</td><td>Low</td><td>Cost associated with cross rack traffic. See Rack Aware Assignment Traffic-Cost.</td><td><code>null</code></td></tr><tr><td>replication.factor</td><td>Medium</td><td>The replication factor for changelog topics and repartition topics created by the application. The default of <code>-1</code> (meaning: use broker default replication factor) requires broker version 2.4 or newer.</td><td><code>-1</code></td></tr><tr><td>retry.backoff.ms</td><td>Low</td><td>The amount of time in milliseconds, before a request is retried.</td><td><code>100</code></td></tr><tr><td>rocksdb.config.setter</td><td>Medium</td><td>The RocksDB configuration.</td><td><code>null</code></td></tr><tr><td>state.cleanup.delay.ms</td><td>Low</td><td>The amount of time in milliseconds to wait before deleting state when a partition has migrated.</td><td><code>600000</code> (10 minutes)</td></tr><tr><td>state.dir</td><td>High</td><td>Directory location for state stores.</td><td><code>/${java.io.tmpdir}/kafka-streams</code></td></tr><tr><td>task.assignor.class</td><td>Medium</td><td>A task assignor class or class name implementing the <code>TaskAssignor</code> interface.</td><td>The high-availability task assignor.</td></tr><tr><td>task.timeout.ms</td><td>Medium</td><td>The maximum amount of time in milliseconds a task might stall due to internal errors and retries until an error is raised. For a timeout of <code>0 ms</code>, a task would raise an error for the first internal error. For any timeout larger than <code>0 ms</code>, a task will retry at least once before an error is raised.</td><td><code>300000</code> (5 minutes)</td></tr><tr><td>topology.optimization</td><td>Medium</td><td>A configuration telling Kafka Streams if it should optimize the topology and what optimizations to apply. Acceptable values are: <code>StreamsConfig.NO_OPTIMIZATION</code> (<code>none</code>), <code>StreamsConfig.OPTIMIZE</code> (<code>all</code>) or a comma separated list of specific optimizations: <code>StreamsConfig.REUSE_KTABLE_SOURCE_TOPICS</code> (<code>reuse.ktable.source.topics</code>), <code>StreamsConfig.MERGE_REPARTITION_TOPICS</code> (<code>merge.repartition.topics</code>), <code>StreamsConfig.SINGLE_STORE_SELF_JOIN</code> (<code>single.store.self.join</code>).</td><td><code>"NO_OPTIMIZATION"</code></td></tr><tr><td>upgrade.from</td><td>Medium</td><td>The version you are upgrading from during a rolling upgrade. See Upgrade From</td><td><code>null</code></td></tr><tr><td>windowstore.changelog.additional.retention.ms</td><td>Low</td><td>Added to a windows maintainMs to ensure data is not deleted from the log prematurely. Allows for clock drift.</td><td><code>86400000</code> (1 day)</td></tr><tr><td>window.size.ms</td><td>Low</td><td>Sets window size for the deserializer in order to calculate window end times.</td><td><code>null</code></td></tr></tbody></table><h2 id=acceptablerecoverylag>acceptable.recovery.lag<a class=td-heading-self-link href=#acceptablerecoverylag aria-label="Heading self-link"></a></h2><blockquote><p>The maximum acceptable lag (total number of offsets to catch up from the changelog) for an instance to be considered caught-up and able to receive an active task. Streams will only assign stateful active tasks to instances whose state stores are within the acceptable recovery lag, if any exist, and assign warmup replicas to restore state in the background for instances that are not yet caught up. Should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.</p><p>Note: if you set this to <code>Long.MAX_VALUE</code> it effectively disables the warmup replicas and task high availability, allowing Streams to immediately produce a balanced assignment and migrate tasks to a new instance without first warming them up.</p></blockquote><h2 id=deserializationexceptionhandler-deprecated-defaultdeserializationexceptionhandler>deserialization.exception.handler (deprecated: default.deserialization.exception.handler)<a class=td-heading-self-link href=#deserializationexceptionhandler-deprecated-defaultdeserializationexceptionhandler aria-label="Heading self-link"></a></h2><blockquote><p>The deserialization exception handler allows you to manage record exceptions that fail to deserialize. This can be caused by corrupt data, incorrect serialization logic, or unhandled record types. The implemented exception handler needs to return a <code>FAIL</code> or <code>CONTINUE</code> depending on the record and the exception thrown. Returning <code>FAIL</code> will signal that Streams should shut down and <code>CONTINUE</code> will signal that Streams should ignore the issue and continue processing. The following library built-in exception handlers are available:</p><ul><li><a href=/40/javadoc/org/apache/kafka/streams/errors/LogAndContinueExceptionHandler.html>LogAndContinueExceptionHandler</a>: This handler logs the deserialization exception and then signals the processing pipeline to continue processing more records. This log-and-skip strategy allows Kafka Streams to make progress instead of failing if there are records that fail to deserialize.</li><li><a href=/40/javadoc/org/apache/kafka/streams/errors/LogAndFailExceptionHandler.html>LogAndFailExceptionHandler</a>. This handler logs the deserialization exception and then signals the processing pipeline to stop processing more records.</li></ul></blockquote><blockquote><p>You can also provide your own customized exception handler besides the library provided ones to meet your needs. For example, you can choose to forward corrupt records into a quarantine topic (think: a &ldquo;dead letter queue&rdquo;) for further processing. To do this, use the Producer API to write a corrupted record directly to the quarantine topic. To be more concrete, you can create a separate <code>KafkaProducer</code> object outside the Streams client, and pass in this object as well as the dead letter queue topic name into the <code>Properties</code> map, which then can be retrieved from the <code>configure</code> function call. The drawback of this approach is that &ldquo;manual&rdquo; writes are side effects that are invisible to the Kafka Streams runtime library, so they do not benefit from the end-to-end processing guarantees of the Streams API:</p><pre><code>public class SendToDeadLetterQueueExceptionHandler implements DeserializationExceptionHandler {
    KafkaProducer&lt;byte[], byte[]&gt; dlqProducer;
    String dlqTopic;

    @Override
    public DeserializationHandlerResponse handle(final ErrorHandlerContext context,
                                                 final ConsumerRecord&lt;byte[], byte[]&gt; record,
                                                 final Exception exception) {

        log.warn(&quot;Exception caught during Deserialization, sending to the dead queue topic; &quot; +
            &quot;taskId: {}, topic: {}, partition: {}, offset: {}&quot;,
            context.taskId(), record.topic(), record.partition(), record.offset(),
            exception);

        dlqProducer.send(new ProducerRecord&lt;&gt;(dlqTopic, record.timestamp(), record.key(), record.value(), record.headers())).get();

        return DeserializationHandlerResponse.CONTINUE;
    }

    @Override
    public void configure(final Map&lt;String, ?&gt; configs) {
        dlqProducer = .. // get a producer from the configs map
        dlqTopic = .. // get the topic name from the configs map
    }
}
</code></pre></blockquote><h2 id=productionexceptionhandler-deprecated-defaultproductionexceptionhandler>production.exception.handler (deprecated: default.production.exception.handler)<a class=td-heading-self-link href=#productionexceptionhandler-deprecated-defaultproductionexceptionhandler aria-label="Heading self-link"></a></h2><blockquote><p>The production exception handler allows you to manage exceptions triggered when trying to interact with a broker such as attempting to produce a record that is too large. By default, Kafka provides and uses the <a href=/40/javadoc/org/apache/kafka/streams/errors/DefaultProductionExceptionHandler.html>DefaultProductionExceptionHandler</a> that always fails when these exceptions occur.</p><p>An exception handler can return <code>FAIL</code>, <code>CONTINUE</code>, or <code>RETRY</code> depending on the record and the exception thrown. Returning <code>FAIL</code> will signal that Streams should shut down. <code>CONTINUE</code> will signal that Streams should ignore the issue and continue processing. For <code>RetriableException</code> the handler may return <code>RETRY</code> to tell the runtime to retry sending the failed record (<strong>Note:</strong> If <code>RETRY</code> is returned for a non-<code>RetriableException</code> it will be treated as <code>FAIL</code>.) If you want to provide an exception handler that always ignores records that are too large, you could implement something like the following:</p><pre><code>import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.common.errors.RecordTooLargeException;
import org.apache.kafka.streams.errors.ProductionExceptionHandler;
import org.apache.kafka.streams.errors.ProductionExceptionHandler.ProductionExceptionHandlerResponse;

public class IgnoreRecordTooLargeHandler implements ProductionExceptionHandler {
    public void configure(Map&lt;String, Object&gt; config) {}

    public ProductionExceptionHandlerResponse handle(final ErrorHandlerContext context,
                                                     final ProducerRecord&lt;byte[], byte[]&gt; record,
                                                     final Exception exception) {
        if (exception instanceof RecordTooLargeException) {
            return ProductionExceptionHandlerResponse.CONTINUE;
        } else {
            return ProductionExceptionHandlerResponse.FAIL;
        }
    }
}

Properties settings = new Properties();

// other various kafka streams settings, e.g. bootstrap servers, application id, etc

settings.put(StreamsConfig.PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG,
             IgnoreRecordTooLargeHandler.class);
</code></pre></blockquote><h2 id=defaulttimestampextractor>default.timestamp.extractor<a class=td-heading-self-link href=#defaulttimestampextractor aria-label="Heading self-link"></a></h2><blockquote><p>A timestamp extractor pulls a timestamp from an instance of <a href=/40/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html>ConsumerRecord</a>. Timestamps are used to control the progress of streams.</p><p>The default extractor is <a href=/40/javadoc/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.html>FailOnInvalidTimestamp</a>. This extractor retrieves built-in timestamps that are automatically embedded into Kafka messages by the Kafka producer client since <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message>Kafka version 0.10</a>. Depending on the setting of Kafka&rsquo;s server-side <code>log.message.timestamp.type</code> broker and <code>message.timestamp.type</code> topic parameters, this extractor provides you with:</p><ul><li><strong>event-time</strong> processing semantics if <code>log.message.timestamp.type</code> is set to <code>CreateTime</code> aka &ldquo;producer time&rdquo; (which is the default). This represents the time when a Kafka producer sent the original message. If you use Kafka&rsquo;s official producer client, the timestamp represents milliseconds since the epoch.</li><li><strong>ingestion-time</strong> processing semantics if <code>log.message.timestamp.type</code> is set to <code>LogAppendTime</code> aka &ldquo;broker time&rdquo;. This represents the time when the Kafka broker received the original message, in milliseconds since the epoch.</li></ul></blockquote><blockquote><p>The <code>FailOnInvalidTimestamp</code> extractor throws an exception if a record contains an invalid (i.e. negative) built-in timestamp, because Kafka Streams would not process this record but silently drop it. Invalid built-in timestamps can occur for various reasons: if for example, you consume a topic that is written to by pre-0.10 Kafka producer clients or by third-party producer clients that don&rsquo;t support the new Kafka 0.10 message format yet; another situation where this may happen is after upgrading your Kafka cluster from <code>0.9</code> to <code>0.10</code>, where all the data that was generated with <code>0.9</code> does not include the <code>0.10</code> message timestamps.</p><p>If you have data with invalid timestamps and want to process it, then there are two alternative extractors available. Both work on built-in timestamps, but handle invalid timestamps differently.</p><ul><li><a href=/40/javadoc/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestamp.html>LogAndSkipOnInvalidTimestamp</a>: This extractor logs a warn message and returns the invalid timestamp to Kafka Streams, which will not process but silently drop the record. This log-and-skip strategy allows Kafka Streams to make progress instead of failing if there are records with an invalid built-in timestamp in your input data.</li><li><a href=/40/javadoc/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestamp.html>UsePartitionTimeOnInvalidTimestamp</a>. This extractor returns the record&rsquo;s built-in timestamp if it is valid (i.e. not negative). If the record does not have a valid built-in timestamps, the extractor returns the previously extracted valid timestamp from a record of the same topic partition as the current record as a timestamp estimation. In case that no timestamp can be estimated, it throws an exception.</li></ul></blockquote><blockquote><p>Another built-in extractor is <a href=/40/javadoc/org/apache/kafka/streams/processor/WallclockTimestampExtractor.html>WallclockTimestampExtractor</a>. This extractor does not actually &ldquo;extract&rdquo; a timestamp from the consumed record but rather returns the current time in milliseconds from the system clock (think: <code>System.currentTimeMillis()</code>), which effectively means Streams will operate on the basis of the so-called <strong>processing-time</strong> of events.</p><p>You can also provide your own timestamp extractors, for instance to retrieve timestamps embedded in the payload of messages. If you cannot extract a valid timestamp, you can either throw an exception, return a negative timestamp, or estimate a timestamp. Returning a negative timestamp will result in data loss - the corresponding record will not be processed but silently dropped. If you want to estimate a new timestamp, you can use the value provided via <code>previousTimestamp</code> (i.e., a Kafka Streams timestamp estimation). Here is an example of a custom <code>TimestampExtractor</code> implementation:</p><pre><code>import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;

// Extracts the embedded timestamp of a record (giving you &quot;event-time&quot; semantics).
public class MyEventTimeExtractor implements TimestampExtractor {

  @Override
  public long extract(final ConsumerRecord&lt;Object, Object&gt; record, final long previousTimestamp) {
    // `Foo` is your own custom class, which we assume has a method that returns
    // the embedded timestamp (milliseconds since midnight, January 1, 1970 UTC).
    long timestamp = -1;
    final Foo myPojo = (Foo) record.value();
    if (myPojo != null) {
      timestamp = myPojo.getTimestampInMillis();
    }
    if (timestamp &lt; 0) {
      // Invalid timestamp!  Attempt to estimate a new timestamp,
      // otherwise fall back to wall-clock time (processing-time).
      if (previousTimestamp &gt;= 0) {
        return previousTimestamp;
      } else {
        return System.currentTimeMillis();
      }
    }
  }

}
</code></pre><p>You would then define the custom timestamp extractor in your Streams configuration as follows:</p><pre><code>import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;

Properties streamsConfiguration = new Properties();
streamsConfiguration.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class);
</code></pre></blockquote><h2 id=defaultkeyserde>default.key.serde<a class=td-heading-self-link href=#defaultkeyserde aria-label="Heading self-link"></a></h2><blockquote><p>The default Serializer/Deserializer class for record keys, null unless set by user. Serialization and deserialization in Kafka Streams happens whenever data needs to be materialized, for example:</p><ul><li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g., via the <code>StreamsBuilder#stream()</code> and <code>KStream#to()</code> methods).</li><li>Whenever data is read from or written to a <em>state store</em>.</li></ul></blockquote><blockquote><p>This is discussed in more detail in <a href=/40/streams/developer-guide/datatypes/#streams-developer-guide-serdes>Data types and serialization</a>.</p></blockquote><h2 id=defaultvalueserde>default.value.serde<a class=td-heading-self-link href=#defaultvalueserde aria-label="Heading self-link"></a></h2><blockquote><p>The default Serializer/Deserializer class for record values, null unless set by user. Serialization and deserialization in Kafka Streams happens whenever data needs to be materialized, for example:</p><ul><li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g., via the <code>StreamsBuilder#stream()</code> and <code>KStream#to()</code> methods).</li><li>Whenever data is read from or written to a <em>state store</em>.</li></ul></blockquote><blockquote><p>This is discussed in more detail in <a href=/40/streams/developer-guide/datatypes/#streams-developer-guide-serdes>Data types and serialization</a>.</p></blockquote><h2 id=rackawareassignmentnon_overlap_cost>rack.aware.assignment.non_overlap_cost<a class=td-heading-self-link href=#rackawareassignmentnon_overlap_cost aria-label="Heading self-link"></a></h2><blockquote><p>This configuration sets the cost of moving a task from the original assignment computed either by <code>StickyTaskAssignor</code> or <code>HighAvailabilityTaskAssignor</code>. Together with <code>rack.aware.assignment.traffic_cost</code>, they control whether the optimizer favors minimizing cross rack traffic or minimizing the movement of tasks in the existing assignment. If this config is set to a larger value than <code>rack.aware.assignment.traffic_cost</code>, the optimizer will try to maintain the existing assignment computed by the task assignor. Note that the optimizer takes the ratio of these two configs into consideration of favoring maintaining existing assignment or minimizing traffic cost. For example, setting <code>rack.aware.assignment.non_overlap_cost</code> to 10 and <code>rack.aware.assignment.traffic_cost</code> to 1 is more likely to maintain existing assignment than setting <code>rack.aware.assignment.non_overlap_cost</code> to 100 and <code>rack.aware.assignment.traffic_cost</code> to 50.</p><p>The default value is null which means default <code>non_overlap_cost</code> in different assignors will be used. In <code>StickyTaskAssignor</code>, it has a default value of 10 and <code>rack.aware.assignment.traffic_cost</code> has a default value of 1, which means maintaining stickiness is preferred in <code>StickyTaskAssignor</code>. In <code>HighAvailabilityTaskAssignor</code>, it has a default value of 1 and <code>rack.aware.assignment.traffic_cost</code> has a default value of 10, which means minimizing cross rack traffic is preferred in <code>HighAvailabilityTaskAssignor</code>.</p></blockquote><h2 id=rackawareassignmentstrategy>rack.aware.assignment.strategy<a class=td-heading-self-link href=#rackawareassignmentstrategy aria-label="Heading self-link"></a></h2><blockquote><p>This configuration sets the strategy Kafka Streams uses for rack aware task assignment so that cross traffic from broker to client can be reduced. This config will only take effect when <code>broker.rack</code> is set on the brokers and <code>client.rack</code> is set on Kafka Streams side. There are two settings for this config:</p><ul><li><code>none</code>. This is the default value which means rack aware task assignment will be disabled.</li><li><code>min_traffic</code>. This settings means that the rack aware task assigner will compute an assignment which tries to minimize cross rack traffic.</li><li><code>balance_subtopology</code>. This settings means that the rack aware task assigner will compute an assignment which will try to balance tasks from same subtopology to different clients and minimize cross rack traffic on top of that.</li></ul></blockquote><blockquote><p>This config can be used together with rack.aware.assignment.non_overlap_cost and rack.aware.assignment.traffic_cost to balance reducing cross rack traffic and maintaining the existing assignment.</p></blockquote><h2 id=rackawareassignmenttags>rack.aware.assignment.tags<a class=td-heading-self-link href=#rackawareassignmenttags aria-label="Heading self-link"></a></h2><blockquote><p>This configuration sets a list of tag keys used to distribute standby replicas across Kafka Streams clients. When configured, Kafka Streams will make a best-effort to distribute the standby tasks over clients with different tag values.</p><p>Tags for the Kafka Streams clients can be set via <code>client.tag.</code> prefix. Example:</p><pre><code>Client-1                                   | Client-2
_______________________________________________________________________
client.tag.zone: eu-central-1a             | client.tag.zone: eu-central-1b
client.tag.cluster: k8s-cluster1           | client.tag.cluster: k8s-cluster1
rack.aware.assignment.tags: zone,cluster   | rack.aware.assignment.tags: zone,cluster


Client-3                                   | Client-4
_______________________________________________________________________
client.tag.zone: eu-central-1a             | client.tag.zone: eu-central-1b
client.tag.cluster: k8s-cluster2           | client.tag.cluster: k8s-cluster2
rack.aware.assignment.tags: zone,cluster   | rack.aware.assignment.tags: zone,cluster
</code></pre><p>In the above example, we have four Kafka Streams clients across two zones (<code>eu-central-1a</code>, <code>eu-central-1b</code>) and across two clusters (<code>k8s-cluster1</code>, <code>k8s-cluster2</code>). For an active task located on <code>Client-1</code>, Kafka Streams will allocate a standby task on <code>Client-4</code>, since <code>Client-4</code> has a different <code>zone</code> and a different <code>cluster</code> than <code>Client-1</code>.</p></blockquote><h2 id=rackawareassignmenttraffic_cost>rack.aware.assignment.traffic_cost<a class=td-heading-self-link href=#rackawareassignmenttraffic_cost aria-label="Heading self-link"></a></h2><blockquote><p>This configuration sets the cost of cross rack traffic. Together with <code>rack.aware.assignment.non_overlap_cost</code>, they control whether the optimizer favors minimizing cross rack traffic or minimizing the movement of tasks in the existing assignment. If this config is set to a larger value than <code>rack.aware.assignment.non_overlap_cost</code>, the optimizer will try to compute an assignment which minimize the cross rack traffic. Note that the optimizer takes the ratio of these two configs into consideration of favoring maintaining existing assignment or minimizing traffic cost. For example, setting <code>rack.aware.assignment.traffic_cost</code> to 10 and <code>rack.aware.assignment.non_overlap_cost</code> to 1 is more likely to minimize cross rack traffic than setting <code>rack.aware.assignment.traffic_cost</code> to 100 and <code>rack.aware.assignment.non_overlap_cost</code> to 50.</p><p>The default value is null which means default traffic cost in different assignors will be used. In <code>StickyTaskAssignor</code>, it has a default value of 1 and <code>rack.aware.assignment.non_overlap_cost</code> has a default value of 10. In <code>HighAvailabilityTaskAssignor</code>, it has a default value of 10 and <code>rack.aware.assignment.non_overlap_cost</code> has a default value of 1.</p></blockquote><h2 id=logsummaryintervalms>log.summary.interval.ms<a class=td-heading-self-link href=#logsummaryintervalms aria-label="Heading self-link"></a></h2><blockquote><p>This configuration controls the output interval for summary information. If greater or equal to 0, the summary log will be output according to the set time interval; If less than 0, summary output is disabled.</p></blockquote><h2 id=enablemetricspush>enable.metrics.push<a class=td-heading-self-link href=#enablemetricspush aria-label="Heading self-link"></a></h2><blockquote><p>Kafka Streams metrics can be pushed to the brokers similar to client metrics. Additionally, Kafka Streams allows to enable/disable metric pushing for each embedded client individually. However, pushing Kafka Streams metrics requires that <code>enable.metric.push</code> is enabled on the main-consumer and admin client.</p></blockquote><h2 id=maxtaskidlems>max.task.idle.ms<a class=td-heading-self-link href=#maxtaskidlems aria-label="Heading self-link"></a></h2><blockquote><p>This configuration controls how long Streams will wait to fetch data in order to provide in-order processing semantics.</p><p>When processing a task that has multiple input partitions (as in a join or merge), Streams needs to choose which partition to process the next record from. When all input partitions have locally buffered data, Streams picks the partition whose next record has the lowest timestamp. This has the desirable effect of collating the input partitions in timestamp order, which is generally what you want in a streaming join or merge. However, when Streams does not have any data buffered locally for one of the partitions, it does not know whether the next record for that partition will have a lower or higher timestamp than the remaining partitions&rsquo; records.</p><p>There are two cases to consider: either there is data in that partition on the broker that Streams has not fetched yet, or Streams is fully caught up with that partition on the broker, and the producers simply haven&rsquo;t produced any new records since Streams polled the last batch.</p><p>The default value of <code>0</code> causes Streams to delay processing a task when it detects that it has no locally buffered data for a partition, but there is data available on the brokers. Specifically, when there is an empty partition in the local buffer, but Streams has a non-zero lag for that partition. However, as soon as Streams catches up to the broker, it will continue processing, even if there is no data in one of the partitions. That is, it will not wait for new data to be <em>produced</em>. This default is designed to sacrifice some throughput in exchange for intuitively correct join semantics.</p><p>Any config value greater than zero indicates the number of <em>extra</em> milliseconds that Streams will wait if it has a caught-up but empty partition. In other words, this is the amount of time to wait for new data to be produced to the input partitions to ensure in-order processing of data in the event of a slow producer.</p><p>The config value of <code>-1</code> indicates that Streams will never wait to buffer empty partitions before choosing the next record by timestamp, which achieves maximum throughput at the expense of introducing out-of-order processing.</p></blockquote><h2 id=maxwarmupreplicas>max.warmup.replicas<a class=td-heading-self-link href=#maxwarmupreplicas aria-label="Heading self-link"></a></h2><blockquote><p>The maximum number of warmup replicas (extra standbys beyond the configured <code>num.standbys</code>) that can be assigned at once for the purpose of keeping the task available on one instance while it is warming up on another instance it has been reassigned to. Used to throttle how much extra broker traffic and cluster state can be used for high availability. Increasing this will allow Streams to warm up more tasks at once, speeding up the time for the reassigned warmups to restore sufficient state for them to be transitioned to active tasks. Must be at least 1.</p><p>Note that one warmup replica corresponds to one <a href=/40/streams/architecture/#streams_architecture_tasks>Stream Task</a>. Furthermore, note that each warmup task can only be promoted to an active task during a rebalance (normally during a so-called probing rebalance, which occur at a frequency specified by the <code>probing.rebalance.interval.ms</code> config). This means that the maximum rate at which active tasks can be migrated from one Kafka Streams instance to another instance can be determined by (<code>max.warmup.replicas</code> / <code>probing.rebalance.interval.ms</code>).</p></blockquote><h2 id=numstandbyreplicas-1>num.standby.replicas<a class=td-heading-self-link href=#numstandbyreplicas-1 aria-label="Heading self-link"></a></h2><blockquote><p>The number of standby replicas. Standby replicas are shadow copies of local state stores. Kafka Streams attempts to create the specified number of replicas per store and keep them up to date as long as there are enough instances running. Standby replicas are used to minimize the latency of task failover. A task that was previously running on a failed instance is preferred to restart on an instance that has standby replicas so that the local state store restoration process from its changelog can be minimized. Details about how Kafka Streams makes use of the standby replicas to minimize the cost of resuming tasks on failover can be found in the <a href=/40/streams/architecture/#streams_architecture_state>State</a> section.</p><p>Recommendation:
Increase the number of standbys to 1 to get instant fail-over, i.e., high-availability. Increasing the number of standbys requires more client-side storage space. For example, with 1 standby, 2x space is required.</p><p>Note:
If you enable n standby tasks, you need to provision n+1 <code>KafkaStreams</code> instances.</p></blockquote><h2 id=numstreamthreads>num.stream.threads<a class=td-heading-self-link href=#numstreamthreads aria-label="Heading self-link"></a></h2><blockquote><p>This specifies the number of stream threads in an instance of the Kafka Streams application. The stream processing code runs in these thread. For more information about Kafka Streams threading model, see <a href=/40/streams/architecture/#streams_architecture_threads>Threading Model</a>.</p></blockquote><h2 id=probingrebalanceintervalms>probing.rebalance.interval.ms<a class=td-heading-self-link href=#probingrebalanceintervalms aria-label="Heading self-link"></a></h2><blockquote><p>The maximum time to wait before triggering a rebalance to probe for warmup replicas that have restored enough to be considered caught up. Streams will only assign stateful active tasks to instances that are caught up and within the acceptable.recovery.lag, if any exist. Probing rebalances are used to query the latest total lag of warmup replicas and transition them to active tasks if ready. They will continue to be triggered as long as there are warmup tasks, and until the assignment is balanced. Must be at least 1 minute.</p></blockquote><h2 id=processingexceptionhandler>processing.exception.handler<a class=td-heading-self-link href=#processingexceptionhandler aria-label="Heading self-link"></a></h2><blockquote><p>The processing exception handler allows you to manage exceptions triggered during the processing of a record. The implemented exception handler needs to return a <code>FAIL</code> or <code>CONTINUE</code> depending on the record and the exception thrown. Returning <code>FAIL</code> will signal that Streams should shut down and <code>CONTINUE</code> will signal that Streams should ignore the issue and continue processing. The following library built-in exception handlers are available:</p><ul><li><a href=/40/javadoc/org/apache/kafka/streams/errors/LogAndContinueProcessingExceptionHandler.html>LogAndContinueProcessingExceptionHandler</a>: This handler logs the processing exception and then signals the processing pipeline to continue processing more records. This log-and-skip strategy allows Kafka Streams to make progress instead of failing if there are records that fail to be processed.</li><li><a href=/40/javadoc/org/apache/kafka/streams/errors/LogAndFailProcessingExceptionHandler.html>LogAndFailProcessingExceptionHandler</a>. This handler logs the processing exception and then signals the processing pipeline to stop processing more records.</li></ul></blockquote><blockquote><p>You can also provide your own customized exception handler besides the library provided ones to meet your needs. For example, you can choose to forward corrupt records into a quarantine topic (think: a &ldquo;dead letter queue&rdquo;) for further processing. To do this, use the Producer API to write a corrupted record directly to the quarantine topic. To be more concrete, you can create a separate <code>KafkaProducer</code> object outside the Streams client, and pass in this object as well as the dead letter queue topic name into the <code>Properties</code> map, which then can be retrieved from the <code>configure</code> function call. The drawback of this approach is that &ldquo;manual&rdquo; writes are side effects that are invisible to the Kafka Streams runtime library, so they do not benefit from the end-to-end processing guarantees of the Streams API:</p><pre><code>public class SendToDeadLetterQueueExceptionHandler implements ProcessingExceptionHandler {
    KafkaProducer&lt;byte[], byte[]&gt; dlqProducer;
    String dlqTopic;

    @Override
    public ProcessingHandlerResponse handle(final ErrorHandlerContext context,
                                            final Record record,
                                            final Exception exception) {

        log.warn(&quot;Exception caught during message processing, sending to the dead queue topic; &quot; +
            &quot;processor node: {}, taskId: {}, source topic: {}, source partition: {}, source offset: {}&quot;,
            context.processorNodeId(), context.taskId(), context.topic(), context.partition(), context.offset(),
            exception);

        dlqProducer.send(new ProducerRecord&lt;&gt;(dlqTopic, null, record.timestamp(), (byte[]) record.key(), (byte[]) record.value(), record.headers()));

        return ProcessingHandlerResponse.CONTINUE;
    }

    @Override
    public void configure(final Map&lt;String, ?&gt; configs) {
        dlqProducer = .. // get a producer from the configs map
        dlqTopic = .. // get the topic name from the configs map
    }
}
</code></pre></blockquote><h2 id=processingguarantee>processing.guarantee<a class=td-heading-self-link href=#processingguarantee aria-label="Heading self-link"></a></h2><blockquote><p>The processing guarantee that should be used. Possible values are <code>"at_least_once"</code> (default) and <code>"exactly_once_v2"</code> (for EOS version 2). Deprecated config options are <code>"exactly_once"</code> (for EOS alpha), and <code>"exactly_once_beta"</code> (for EOS version 2). Using <code>"exactly_once_v2"</code> (or the deprecated <code>"exactly_once_beta"</code>) requires broker version 2.5 or newer, while using the deprecated <code>"exactly_once"</code> requires broker version 0.11.0 or newer. Note that if exactly-once processing is enabled, the default for parameter <code>commit.interval.ms</code> changes to 100ms. Additionally, consumers are configured with <code>isolation.level="read_committed"</code> and producers are configured with <code>enable.idempotence=true</code> per default. Note that by default exactly-once processing requires a cluster of at least three brokers what is the recommended setting for production. For development, you can change this configuration by adjusting broker setting <code>transaction.state.log.replication.factor</code> and <code>transaction.state.log.min.isr</code> to the number of brokers you want to use. For more details see <a href=/40/streams/core-concepts/#streams_processing_guarantee>Processing Guarantees</a>.</p><p>Recommendation:
While it is technically possible to use EOS with any replication factor, using a replication factor lower than 3 effectively voids EOS. Thus it is strongly recommended to use a replication factor of 3 (together with <code>min.in.sync.replicas=2</code>). This recommendation applies to all topics (i.e. <code>__transaction_state</code>, <code>__consumer_offsets</code>, Kafka Streams internal topics, and user topics).</p></blockquote><h2 id=processorwrapperclass>processor.wrapper.class<a class=td-heading-self-link href=#processorwrapperclass aria-label="Heading self-link"></a></h2><blockquote><p>A class or class name implementing the <code>ProcessorWrapper</code> interface. This feature allows you to wrap any of the processors in the compiled topology, including both custom processor implementations and those created by Streams for DSL operators. This can be useful for logging or tracing implementations since it allows access to the otherwise-hidden processor context for DSL operators, and also allows for injecting additional debugging information to an entire application topology with just a single config.</p><p>IMPORTANT: This MUST be passed in when creating the topology, and will not be applied unless passed in to the appropriate topology-building constructor. You should use the <code>StreamsBuilder#new(TopologyConfig)</code> constructor for DSL applications, and the <code>Topology#new(TopologyConfig)</code> constructor for PAPI applications.</p></blockquote><h2 id=replicationfactor-1>replication.factor<a class=td-heading-self-link href=#replicationfactor-1 aria-label="Heading self-link"></a></h2><blockquote><p>This specifies the replication factor of internal topics that Kafka Streams creates when local states are used or a stream is repartitioned for aggregation. Replication is important for fault tolerance. Without replication even a single broker failure may prevent progress of the stream processing application. It is recommended to use a similar replication factor as source topics.</p><p>Recommendation:
Increase the replication factor to 3 to ensure that the internal Kafka Streams topic can tolerate up to 2 broker failures. Note that you will require more storage space as well (3x with the replication factor of 3).</p></blockquote><h2 id=rocksdbconfigsetter>rocksdb.config.setter<a class=td-heading-self-link href=#rocksdbconfigsetter aria-label="Heading self-link"></a></h2><blockquote><p>The RocksDB configuration. Kafka Streams uses RocksDB as the default storage engine for persistent stores. To change the default configuration for RocksDB, you can implement <code>RocksDBConfigSetter</code> and provide your custom class via <a href=/40/javadoc/org/apache/kafka/streams/state/RocksDBConfigSetter.html>rocksdb.config.setter</a>.</p><p>Here is an example that adjusts the memory size consumed by RocksDB.</p><pre><code>public static class CustomRocksDBConfig implements RocksDBConfigSetter {
    // This object should be a member variable so it can be closed in RocksDBConfigSetter#close.
    private org.rocksdb.Cache cache = new org.rocksdb.LRUCache(16 * 1024L * 1024L);

    @Override
    public void setConfig(final String storeName, final Options options, final Map&lt;String, Object&gt; configs) {
        // See #1 below.
        BlockBasedTableConfig tableConfig = (BlockBasedTableConfig) options.tableFormatConfig();
        tableConfig.setBlockCache(cache);
        // See #2 below.
        tableConfig.setBlockSize(16 * 1024L);
        // See #3 below.
        tableConfig.setCacheIndexAndFilterBlocks(true);
        options.setTableFormatConfig(tableConfig);
        // See #4 below.
        options.setMaxWriteBufferNumber(2);
    }

    @Override
    public void close(final String storeName, final Options options) {
        // See #5 below.
        cache.close();
    }
}

Properties streamsSettings = new Properties();
streamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class);
</code></pre><p>Notes for example:</p><ol><li><code>BlockBasedTableConfig tableConfig = (BlockBasedTableConfig) options.tableFormatConfig();</code> Get a reference to the existing table config rather than create a new one, so you don&rsquo;t accidentally overwrite defaults such as the <code>BloomFilter</code>, which is an important optimization.</li><li><code>tableConfig.setBlockSize(16 * 1024L);</code> Modify the default <a href=https://github.com/apache/kafka/blob/2.3/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java#L79>block size</a> per these instructions from the <a href=https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB#indexes-and-filter-blocks>RocksDB GitHub</a>.</li><li><code>tableConfig.setCacheIndexAndFilterBlocks(true);</code> Do not let the index and filter blocks grow unbounded. For more information, see the <a href=https://github.com/facebook/rocksdb/wiki/Block-Cache#caching-index-and-filter-blocks>RocksDB GitHub</a>.</li><li><code>options.setMaxWriteBufferNumber(2);</code> See the advanced options in the <a href=https://github.com/facebook/rocksdb/blob/8dee8cad9ee6b70fd6e1a5989a8156650a70c04f/include/rocksdb/advanced_options.h#L103>RocksDB GitHub</a>.</li><li><code>cache.close();</code> To avoid memory leaks, you must close any objects you constructed that extend org.rocksdb.RocksObject. See <a href=https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#memory-management>RocksJava docs</a> for more details.</li></ol></blockquote><h4 id=statedir>state.dir<a class=td-heading-self-link href=#statedir aria-label="Heading self-link"></a></h4><blockquote><p>The state directory. Kafka Streams persists local states under the state directory. Each application has a subdirectory on its hosting machine that is located under the state directory. The name of the subdirectory is the application ID. The state stores associated with the application are created under this subdirectory. When running multiple instances of the same application on a single machine, this path must be unique for each such instance.</p></blockquote><h4 id=taskassignorclass>task.assignor.class<a class=td-heading-self-link href=#taskassignorclass aria-label="Heading self-link"></a></h4><blockquote><p>A task assignor class or class name implementing the <code>org.apache.kafka.streams.processor.assignment.TaskAssignor</code> interface. Defaults to the high-availability task assignor. One possible alternative implementation provided in Apache Kafka is the <code>org.apache.kafka.streams.processor.assignment.assignors.StickyTaskAssignor</code>, which was the default task assignor before KIP-441 and minimizes task movement at the cost of stateful task availability. Alternative implementations of the task assignment algorithm can be plugged into the application by implementing a custom <code>TaskAssignor</code> and setting this config to the name of the custom task assignor class.</p></blockquote><h4 id=topologyoptimization>topology.optimization<a class=td-heading-self-link href=#topologyoptimization aria-label="Heading self-link"></a></h4><blockquote><p>A configuration telling Kafka Streams if it should optimize the topology and what optimizations to apply. Acceptable values are: <code>StreamsConfig.NO_OPTIMIZATION</code> (<code>none</code>), <code>StreamsConfig.OPTIMIZE</code> (<code>all</code>) or a comma separated list of specific optimizations: <code>StreamsConfig.REUSE_KTABLE_SOURCE_TOPICS</code> (<code>reuse.ktable.source.topics</code>), <code>StreamsConfig.MERGE_REPARTITION_TOPICS</code> (<code>merge.repartition.topics</code>), <code>StreamsConfig.SINGLE_STORE_SELF_JOIN</code> (<code>single.store.self.join</code>).</p></blockquote><p>We recommend listing specific optimizations in the config for production code so that the structure of your topology will not change unexpectedly during upgrades of the Streams library.</p><p>These optimizations include moving/reducing repartition topics and reusing the source topic as the changelog for source KTables. These optimizations will save on network traffic and storage in Kafka without changing the semantics of your applications. Enabling them is recommended.</p><p>Note that as of 2.3, you need to do two things to enable optimizations. In addition to setting this config to <code>StreamsConfig.OPTIMIZE</code>, you&rsquo;ll need to pass in your configuration properties when building your topology by using the overloaded <code>StreamsBuilder.build(Properties)</code> method. For example <code>KafkaStreams myStream = new KafkaStreams(streamsBuilder.build(properties), properties)</code>.</p><h4 id=windowedinnerclassserde>windowed.inner.class.serde<a class=td-heading-self-link href=#windowedinnerclassserde aria-label="Heading self-link"></a></h4><blockquote><p>Serde for the inner class of a windowed record. Must implement the org.apache.kafka.common.serialization.Serde interface.</p></blockquote><p>Note that this config is only used by plain consumer/producer clients that set a windowed de/serializer type via configs. For Kafka Streams applications that deal with windowed types, you must pass in the inner serde type when you instantiate the windowed serde object for your topology.</p><h4 id=upgradefrom>upgrade.from<a class=td-heading-self-link href=#upgradefrom aria-label="Heading self-link"></a></h4><blockquote><p>The version you are upgrading from. It is important to set this config when performing a rolling upgrade to certain versions, as described in the upgrade guide. You should set this config to the appropriate version before bouncing your instances and upgrading them to the newer version. Once everyone is on the newer version, you should remove this config and do a second rolling bounce. It is only necessary to set this config and follow the two-bounce upgrade path when upgrading from below version 2.0, or when upgrading to 2.4+ from any version lower than 2.4.</p></blockquote><h3 id=kafka-consumers-producer-and-admin-client-configuration-parameters>Kafka consumers, producer and admin client configuration parameters<a class=td-heading-self-link href=#kafka-consumers-producer-and-admin-client-configuration-parameters aria-label="Heading self-link"></a></h3><p>You can specify parameters for the Kafka <a href=/40/javadoc/org/apache/kafka/clients/consumer/package-summary.html>consumers</a>, <a href=/40/javadoc/org/apache/kafka/clients/producer/package-summary.html>producers</a>, and <a href=/40/javadoc/org/apache/kafka/kafka/clients/admin/package-summary.html>admin client</a> that are used internally. The consumer, producer and admin client settings are defined by specifying parameters in a <code>StreamsConfig</code> instance.</p><p>In this example, the Kafka <a href=/40/javadoc/org/apache/kafka/clients/consumer/ConsumerConfig.html#SESSION_TIMEOUT_MS_CONFIG>consumer session timeout</a> is configured to be 60000 milliseconds in the Streams settings:</p><pre><code> Properties streamsSettings = new Properties();
 // Example of a &quot;normal&quot; setting for Kafka Streams
 streamsSettings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;kafka-broker-01:9092&quot;);
 // Customize the Kafka consumer settings of your Streams application
 streamsSettings.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 60000);
</code></pre><h4 id=naming>Naming<a class=td-heading-self-link href=#naming aria-label="Heading self-link"></a></h4><p>Some consumer, producer and admin client configuration parameters use the same parameter name, and Kafka Streams library itself also uses some parameters that share the same name with its embedded client. For example, <code>send.buffer.bytes</code> and <code>receive.buffer.bytes</code> are used to configure TCP buffers; <code>request.timeout.ms</code> and <code>retry.backoff.ms</code> control retries for client request. You can avoid duplicate names by prefix parameter names with <code>consumer.</code>, <code>producer.</code>, or <code>admin.</code> (e.g., <code>consumer.send.buffer.bytes</code> and <code>producer.send.buffer.bytes</code>).</p><pre><code> Properties streamsSettings = new Properties();
 // same value for consumer, producer, and admin client
 streamsSettings.put(&quot;PARAMETER_NAME&quot;, &quot;value&quot;);
 // different values for consumer and producer
 streamsSettings.put(&quot;consumer.PARAMETER_NAME&quot;, &quot;consumer-value&quot;);
 streamsSettings.put(&quot;producer.PARAMETER_NAME&quot;, &quot;producer-value&quot;);
 streamsSettings.put(&quot;admin.PARAMETER_NAME&quot;, &quot;admin-value&quot;);
 // alternatively, you can use
 streamsSettings.put(StreamsConfig.consumerPrefix(&quot;PARAMETER_NAME&quot;), &quot;consumer-value&quot;);
 streamsSettings.put(StreamsConfig.producerPrefix(&quot;PARAMETER_NAME&quot;), &quot;producer-value&quot;);
 streamsSettings.put(StreamsConfig.adminClientPrefix(&quot;PARAMETER_NAME&quot;), &quot;admin-value&quot;);
</code></pre><p>You could further separate consumer configuration by adding different prefixes:</p><ul><li><code>main.consumer.</code> for main consumer which is the default consumer of stream source.</li><li><code>restore.consumer.</code> for restore consumer which is in charge of state store recovery.</li><li><code>global.consumer.</code> for global consumer which is used in global KTable construction.</li></ul><p>For example, if you only want to set restore consumer config without touching other consumers&rsquo; settings, you could simply use <code>restore.consumer.</code> to set the config.</p><pre><code> Properties streamsSettings = new Properties();
 // same config value for all consumer types
 streamsSettings.put(&quot;consumer.PARAMETER_NAME&quot;, &quot;general-consumer-value&quot;);
 // set a different restore consumer config. This would make restore consumer take restore-consumer-value,
 // while main consumer and global consumer stay with general-consumer-value
 streamsSettings.put(&quot;restore.consumer.PARAMETER_NAME&quot;, &quot;restore-consumer-value&quot;);
 // alternatively, you can use
 streamsSettings.put(StreamsConfig.restoreConsumerPrefix(&quot;PARAMETER_NAME&quot;), &quot;restore-consumer-value&quot;);
</code></pre><p>Same applied to <code>main.consumer.</code> and <code>main.consumer.</code>, if you only want to specify one consumer type config.</p><p>Additionally, to configure the internal repartition/changelog topics, you could use the <code>topic.</code> prefix, followed by any of the standard topic configs.</p><pre><code> Properties streamsSettings = new Properties();
 // Override default for both changelog and repartition topics
 streamsSettings.put(&quot;topic.PARAMETER_NAME&quot;, &quot;topic-value&quot;);
 // alternatively, you can use
 streamsSettings.put(StreamsConfig.topicPrefix(&quot;PARAMETER_NAME&quot;), &quot;topic-value&quot;);
</code></pre><h4 id=default-values>Default Values<a class=td-heading-self-link href=#default-values aria-label="Heading self-link"></a></h4><p>Kafka Streams uses different default values for some of the underlying client configs, which are summarized below. For detailed descriptions of these configs, see <a href=/40/documentation.html#producerconfigs>Producer Configs</a> and <a href=/40/documentation.html#consumerconfigs>Consumer Configs</a>.</p><table><thead><tr><th>Parameter Name</th><th>Corresponding Client</th><th>Streams Default</th></tr></thead><tbody><tr><td>auto.offset.reset</td><td>Consumer</td><td><code>earliest</code></td></tr><tr><td>linger.ms</td><td>Producer</td><td><code>100</code></td></tr><tr><td>max.poll.records</td><td>Consumer</td><td><code>1000</code></td></tr><tr><td>client.id</td><td>-</td><td><code>&lt;application.id>-&lt;random-UUID></code></td></tr></tbody></table><p>If EOS is enabled, other parameters have the following default values.</p><table><thead><tr><th>Parameter Name</th><th>Corresponding Client</th><th>Streams Default</th></tr></thead><tbody><tr><td>transaction.timeout.ms</td><td>Producer</td><td><code>10000</code></td></tr><tr><td>delivery.timeout.ms</td><td>Producer</td><td><code>Integer.MAX_VALUE</code></td></tr></tbody></table><h3 id=parameters-controlled-by-kafka-streams>Parameters controlled by Kafka Streams<a class=td-heading-self-link href=#parameters-controlled-by-kafka-streams aria-label="Heading self-link"></a></h3><p>Some parameters are not configurable by the user. If you supply a value that is different from the default value, your value is ignored. Below is a list of some of these parameters.</p><table><thead><tr><th>Parameter Name</th><th>Corresponding Client</th><th>Streams Default</th></tr></thead><tbody><tr><td>allow.auto.create.topics</td><td>Consumer</td><td><code>false</code></td></tr><tr><td>group.id</td><td>Consumer</td><td><code>application.id</code></td></tr><tr><td>enable.auto.commit</td><td>Consumer</td><td><code>false</code></td></tr><tr><td>partition.assignment.strategy</td><td>Consumer</td><td><code>StreamsPartitionAssignor</code></td></tr></tbody></table><p>If EOS is enabled, other parameters are set with the following values.</p><table><thead><tr><th>Parameter Name</th><th>Corresponding Client</th><th>Streams Default</th></tr></thead><tbody><tr><td>isolation.level</td><td>Consumer</td><td><code>READ_COMMITTED</code></td></tr><tr><td>enable.idempotence</td><td>Producer</td><td><code>true</code></td></tr></tbody></table><h3 id=clientid>client.id<a class=td-heading-self-link href=#clientid aria-label="Heading self-link"></a></h3><p>Kafka Streams uses the <code>client.id</code> parameter to compute derived client IDs for internal clients. If you don&rsquo;t set <code>client.id</code>, Kafka Streams sets it to <code>&lt;application.id>-&lt;random-UUID></code>.</p><p>This value will be used to derive the client IDs of the following internal clients.</p><table><thead><tr><th>Client</th><th>client.id</th></tr></thead><tbody><tr><td>Consumer</td><td><code>&lt;client.id>-StreamThread-&lt;threadIdx>-consumer</code></td></tr><tr><td>Restore consumer</td><td><code>&lt;client.id>-StreamThread-&lt;threadIdx>-restore-consumer</code></td></tr><tr><td>Global consumer</td><td><code>&lt;client.id>-global-consumer</code></td></tr><tr><td>Producer</td><td><strong>For Non-EOS and EOS v2:</strong><code> &lt;client.id>-StreamThread-&lt;threadIdx>-producer</code></td></tr><tr><td><strong>For EOS v1:</strong><code> &lt;client.id>-StreamThread-&lt;threadIdx>-&lt;taskId>-producer</code></td><td></td></tr><tr><td>Admin</td><td><code>&lt;client.id>-admin</code></td></tr></tbody></table><h4 id=enableautocommit>enable.auto.commit<a class=td-heading-self-link href=#enableautocommit aria-label="Heading self-link"></a></h4><blockquote><p>The consumer auto commit. To guarantee at-least-once processing semantics and turn off auto commits, Kafka Streams overrides this consumer config value to <code>false</code>. Consumers will only commit explicitly via <em>commitSync</em> calls when the Kafka Streams library or a user decides to commit the current processing state.</p></blockquote><p><a href=/40/streams/developer-guide/write-streams>Previous</a> <a href=/40/streams/developer-guide/dsl-api/>Next</a></p><ul><li><a href=/documentation>Documentation</a></li><li><a href=/streams>Kafka Streams</a></li><li><a href=/streams/developer-guide/>Developer Guide</a></li></ul><style>.feedback--answer{display:inline-block}.feedback--answer-no{margin-left:1em}.feedback--response{display:none;margin-top:1em}.feedback--response__visible{display:block}</style><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p></div><script>const yesButton=document.querySelector(".feedback--answer-yes"),noButton=document.querySelector(".feedback--answer-no"),yesResponse=document.querySelector(".feedback--response-yes"),noResponse=document.querySelector(".feedback--response-no"),disableButtons=()=>{yesButton.disabled=!0,noButton.disabled=!0},sendFeedback=e=>{if(typeof gtag!="function")return;gtag("event","page_helpful",{event_category:"Helpful",event_label:window.location.pathname,value:e})};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(100)}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(0)})</script><br><div class=td-page-meta__lastmod>Last modified March 28, 2025: <a href=https://github.com/apache/kafka-site//commit/4222b044c6a6711fe1afe8f6371eaa3cb5430a30>Updates from 4.0 (4222b044)</a></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>