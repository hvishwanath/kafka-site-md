<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://example.kafka-site-md.dev/26/getting-started/><link rel=alternate type=application/rss+xml href=https://example.kafka-site-md.dev/26/getting-started/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Getting Started | </title><meta name=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:title" content="Getting Started"><meta property="og:description" content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta property="og:type" content="website"><meta property="og:url" content="https://example.kafka-site-md.dev/26/getting-started/"><meta itemprop=name content="Getting Started"><meta itemprop=description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><meta name=twitter:card content="summary"><meta name=twitter:title content="Getting Started"><meta name=twitter:description content="This section provides an overview of what Kafka is, why it is useful, and how to get started using it."><link rel=preload href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css as=style integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><link href=/scss/main.min.7ed0eb3fc68a0678ca492e0db9c00f8e8d5b776bbb2fb833732191f6bbf02877.css rel=stylesheet integrity="sha256-ftDrP8aKBnjKSS4NucAPjo1bd2u7L7gzcyGR9rvwKHc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="154" height="250" viewBox="0 0 256 416" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid"><path d="M201.816 230.216c-16.186.0-30.697 7.171-40.634 18.461l-25.463-18.026c2.703-7.442 4.255-15.433 4.255-23.797.0-8.219-1.498-16.076-4.112-23.408l25.406-17.835c9.936 11.233 24.409 18.365 40.548 18.365 29.875.0 54.184-24.305 54.184-54.184.0-29.879-24.309-54.184-54.184-54.184s-54.184 24.305-54.184 54.184c0 5.348.808 10.505 2.258 15.389l-25.423 17.844c-10.62-13.175-25.911-22.374-43.333-25.182v-30.64c24.544-5.155 43.037-26.962 43.037-53.019C124.171 24.305 99.862.0 69.987.0 40.112.0 15.803 24.305 15.803 54.184c0 25.708 18.014 47.246 42.067 52.769v31.038C25.044 143.753.0 172.401.0 206.854c0 34.621 25.292 63.374 58.355 68.94v32.774c-24.299 5.341-42.552 27.011-42.552 52.894.0 29.879 24.309 54.184 54.184 54.184s54.184-24.305 54.184-54.184c0-25.883-18.253-47.553-42.552-52.894v-32.775a69.965 69.965.0 0042.6-24.776l25.633 18.143c-1.423 4.84-2.22 9.946-2.22 15.24.0 29.879 24.309 54.184 54.184 54.184S256 314.279 256 284.4c0-29.879-24.309-54.184-54.184-54.184zm0-126.695c14.487.0 26.27 11.788 26.27 26.271s-11.783 26.27-26.27 26.27-26.27-11.787-26.27-26.27 11.783-26.271 26.27-26.271zm-158.1-49.337c0-14.483 11.784-26.27 26.271-26.27s26.27 11.787 26.27 26.27c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27zm52.541 307.278c0 14.483-11.783 26.27-26.27 26.27s-26.271-11.787-26.271-26.27 11.784-26.27 26.271-26.27 26.27 11.787 26.27 26.27zm-26.272-117.97c-20.205.0-36.642-16.434-36.642-36.638.0-20.205 16.437-36.642 36.642-36.642 20.204.0 36.641 16.437 36.641 36.642.0 20.204-16.437 36.638-36.641 36.638zm131.831 67.179c-14.487.0-26.27-11.788-26.27-26.271s11.783-26.27 26.27-26.27 26.27 11.787 26.27 26.27-11.783 26.271-26.27 26.271z" style="fill:#231f20"/></svg></span><span class=navbar-brand__name></span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/41/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li><li class=nav-item><a class=nav-link href=/testimonials/><span>Testimonials</span></a></li><li class=nav-item><a class=nav-link href=/community/downloads/><span>Download Kafka</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Releases</a><ul class=dropdown-menu><li><a class=dropdown-item href=/41/>4.1</a></li><li><a class=dropdown-item href=/40/>4.0</a></li><li><a class=dropdown-item href=/39/>3.9</a></li><li><a class=dropdown-item href=/38/>3.8</a></li><li><a class=dropdown-item href=/37/>3.7</a></li><li><a class=dropdown-item href=/36/>3.6</a></li><li><a class=dropdown-item href=/35/>3.5</a></li><li><a class=dropdown-item href=/34/>3.4</a></li><li><a class=dropdown-item href=/33/>3.3</a></li><li><a class=dropdown-item href=/32/>3.2</a></li><li><a class=dropdown-item href=/31/>3.1</a></li><li><a class=dropdown-item href=/30/>3.0</a></li><li><a class=dropdown-item href=/28/>2.8</a></li><li><a class=dropdown-item href=/27/>2.7</a></li><li><a class=dropdown-item href=/26/>2.6</a></li><li><a class=dropdown-item href=/25/>2.5</a></li><li><a class=dropdown-item href=/24/>2.4</a></li><li><a class=dropdown-item href=/23/>2.3</a></li><li><a class=dropdown-item href=/22/>2.2</a></li><li><a class=dropdown-item href=/21/>2.1</a></li><li><a class=dropdown-item href=/20/>2.0</a></li><li><a class=dropdown-item href=/11/>1.1</a></li><li><a class=dropdown-item href=/10/>1.0</a></li><li><a class=dropdown-item href=/0110/>0.11.0</a></li><li><a class=dropdown-item href=/0102/>0.10.2</a></li><li><a class=dropdown-item href=/0101/>0.10.1</a></li><li><a class=dropdown-item href=/0100/>0.10.0</a></li><li><a class=dropdown-item href=/090/>0.9.0</a></li><li><a class=dropdown-item href=/082/>0.8.2</a></li><li><a class=dropdown-item href=/081/>0.8.1</a></li><li><a class=dropdown-item href=/080/>0.8.0</a></li><li><a class=dropdown-item href=/07/>0.7</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e58a36913f949563db0a14b5eaf8f6a5.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/26/getting-started/>Return to the regular view of this page</a>.</p></div><h1 class=title>Getting Started</h1><div class=lead>This section provides an overview of what Kafka is, why it is useful, and how to get started using it.</div><ul><li>1: <a href=#pg-cdddf029832837440263ae9b9f6f47b4>Introduction</a></li><li>2: <a href=#pg-357d6451ed3c2fc1b9a6e0585b6976a0>Use Cases</a></li><li>3: <a href=#pg-b311f6ce39abaee46980a18f966c9754>Quick Start</a></li><li>4: <a href=#pg-1edef7ba4ae8fea65d22e152161a5fc6>Ecosystem</a></li><li>5: <a href=#pg-7950b7090e095aa864613914ee39fcad>Upgrading</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-cdddf029832837440263ae9b9f6f47b4>1 - Introduction</h1><h2 id=what-is-event-streaming>What is event streaming?<a class=td-heading-self-link href=#what-is-event-streaming aria-label="Heading self-link"></a></h2><p>Event streaming is the digital equivalent of the human body&rsquo;s central nervous system. It is the technological foundation for the &lsquo;always-on&rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.</p><p>Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.</p><h2 id=what-can-i-use-event-streaming-for>What can I use event streaming for?<a class=td-heading-self-link href=#what-can-i-use-event-streaming-for aria-label="Heading self-link"></a></h2><p>Event streaming is applied to a <a href=/powered-by>wide variety of use cases</a> across a plethora of industries and organizations. Its many examples include:</p><ul><li>To process payments and financial transactions in real-time, such as in stock exchanges, banks, and insurances.</li><li>To track and monitor cars, trucks, fleets, and shipments in real-time, such as in logistics and the automotive industry.</li><li>To continuously capture and analyze sensor data from IoT devices or other equipment, such as in factories and wind parks.</li><li>To collect and immediately react to customer interactions and orders, such as in retail, the hotel and travel industry, and mobile applications.</li><li>To monitor patients in hospital care and predict changes in condition to ensure timely treatment in emergencies.</li><li>To connect, store, and make available data produced by different divisions of a company.</li><li>To serve as the foundation for data platforms, event-driven architectures, and microservices.</li></ul><h2 id=apache-kafka-is-an-event-streaming-platform-what-does-that-mean>Apache Kafka® is an event streaming platform. What does that mean?<a class=td-heading-self-link href=#apache-kafka-is-an-event-streaming-platform-what-does-that-mean aria-label="Heading self-link"></a></h2><p>Kafka combines three key capabilities so you can implement <a href=/powered-by>your use cases</a> for event streaming end-to-end with a single battle-tested solution:</p><ol><li>To <strong>publish</strong> (write) and <strong>subscribe to</strong> (read) streams of events, including continuous import/export of your data from other systems.</li><li>To <strong>store</strong> streams of events durably and reliably for as long as you want.</li><li>To <strong>process</strong> streams of events as they occur or retrospectively.</li></ol><p>And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.</p><h2 id=how-does-kafka-work-in-a-nutshell>How does Kafka work in a nutshell?<a class=td-heading-self-link href=#how-does-kafka-work-in-a-nutshell aria-label="Heading self-link"></a></h2><p>Kafka is a distributed system consisting of <strong>servers</strong> and <strong>clients</strong> that communicate via a high-performance <a href=/protocol.html>TCP network protocol</a>. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.</p><p><strong>Servers</strong> : Kafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run <a href=/#connect>Kafka Connect</a> to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss.</p><p><strong>Clients</strong> : They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka ships with some such clients included, which are augmented by <a href=https://cwiki.apache.org/confluence/display/KAFKA/Clients>dozens of clients</a> provided by the Kafka community: clients are available for Java and Scala including the higher-level <a href=/streams/>Kafka Streams</a> library, for Go, Python, C/C++, and many other programming languages as well as REST APIs.</p><h2 id=main-concepts-and-terminology>Main Concepts and Terminology<a class=td-heading-self-link href=#main-concepts-and-terminology aria-label="Heading self-link"></a></h2><p>An <strong>event</strong> records the fact that &ldquo;something happened&rdquo; in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here&rsquo;s an example event:</p><ul><li>Event key: &ldquo;Alice&rdquo;</li><li>Event value: &ldquo;Made a payment of $200 to Bob&rdquo;</li><li>Event timestamp: &ldquo;Jun. 25, 2020 at 2:06 p.m.&rdquo;</li></ul><p><strong>Producers</strong> are those client applications that publish (write) events to Kafka, and <strong>consumers</strong> are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers. Kafka provides various <a href=/#intro_guarantees>guarantees</a> such as the ability to process events exactly-once.</p><p>Events are organized and durably stored in <strong>topics</strong>. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be &ldquo;payments&rdquo;. Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka&rsquo;s performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.</p><p>Topics are <strong>partitioned</strong> , meaning a topic is spread over a number of &ldquo;buckets&rdquo; located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic&rsquo;s partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka <a href=/#intro_guarantees>guarantees</a> that any consumer of a given topic-partition will always read that partition&rsquo;s events in exactly the same order as they were written.</p><p><img src=/images/streams-and-tables-p1_p4.png> Figure: This example topic has four partitions P1–P4. Two different producer clients are publishing, independently from each other, new events to the topic by writing events over the network to the topic&rsquo;s partitions. Events with the same key (denoted by their color in the figure) are written to the same partition. Note that both producers can write to the same partition if appropriate.</p><p>To make your data fault-tolerant and highly-available, every topic can be <strong>replicated</strong> , even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. This replication is performed at the level of topic-partitions.</p><p>This primer should be sufficient for an introduction. The <a href=/#design>Design</a> section of the documentation explains Kafka&rsquo;s various concepts in full detail, if you are interested.</p><h2 id=kafka-apis>Kafka APIs<a class=td-heading-self-link href=#kafka-apis aria-label="Heading self-link"></a></h2><p>In addition to command line tooling for management and administration tasks, Kafka has five core APIs for Java and Scala:</p><ul><li>The <a href=/documentation.html#adminapi>Admin API</a> to manage and inspect topics, brokers, and other Kafka objects.</li><li>The <a href=/documentation.html#producerapi>Producer API</a> to publish (write) a stream of events to one or more Kafka topics.</li><li>The <a href=/documentation.html#consumerapi>Consumer API</a> to subscribe to (read) one or more topics and to process the stream of events produced to them.</li><li>The <a href=/streams>Kafka Streams API</a> to implement stream processing applications and microservices. It provides higher-level functions to process event streams, including transformations, stateful operations like aggregations and joins, windowing, processing based on event-time, and more. Input is read from one or more topics in order to generate output to one or more topics, effectively transforming the input streams to output streams.</li><li>The <a href=/documentation.html#connect>Kafka Connect API</a> to build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications so they can integrate with Kafka. For example, a connector to a relational database like PostgreSQL might capture every change to a set of tables. However, in practice, you typically don&rsquo;t need to implement your own connectors because the Kafka community already provides hundreds of ready-to-use connectors.</li></ul><h2 id=where-to-go-from-here>Where to go from here<a class=td-heading-self-link href=#where-to-go-from-here aria-label="Heading self-link"></a></h2><ul><li>To get hands-on experience with Kafka, follow the <a href=/quickstart>Quickstart</a>.</li><li>To understand Kafka in more detail, read the <a href=/>Documentation</a>. You also have your choice of <a href=/books-and-papers>Kafka books and academic papers</a>.</li><li>Browse through the <a href=/powered-by>Use Cases</a> to learn how other users in our world-wide community are getting value out of Kafka.</li><li>Join a <a href=/events>local Kafka meetup group</a> and <a href=https://kafka-summit.org/past-events/>watch talks from Kafka Summit</a>, the main conference of the Kafka community.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-357d6451ed3c2fc1b9a6e0585b6976a0>2 - Use Cases</h1><p>Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see <a href=https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/>this blog post</a>.</p><h2 id=messaging>Messaging<a class=td-heading-self-link href=#messaging aria-label="Heading self-link"></a></h2><p>Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</p><p>In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.</p><p>In this domain Kafka is comparable to traditional messaging systems such as <a href=http://activemq.apache.org>ActiveMQ</a> or <a href=https://www.rabbitmq.com>RabbitMQ</a>.</p><h2 id=website-activity-tracking>Website Activity Tracking<a class=td-heading-self-link href=#website-activity-tracking aria-label="Heading self-link"></a></h2><p>The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.</p><p>Activity tracking is often very high volume as many activity messages are generated for each user page view.</p><h2 id=metrics>Metrics<a class=td-heading-self-link href=#metrics aria-label="Heading self-link"></a></h2><p>Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.</p><h2 id=log-aggregation>Log Aggregation<a class=td-heading-self-link href=#log-aggregation aria-label="Heading self-link"></a></h2><p>Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.</p><h2 id=stream-processing>Stream Processing<a class=td-heading-self-link href=#stream-processing aria-label="Heading self-link"></a></h2><p>Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an &ldquo;articles&rdquo; topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called <a href=/streams>Kafka Streams</a> is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include <a href=https://storm.apache.org/>Apache Storm</a> and <a href=http://samza.apache.org/>Apache Samza</a>.</p><h2 id=event-sourcing>Event Sourcing<a class=td-heading-self-link href=#event-sourcing aria-label="Heading self-link"></a></h2><p><a href=http://martinfowler.com/eaaDev/EventSourcing.html>Event sourcing</a> is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka&rsquo;s support for very large stored log data makes it an excellent backend for an application built in this style.</p><h2 id=commit-log>Commit Log<a class=td-heading-self-link href=#commit-log aria-label="Heading self-link"></a></h2><p>Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The <a href=/documentation.html#compaction>log compaction</a> feature in Kafka helps support this usage. In this usage Kafka is similar to <a href=https://bookkeeper.apache.org/>Apache BookKeeper</a> project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b311f6ce39abaee46980a18f966c9754>3 - Quick Start</h1><p>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use <code>bin\windows\</code> instead of <code>bin/</code>, and change the script extension to <code>.bat</code>.</p><h2 id=step-1-download-the-code>Step 1: Download the code<a class=td-heading-self-link href=#step-1-download-the-code aria-label="Heading self-link"></a></h2><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.6.3/kafka_2.13-2.6.3.tgz" title="Kafka downloads">Download</a> the 2.6.3 release and un-tar it.</p><pre><code>&gt; tar -xzf kafka_2.13-2.6.3.tgz
&gt; cd kafka_2.13-2.6.3
</code></pre><h2 id=step-2-start-the-server>Step 2: Start the server<a class=td-heading-self-link href=#step-2-start-the-server aria-label="Heading self-link"></a></h2><p>Kafka uses <a href=https://zookeeper.apache.org/>ZooKeeper</a> so you need to first start a ZooKeeper server if you don&rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p><pre><code>&gt; bin/zookeeper-server-start.sh config/zookeeper.properties
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
</code></pre><p>Now start the Kafka server:</p><pre><code>&gt; bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
</code></pre><h2 id=step-3-create-a-topic>Step 3: Create a topic<a class=td-heading-self-link href=#step-3-create-a-topic aria-label="Heading self-link"></a></h2><p>Let&rsquo;s create a topic named &ldquo;test&rdquo; with a single partition and only one replica:</p><pre><code>&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test
</code></pre><p>We can now see that topic if we run the list topic command:</p><pre><code>&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
test
</code></pre><p>Alternatively, instead of manually creating topics you can also configure your brokers to auto-create topics when a non-existent topic is published to.</p><h2 id=step-4-send-some-messages>Step 4: Send some messages<a class=td-heading-self-link href=#step-4-send-some-messages aria-label="Heading self-link"></a></h2><p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default, each line will be sent as a separate message.</p><p>Run the producer and then type a few messages into the console to send to the server.</p><pre><code>&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test
This is a message
This is another message
</code></pre><h2 id=step-5-start-a-consumer>Step 5: Start a consumer<a class=td-heading-self-link href=#step-5-start-a-consumer aria-label="Heading self-link"></a></h2><p>Kafka also has a command line consumer that will dump out messages to standard output.</p><pre><code>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
This is a message
This is another message
</code></pre><p>If you have each of the above commands running in a different terminal then you should now be able to type messages into the producer terminal and see them appear in the consumer terminal.</p><p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p><h2 id=step-6-setting-up-a-multi-broker-cluster>Step 6: Setting up a multi-broker cluster<a class=td-heading-self-link href=#step-6-setting-up-a-multi-broker-cluster aria-label="Heading self-link"></a></h2><p>So far we have been running against a single broker, but that&rsquo;s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let&rsquo;s expand our cluster to three nodes (still all on our local machine).</p><p>First we make a config file for each of the brokers (on Windows use the <code>copy</code> command instead):</p><pre><code>&gt; cp config/server.properties config/server-1.properties
&gt; cp config/server.properties config/server-2.properties
</code></pre><p>Now edit these new files and set the following properties:</p><pre><code>config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dirs=/tmp/kafka-logs-1

config/server-2.properties:
    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dirs=/tmp/kafka-logs-2
</code></pre><p>The <code>broker.id</code> property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each other&rsquo;s data.</p><p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p><pre><code>&gt; bin/kafka-server-start.sh config/server-1.properties &amp;
...
&gt; bin/kafka-server-start.sh config/server-2.properties &amp;
...
</code></pre><p>Now create a new topic with a replication factor of three:</p><pre><code>&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic
</code></pre><p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the &ldquo;describe topics&rdquo; command:</p><pre><code>&gt; bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
</code></pre><p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p><ul><li>&ldquo;leader&rdquo; is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li><li>&ldquo;replicas&rdquo; is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li><li>&ldquo;isr&rdquo; is the set of &ldquo;in-sync&rdquo; replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li></ul><p>Note that in my example node 1 is the leader for the only partition of the topic.</p><p>We can run the same command on the original topic we created to see where it is:</p><pre><code>&gt; bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
</code></pre><p>So there is no surprise there&ndash;the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p><p>Let&rsquo;s publish a few messages to our new topic:</p><pre><code>&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
^C
</code></pre><p>Now let&rsquo;s consume these messages:</p><pre><code>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
</code></pre><p>Now let&rsquo;s test out fault-tolerance. Broker 1 was acting as the leader so let&rsquo;s kill it:</p><pre><code>&gt; ps aux | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...
&gt; kill -9 7564
</code></pre><p>On Windows use:</p><pre><code>&gt; wmic process where &quot;caption = 'java.exe' and commandline like '%server-1.properties%'&quot; get processid
ProcessId
6016
&gt; taskkill /pid 6016 /f
</code></pre><p>Leadership has switched to one of the followers and node 1 is no longer in the in-sync replica set:</p><pre><code>&gt; bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
</code></pre><p>But the messages are still available for consumption even though the leader that took the writes originally is down:</p><pre><code>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
</code></pre><h2 id=step-7-use-kafka-connect-to-importexport-data>Step 7: Use Kafka Connect to import/export data<a class=td-heading-self-link href=#step-7-use-kafka-connect-to-importexport-data aria-label="Heading self-link"></a></h2><p>Reading data from the console and writing it back to the console is a convenient place to start, but you&rsquo;ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data.</p><p>Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs <em>connectors</em> , which implement the custom logic for interacting with an external system. In this quickstart we&rsquo;ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file.</p><p>First, we&rsquo;ll start by creating some seed data to test with:</p><pre><code>&gt; echo -e &quot;foo
bar&quot; &gt; test.txt
</code></pre><p>Or on Windows:</p><pre><code>&gt; echo foo&gt; test.txt
&gt; echo bar&gt;&gt; test.txt
</code></pre><p>Next, we&rsquo;ll start two connectors running in <em>standalone</em> mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p><pre><code>&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
</code></pre><p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file.</p><p>During startup you&rsquo;ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from <code>test.txt</code> and producing them to the topic <code>connect-test</code>, and the sink connector should start reading messages from the topic <code>connect-test</code> and write them to the file <code>test.sink.txt</code>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p><pre><code>&gt; more test.sink.txt
foo
bar
</code></pre><p>Note that the data is being stored in the Kafka topic <code>connect-test</code>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p><pre><code>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;foo&quot;}
{&quot;schema&quot;:{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false},&quot;payload&quot;:&quot;bar&quot;}
...
</code></pre><p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p><pre><code>&gt; echo Another line&gt;&gt; test.txt
</code></pre><p>You should see the line appear in the console consumer output and in the sink file.</p><h2 id=step-8-use-kafka-streams-to-process-data>Step 8: Use Kafka Streams to process data<a class=td-heading-self-link href=#step-8-use-kafka-streams-to-process-data aria-label="Heading self-link"></a></h2><p>Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters. Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&rsquo;s server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more. This <a href=/26/streams/quickstart/>quickstart example</a> will demonstrate how to run a streaming application coded in this library.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1edef7ba4ae8fea65d22e152161a5fc6>4 - Ecosystem</h1><p>There are a plethora of tools that integrate with Kafka outside the main distribution. The <a href=https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem>ecosystem page</a> lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7950b7090e095aa864613914ee39fcad>5 - Upgrading</h1><h2 id=upgrading-to-263-from-any-version-08x-through-25x>Upgrading to 2.6.3 from any version 0.8.x through 2.5.x<a class=td-heading-self-link href=#upgrading-to-263-from-any-version-08x-through-25x aria-label="Heading self-link"></a></h2><p><strong>If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.5</code>, <code>2.4</code>, etc.)</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from version 0.11.0.x or above, and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.5</code>, <code>2.4</code>, etc.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to <code>2.6</code>.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.6 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><h3 id=notable-changes-in-260>Notable changes in 2.6.0<a class=td-heading-self-link href=#notable-changes-in-260 aria-label="Heading self-link"></a></h3><ul><li>Kafka Streams adds a new processing mode (requires broker 2.5 or newer) that improves application scalability using exactly-once guarantees (cf. <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics>KIP-447</a>)</li><li>TLSv1.3 has been enabled by default for Java 11 or newer. The client and server will negotiate TLSv1.3 if both support it and fallback to TLSv1.2 otherwise. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-573%3A+Enable+TLSv1.3+by+default>KIP-573</a> for more details.</li><li>The default value for the <code>client.dns.lookup</code> configuration has been changed from <code>default</code> to <code>use_all_dns_ips</code>. If a hostname resolves to multiple IP addresses, clients and brokers will now attempt to connect to each IP in sequence until the connection is successfully established. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-602%3A+Change+default+value+for+client.dns.lookup>KIP-602</a> for more details.</li><li><code>NotLeaderForPartitionException</code> has been deprecated and replaced with <code>NotLeaderOrFollowerException</code>. Fetch requests and other requests intended only for the leader or follower return NOT_LEADER_OR_FOLLOWER(6) instead of REPLICA_NOT_AVAILABLE(9) if the broker is not a replica, ensuring that this transient error during reassignments is handled by all clients as a retriable exception.</li><li>There are several notable changes to the reassignment tool <code>kafka-reassign-partitions.sh</code> following the completion of <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment>KIP-455</a>. This tool now requires the <code>--additional</code> flag to be provided when changing the throttle of an active reassignment. Reassignment cancellation is now possible using the <code>--cancel</code> command. Finally, reassignment with <code>--zookeeper</code> has been deprecated in favor of <code>--bootstrap-server</code>. See the KIP for more detail.</li></ul><h2 id=upgrading-to-250-from-any-version-08x-through-24x>Upgrading to 2.5.0 from any version 0.8.x through 2.4.x<a class=td-heading-self-link href=#upgrading-to-250-from-any-version-08x-through-24x aria-label="Heading self-link"></a></h2><p><strong>If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.4</code>, <code>2.3</code>, etc.)</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from version 0.11.0.x or above, and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.4</code>, <code>2.3</code>, etc.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to <code>2.5</code>.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.5 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><h3 id=notable-changes-in-250>Notable changes in 2.5.0<a class=td-heading-self-link href=#notable-changes-in-250 aria-label="Heading self-link"></a></h3><ul><li>When <code>RebalanceProtocol#COOPERATIVE</code> is used, <code>Consumer#poll</code> can still return data while it is in the middle of a rebalance for those partitions still owned by the consumer; in addition <code>Consumer#commitSync</code> now may throw a non-fatal <code>RebalanceInProgressException</code> to notify users of such an event, in order to distinguish from the fatal <code>CommitFailedException</code> and allow users to complete the ongoing rebalance and then reattempt committing offsets for those still-owned partitions.</li><li>For improved resiliency in typical network environments, the default value of <code>zookeeper.session.timeout.ms</code> has been increased from 6s to 18s and <code>replica.lag.time.max.ms</code> from 10s to 30s.</li><li>New DSL operator <code>cogroup()</code> has been added for aggregating multiple streams together at once.</li><li>Added a new <code>KStream.toTable()</code> API to translate an input event stream into a KTable.</li><li>Added a new Serde type <code>Void</code> to represent null keys or null values from input topic.</li><li>Deprecated <code>UsePreviousTimeOnInvalidTimestamp</code> and replaced it with <code>UsePartitionTimeOnInvalidTimeStamp</code>.</li><li>Improved exactly-once semantics by adding a pending offset fencing mechanism and stronger transactional commit consistency check, which greatly simplifies the implementation of a scalable exactly-once application. We also added a new exactly-once semantics code example under <a href=https://github.com/apache/kafka/tree/2.5/examples>examples</a> folder. Check out <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics>KIP-447</a> for the full details.</li><li>Added a new public api <code>KafkaStreams.queryMetadataForKey(String, K, Serializer) to get detailed information on the key being queried. It provides information about the partition number where the key resides in addition to hosts containing the active and standby partitions for the key.</code></li><li>Provided support to query stale stores (for high availability) and the stores belonging to a specific partition by deprecating <code>KafkaStreams.store(String, QueryableStoreType)</code> and replacing it with <code>KafkaStreams.store(StoreQueryParameters)</code>.</li><li>Added a new public api to access lag information for stores local to an instance with <code>KafkaStreams.allLocalStorePartitionLags()</code>.</li><li>Scala 2.11 is no longer supported. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-531%3A+Drop+support+for+Scala+2.11+in+Kafka+2.5>KIP-531</a> for details.</li><li>All Scala classes from the package <code>kafka.security.auth</code> have been deprecated. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface>KIP-504</a> for details of the new Java authorizer API added in 2.4.0. Note that <code>kafka.security.auth.Authorizer</code> and <code>kafka.security.auth.SimpleAclAuthorizer</code> were deprecated in 2.4.0.</li><li>TLSv1 and TLSv1.1 have been disabled by default since these have known security vulnerabilities. Only TLSv1.2 is now enabled by default. You can continue to use TLSv1 and TLSv1.1 by explicitly enabling these in the configuration options <code>ssl.protocol</code> and <code>ssl.enabled.protocols</code>.</li><li>ZooKeeper has been upgraded to 3.5.7, and a ZooKeeper upgrade from 3.4.X to 3.5.7 can fail if there are no snapshot files in the 3.4 data directory. This usually happens in test upgrades where ZooKeeper 3.5.7 is trying to load an existing 3.4 data dir in which no snapshot file has been created. For more details about the issue please refer to <a href=https://issues.apache.org/jira/browse/ZOOKEEPER-3056>ZOOKEEPER-3056</a>. A fix is given in <a href=https://issues.apache.org/jira/browse/ZOOKEEPER-3056>ZOOKEEPER-3056</a>, which is to set <code>snapshot.trust.empty=true</code> config in <code>zookeeper.properties</code> before the upgrade.</li><li>ZooKeeper version 3.5.7 supports TLS-encrypted connectivity to ZooKeeper both with or without client certificates, and additional Kafka configurations are available to take advantage of this. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-515%3A+Enable+ZK+client+to+use+the+new+TLS+supported+authentication>KIP-515</a> for details.</li></ul><h2 id=upgrading-to-240-from-any-version-08x-through-23x>Upgrading to 2.4.0 from any version 0.8.x through 2.3.x<a class=td-heading-self-link href=#upgrading-to-240-from-any-version-08x-through-23x aria-label="Heading self-link"></a></h2><p><strong>If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.3</code>, <code>2.2</code>, etc.)</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from version 0.11.0.x or above, and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g., <code>2.3</code>, <code>2.2</code>, etc.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to <code>2.4</code>.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.4 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>ZooKeeper has been upgraded to 3.5.6. ZooKeeper upgrade from 3.4.X to 3.5.6 can fail if there are no snapshot files in 3.4 data directory. This usually happens in test upgrades where ZooKeeper 3.5.6 is trying to load an existing 3.4 data dir in which no snapshot file has been created. For more details about the issue please refer to <a href=https://issues.apache.org/jira/browse/ZOOKEEPER-3056>ZOOKEEPER-3056</a>. A fix is given in <a href=https://issues.apache.org/jira/browse/ZOOKEEPER-3056>ZOOKEEPER-3056</a>, which is to set <code>snapshot.trust.empty=true</code> config in <code>zookeeper.properties</code> before the upgrade. But we have observed data loss in standalone cluster upgrades when using <code>snapshot.trust.empty=true</code> config. For more details about the issue please refer to <a href=https://issues.apache.org/jira/browse/ZOOKEEPER-3644>ZOOKEEPER-3644</a>. So we recommend the safe workaround of copying empty <a href=https://issues.apache.org/jira/secure/attachment/12928686/snapshot.0>snapshot</a> file to the 3.4 data directory, if there are no snapshot files in 3.4 data directory. For more details about the workaround please refer to <a href=https://cwiki.apache.org/confluence/display/ZOOKEEPER/Upgrade+FAQ>ZooKeeper Upgrade FAQ</a>.</li><li>An embedded Jetty based <a href=http://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_adminserver>AdminServer</a> added in ZooKeeper 3.5. AdminServer is enabled by default in ZooKeeper and is started on port 8080. AdminServer is disabled by default in the ZooKeeper config (<code>zookeeper.properties</code>) provided by the Apache Kafka distribution. Make sure to update your local <code>zookeeper.properties</code> file with <code>admin.enableServer=false</code> if you wish to disable the AdminServer. Please refer <a href=http://zookeeper.apache.org/doc/r3.5.6/zookeeperAdmin.html#sc_adminserver>AdminServer config</a> to configure the AdminServer.</li></ol><h3 id=notable-changes-in-240>Notable changes in 2.4.0<a class=td-heading-self-link href=#notable-changes-in-240 aria-label="Heading self-link"></a></h3><ul><li>A new Admin API has been added for partition reassignments. Due to changing the way Kafka propagates reassignment information, it is possible to lose reassignment state in failure edge cases while upgrading to the new version. It is not recommended to start reassignments while upgrading.</li><li>ZooKeeper has been upgraded from 3.4.14 to 3.5.6. TLS and dynamic reconfiguration are supported by the new version.</li><li>The <code>bin/kafka-preferred-replica-election.sh</code> command line tool has been deprecated. It has been replaced by <code>bin/kafka-leader-election.sh</code>.</li><li>The methods <code>electPreferredLeaders</code> in the Java <code>AdminClient</code> class have been deprecated in favor of the methods <code>electLeaders</code>.</li><li>Scala code leveraging the <code>NewTopic(String, int, short)</code> constructor with literal values will need to explicitly call <code>toShort</code> on the second literal.</li><li>The argument in the constructor <code>GroupAuthorizationException(String)</code> is now used to specify an exception message. Previously it referred to the group that failed authorization. This was done for consistency with other exception types and to avoid potential misuse. The constructor <code>TopicAuthorizationException(String)</code> which was previously used for a single unauthorized topic was changed similarly.</li><li>The internal <code>PartitionAssignor</code> interface has been deprecated and replaced with a new <code>ConsumerPartitionAssignor</code> in the public API. Some methods/signatures are slightly different between the two interfaces. Users implementing a custom PartitionAssignor should migrate to the new interface as soon as possible.</li><li>The <code>DefaultPartitioner</code> now uses a sticky partitioning strategy. This means that records for specific topic with null keys and no assigned partition will be sent to the same partition until the batch is ready to be sent. When a new batch is created, a new partition is chosen. This decreases latency to produce, but it may result in uneven distribution of records across partitions in edge cases. Generally users will not be impacted, but this difference may be noticeable in tests and other situations producing records for a very short amount of time.</li><li>The blocking <code>KafkaConsumer#committed</code> methods have been extended to allow a list of partitions as input parameters rather than a single partition. It enables fewer request/response iterations between clients and brokers fetching for the committed offsets for the consumer group. The old overloaded functions are deprecated and we would recommend users to make their code changes to leverage the new methods (details can be found in <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-520%3A+Add+overloaded+Consumer%23committed+for+batching+partitions>KIP-520</a>).</li><li>We&rsquo;ve introduced a new <code>INVALID_RECORD</code> error in the produce response to distinguish from the <code>CORRUPT_MESSAGE</code> error. To be more concrete, previously when a batch of records were sent as part of a single request to the broker and one or more of the records failed the validation due to various causes (mismatch magic bytes, crc checksum errors, null key for log compacted topics, etc), the whole batch would be rejected with the same and misleading <code>CORRUPT_MESSAGE</code>, and the caller of the producer client would see the corresponding exception from either the future object of <code>RecordMetadata</code> returned from the <code>send</code> call as well as in the <code>Callback#onCompletion(RecordMetadata metadata, Exception exception)</code> Now with the new error code and improved error messages of the exception, producer callers would be better informed about the root cause why their sent records were failed.</li><li>We are introducing incremental cooperative rebalancing to the clients&rsquo; group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance. The <code>ConsumerCoordinator</code> will choose the latest <code>RebalanceProtocol</code> that is commonly supported by all of the consumer&rsquo;s supported assignors. You can use the new built-in <code>CooperativeStickyAssignor</code> or plug in your own custom cooperative assignor. To do so you must implement the <code>ConsumerPartitionAssignor</code> interface and include <code>RebalanceProtocol.COOPERATIVE</code> in the list returned by <code>ConsumerPartitionAssignor#supportedProtocols</code>. Your custom assignor can then leverage the <code>ownedPartitions</code> field in each consumer&rsquo;s <code>Subscription</code> to give partitions back to their previous owners whenever possible. Note that when a partition is to be reassigned to another consumer, it <em>must</em> be removed from the new assignment until it has been revoked from its original owner. Any consumer that has to revoke a partition will trigger a followup rebalance to allow the revoked partition to safely be assigned to its new owner. See the <a href=/https://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.RebalanceProtocol.html>ConsumerPartitionAssignor RebalanceProtocol javadocs</a> for more information.<br>To upgrade from the old (eager) protocol, which always revokes all partitions before rebalancing, to cooperative rebalancing, you must follow a specific upgrade path to get all clients on the same <code>ConsumerPartitionAssignor</code> that supports the cooperative protocol. This can be done with two rolling bounces, using the <code>CooperativeStickyAssignor</code> for the example: during the first one, add &ldquo;cooperative-sticky&rdquo; to the list of supported assignors for each member (without removing the previous assignor &ndash; note that if previously using the default, you must include that explicitly as well). You then bounce and/or upgrade it. Once the entire group is on 2.4+ and all members have the &ldquo;cooperative-sticky&rdquo; among their supported assignors, remove the other assignor(s) and perform a second rolling bounce so that by the end all members support only the cooperative protocol. For further details on the cooperative rebalancing protocol and upgrade path, see <a href=https://cwiki.apache.org/confluence/x/vAclBg>KIP-429</a>.</li><li>There are some behavioral changes to the <code>ConsumerRebalanceListener</code>, as well as a new API. Exceptions thrown during any of the listener&rsquo;s three callbacks will no longer be swallowed, and will instead be re-thrown all the way up to the <code>Consumer.poll()</code> call. The <code>onPartitionsLost</code> method has been added to allow users to react to abnormal circumstances where a consumer may have lost ownership of its partitions (such as a missed rebalance) and cannot commit offsets. By default, this will simply call the existing <code>onPartitionsRevoked</code> API to align with previous behavior. Note however that <code>onPartitionsLost</code> will not be called when the set of lost partitions is empty. This means that no callback will be invoked at the beginning of the first rebalance of a new consumer joining the group.<br>The semantics of the <code>ConsumerRebalanceListener's</code> callbacks are further changed when following the cooperative rebalancing protocol described above. In addition to <code>onPartitionsLost</code>, <code>onPartitionsRevoked</code> will also never be called when the set of revoked partitions is empty. The callback will generally be invoked only at the end of a rebalance, and only on the set of partitions that are being moved to another consumer. The <code>onPartitionsAssigned</code> callback will however always be called, even with an empty set of partitions, as a way to notify users of a rebalance event (this is true for both cooperative and eager). For details on the new callback semantics, see the <a href=/https://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html>ConsumerRebalanceListener javadocs</a>.</li><li>The Scala trait <code>kafka.security.auth.Authorizer</code> has been deprecated and replaced with a new Java API <code>org.apache.kafka.server.authorizer.Authorizer</code>. The authorizer implementation class <code>kafka.security.auth.SimpleAclAuthorizer</code> has also been deprecated and replaced with a new implementation <code>kafka.security.authorizer.AclAuthorizer</code>. <code>AclAuthorizer</code> uses features supported by the new API to improve authorization logging and is compatible with <code>SimpleAclAuthorizer</code>. For more details, see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface>KIP-504</a>.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-20x-or-21x-or-22x-to-230>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x or 2.2.x to 2.3.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-20x-or-21x-or-22x-to-230 aria-label="Heading self-link"></a></h2><p><strong>If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0, 1.1).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x, 1.0.x, 1.1.x, 2.0.x, or 2.1.x, and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0, 1.0, 1.1, 2.0, 2.1, 2.2).</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 2.3.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.3 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><h3 id=notable-changes-in-230>Notable changes in 2.3.0<a class=td-heading-self-link href=#notable-changes-in-230 aria-label="Heading self-link"></a></h3><ul><li>We are introducing a new rebalancing protocol for Kafka Connect based on <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect>incremental cooperative rebalancing</a>. The new protocol does not require stopping all the tasks during a rebalancing phase between Connect workers. Instead, only the tasks that need to be exchanged between workers are stopped and they are started in a follow up rebalance. The new Connect protocol is enabled by default beginning with 2.3.0. For more details on how it works and how to enable the old behavior of eager rebalancing, checkout <a href=/26/#connect_administration>incremental cooperative rebalancing design</a>.</li><li>We are introducing static membership towards consumer user. This feature reduces unnecessary rebalances during normal application upgrades or rolling bounces. For more details on how to use it, checkout <a href=/26/#static_membership>static membership design</a>.</li><li>Kafka Streams DSL switches its used store types. While this change is mainly transparent to users, there are some corner cases that may require code changes. See the <a href=/26/streams/upgrade-guide/#streams_api_changes_230>Kafka Streams upgrade section</a> for more details.</li><li>Kafka Streams 2.3.0 requires 0.11 message format or higher and does not work with older message format.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-20x-or-21x-to-220>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x to 2.2.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-20x-or-21x-to-220 aria-label="Heading self-link"></a></h2><p><strong>If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0, 1.1).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x, 1.0.x, 1.1.x, or 2.0.x and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0, 1.0, 1.1, 2.0).</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 2.2.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.2 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><h3 id=notable-changes-in-221>Notable changes in 2.2.1<a class=td-heading-self-link href=#notable-changes-in-221 aria-label="Heading self-link"></a></h3><ul><li>Kafka Streams 2.2.1 requires 0.11 message format or higher and does not work with older message format.</li></ul><h3 id=notable-changes-in-220>Notable changes in 2.2.0<a class=td-heading-self-link href=#notable-changes-in-220 aria-label="Heading self-link"></a></h3><ul><li>The default consumer group id has been changed from the empty string (<code>""</code>) to <code>null</code>. Consumers who use the new default group id will not be able to subscribe to topics, and fetch or commit offsets. The empty string as consumer group id is deprecated but will be supported until a future major release. Old clients that rely on the empty string group id will now have to explicitly provide it as part of their consumer config. For more information see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-289%3A+Improve+the+default+group+id+behavior+in+KafkaConsumer>KIP-289</a>.</li><li>The <code>bin/kafka-topics.sh</code> command line tool is now able to connect directly to brokers with <code>--bootstrap-server</code> instead of zookeeper. The old <code>--zookeeper</code> option is still available for now. Please read <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-377%3A+TopicCommand+to+use+AdminClient>KIP-377</a> for more information.</li><li>Kafka Streams depends on a newer version of RocksDBs that requires MacOS 10.13 or higher.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-or-200-to-210>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, or 2.0.0 to 2.1.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-11x-or-200-to-210 aria-label="Heading self-link"></a></h2><p><strong>Note that 2.1.x contains a change to the internal schema used to store consumer offsets. Once the upgrade is complete, it will not be possible to downgrade to previous versions. See the rolling upgrade notes below for more detail.</strong></p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0, 1.1).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x, 1.0.x, 1.1.x, or 2.0.x and you have not overridden the message format, then you only need to override the inter-broker protocol version.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0, 1.0, 1.1, 2.0).</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. Once you have done so, the brokers will be running the latest version and you can verify that the cluster&rsquo;s behavior and performance meets expectations. It is still possible to downgrade at this point if there are any problems.</li><li>Once the cluster&rsquo;s behavior and performance has been verified, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 2.1.</li><li>Restart the brokers one by one for the new protocol version to take effect. Once the brokers begin using the latest protocol version, it will no longer be possible to downgrade the cluster to an older version.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.1 on each broker and restart them one by one. Note that the older Scala clients, which are no longer maintained, do not support the message format introduced in 0.11, so to avoid conversion costs (or to take advantage of exactly once semantics), the newer Java clients must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>Offset expiration semantics has slightly changed in this version. According to the new semantics, offsets of partitions in a group will not be removed while the group is subscribed to the corresponding topic and is still active (has active consumers). If group becomes empty all its offsets will be removed after default offset retention period (or the one set by broker) has passed (unless the group becomes active again). Offsets associated with standalone (simple) consumers, that do not use Kafka group management, will be removed after default offset retention period (or the one set by broker) has passed since their last commit.</li><li>The default for console consumer&rsquo;s <code>enable.auto.commit</code> property when no <code>group.id</code> is provided is now set to <code>false</code>. This is to avoid polluting the consumer coordinator cache as the auto-generated group is not likely to be used by other consumers.</li><li>The default value for the producer&rsquo;s <code>retries</code> config was changed to <code>Integer.MAX_VALUE</code>, as we introduced <code>delivery.timeout.ms</code> in <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-91+Provide+Intuitive+User+Timeouts+in+The+Producer>KIP-91</a>, which sets an upper bound on the total time between sending a record and receiving acknowledgement from the broker. By default, the delivery timeout is set to 2 minutes.</li><li>By default, MirrorMaker now overrides <code>delivery.timeout.ms</code> to <code>Integer.MAX_VALUE</code> when configuring the producer. If you have overridden the value of <code>retries</code> in order to fail faster, you will instead need to override <code>delivery.timeout.ms</code>.</li><li>The <code>ListGroup</code> API now expects, as a recommended alternative, <code>Describe Group</code> access to the groups a user should be able to list. Even though the old <code>Describe Cluster</code> access is still supported for backward compatibility, using it for this API is not advised.</li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=87298242">KIP-336</a> deprecates the ExtendedSerializer and ExtendedDeserializer interfaces and propagates the usage of Serializer and Deserializer. ExtendedSerializer and ExtendedDeserializer were introduced with <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers>KIP-82</a> to provide record headers for serializers and deserializers in a Java 7 compatible fashion. Now we consolidated these interfaces as Java 7 support has been dropped since.</li></ol><h3 id=notable-changes-in-210>Notable changes in 2.1.0<a class=td-heading-self-link href=#notable-changes-in-210 aria-label="Heading self-link"></a></h3><ul><li>Jetty has been upgraded to 9.4.12, which excludes TLS_RSA_* ciphers by default because they do not support forward secrecy, see <a href=https://github.com/eclipse/jetty.project/issues/2807>https://github.com/eclipse/jetty.project/issues/2807</a> for more information.</li><li>Unclean leader election is automatically enabled by the controller when <code>unclean.leader.election.enable</code> config is dynamically updated by using per-topic config override.</li><li>The <code>AdminClient</code> has added a method <code>AdminClient#metrics()</code>. Now any application using the <code>AdminClient</code> can gain more information and insight by viewing the metrics captured from the <code>AdminClient</code>. For more information see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-324%3A+Add+method+to+get+metrics%28%29+in+AdminClient>KIP-324</a></li><li>Kafka now supports Zstandard compression from <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression>KIP-110</a>. You must upgrade the broker as well as clients to make use of it. Consumers prior to 2.1.0 will not be able to read from topics which use Zstandard compression, so you should not enable it for a topic until all downstream consumers are upgraded. See the KIP for more detail.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-or-11x-to-200>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, or 1.1.x to 2.0.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-0110x-10x-or-11x-to-200 aria-label="Heading self-link"></a></h2><p>Kafka 2.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 2.0.0 before upgrading.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0, 1.1).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x, 1.0.x, or 1.1.x and you have not overridden the message format, then you only need to override the inter-broker protocol format.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0, 1.0, 1.1).</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 2.0.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 2.0 on each broker and restart them one by one. Note that the older Scala consumer does not support the new message format introduced in 0.11, so to avoid the performance cost of down-conversion (or to take advantage of exactly once semantics), the newer Java consumer must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>If you are willing to accept downtime, you can simply take all the brokers down, update the code and start them back up. They will start with the new protocol by default.</li><li>Bumping the protocol version and restarting can be done any time after the brokers are upgraded. It does not have to be immediately after. Similarly for the message format version.</li><li>If you are using Java8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities. Hot-swapping the jar-file only might not work.</li><li>ACLs should not be added to prefixed resources, (added in <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs>KIP-290</a>), until all brokers in the cluster have been updated.</li></ol><p><strong>NOTE:</strong> any prefixed ACLs added to a cluster, even after the cluster is fully upgraded, will be ignored should the cluster be downgraded again.</p><h3 id=notable-changes-in-200>Notable changes in 2.0.0<a class=td-heading-self-link href=#notable-changes-in-200 aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/x/oYtjB>KIP-186</a> increases the default offset retention time from 1 day to 7 days. This makes it less likely to &ldquo;lose&rdquo; offsets in an application that commits infrequently. It also increases the active set of offsets and therefore can increase memory usage on the broker. Note that the console consumer currently enables offset commit by default and can be the source of a large number of offsets which this change will now preserve for 7 days instead of 1. You can preserve the existing behavior by setting the broker config <code>offsets.retention.minutes</code> to 1440.</li><li>Support for Java 7 has been dropped, Java 8 is now the minimum version required.</li><li>The default value for <code>ssl.endpoint.identification.algorithm</code> was changed to <code>https</code>, which performs hostname verification (man-in-the-middle attacks are possible otherwise). Set <code>ssl.endpoint.identification.algorithm</code> to an empty string to restore the previous behaviour.</li><li><a href=https://issues.apache.org/jira/browse/KAFKA-5674>KAFKA-5674</a> extends the lower interval of <code>max.connections.per.ip</code> minimum to zero and therefore allows IP-based filtering of inbound connections.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-272%3A+Add+API+version+tag+to+broker%27s+RequestsPerSec+metric>KIP-272</a> added API version tag to the metric <code>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce|FetchConsumer|FetchFollower|...}</code>. This metric now becomes <code>kafka.network:type=RequestMetrics,name=RequestsPerSec,request={Produce|FetchConsumer|FetchFollower|...},version={0|1|2|3|...}</code>. This will impact JMX monitoring tools that do not automatically aggregate. To get the total count for a specific request type, the tool needs to be updated to aggregate across different versions.</li><li><a href=https://cwiki.apache.org/confluence/x/uaBzB>KIP-225</a> changed the metric &ldquo;records.lag&rdquo; to use tags for topic and partition. The original version with the name format &ldquo;{topic}-{partition}.records-lag&rdquo; has been removed.</li><li>The Scala consumers, which have been deprecated since 0.11.0.0, have been removed. The Java consumer has been the recommended option since 0.10.0.0. Note that the Scala consumers in 1.1.0 (and older) will continue to work even if the brokers are upgraded to 2.0.0.</li><li>The Scala producers, which have been deprecated since 0.10.0.0, have been removed. The Java producer has been the recommended option since 0.9.0.0. Note that the behaviour of the default partitioner in the Java producer differs from the default partitioner in the Scala producers. Users migrating should consider configuring a custom partitioner that retains the previous behaviour. Note that the Scala producers in 1.1.0 (and older) will continue to work even if the brokers are upgraded to 2.0.0.</li><li>MirrorMaker and ConsoleConsumer no longer support the Scala consumer, they always use the Java consumer.</li><li>The ConsoleProducer no longer supports the Scala producer, it always uses the Java producer.</li><li>A number of deprecated tools that rely on the Scala clients have been removed: ReplayLogProducer, SimpleConsumerPerformance, SimpleConsumerShell, ExportZkOffsets, ImportZkOffsets, UpdateOffsetsInZK, VerifyConsumerRebalance.</li><li>The deprecated kafka.tools.ProducerPerformance has been removed, please use org.apache.kafka.tools.ProducerPerformance.</li><li>New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from older version.</li><li><a href=https://cwiki.apache.org/confluence/x/DVyHB>KIP-284</a> changed the retention time for Kafka Streams repartition topics by setting its default value to <code>Long.MAX_VALUE</code>.</li><li>Updated <code>ProcessorStateManager</code> APIs in Kafka Streams for registering state stores to the processor topology. For more details please read the Streams <a href=/26/streams/upgrade-guide/#streams_api_changes_200>Upgrade Guide</a>.</li><li>In earlier releases, Connect&rsquo;s worker configuration required the <code>internal.key.converter</code> and <code>internal.value.converter</code> properties. In 2.0, these are <a href=https://cwiki.apache.org/confluence/x/AZQ7B>no longer required</a> and default to the JSON converter. You may safely remove these properties from your Connect standalone and distributed worker configurations:<br><code>internal.key.converter=org.apache.kafka.connect.json.JsonConverter</code> <code>internal.key.converter.schemas.enable=false</code> <code>internal.value.converter=org.apache.kafka.connect.json.JsonConverter</code> <code>internal.value.converter.schemas.enable=false</code></li><li><a href=https://cwiki.apache.org/confluence/x/5kiHB>KIP-266</a> adds a new consumer configuration <code>default.api.timeout.ms</code> to specify the default timeout to use for <code>KafkaConsumer</code> APIs that could block. The KIP also adds overloads for such blocking APIs to support specifying a specific timeout to use for each of them instead of using the default timeout set by <code>default.api.timeout.ms</code>. In particular, a new <code>poll(Duration)</code> API has been added which does not block for dynamic partition assignment. The old <code>poll(long)</code> API has been deprecated and will be removed in a future version. Overloads have also been added for other <code>KafkaConsumer</code> methods like <code>partitionsFor</code>, <code>listTopics</code>, <code>offsetsForTimes</code>, <code>beginningOffsets</code>, <code>endOffsets</code> and <code>close</code> that take in a <code>Duration</code>.</li><li>Also as part of KIP-266, the default value of <code>request.timeout.ms</code> has been changed to 30 seconds. The previous value was a little higher than 5 minutes to account for maximum time that a rebalance would take. Now we treat the JoinGroup request in the rebalance as a special case and use a value derived from <code>max.poll.interval.ms</code> for the request timeout. All other request types use the timeout defined by <code>request.timeout.ms</code></li><li>The internal method <code>kafka.admin.AdminClient.deleteRecordsBefore</code> has been removed. Users are encouraged to migrate to <code>org.apache.kafka.clients.admin.AdminClient.deleteRecords</code>.</li><li>The AclCommand tool <code>--producer</code> convenience option uses the <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-277+-+Fine+Grained+ACL+for+CreateTopics+API>KIP-277</a> finer grained ACL on the given topic.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-176%3A+Remove+deprecated+new-consumer+option+for+tools>KIP-176</a> removes the <code>--new-consumer</code> option for all consumer based tools. This option is redundant since the new consumer is automatically used if &ndash;bootstrap-server is defined.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs>KIP-290</a> adds the ability to define ACLs on prefixed resources, e.g. any topic starting with &lsquo;foo&rsquo;.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-283%3A+Efficient+Memory+Usage+for+Down-Conversion>KIP-283</a> improves message down-conversion handling on Kafka broker, which has typically been a memory-intensive operation. The KIP adds a mechanism by which the operation becomes less memory intensive by down-converting chunks of partition data at a time which helps put an upper bound on memory consumption. With this improvement, there is a change in <code>FetchResponse</code> protocol behavior where the broker could send an oversized message batch towards the end of the response with an invalid offset. Such oversized messages must be ignored by consumer clients, as is done by <code>KafkaConsumer</code>.</li></ul><p>KIP-283 also adds new topic and broker configurations <code>message.downconversion.enable</code> and <code>log.message.downconversion.enable</code> respectively to control whether down-conversion is enabled. When disabled, broker does not perform any down-conversion and instead sends an <code>UNSUPPORTED_VERSION</code> error to the client.</p><ul><li>Dynamic broker configuration options can be stored in ZooKeeper using kafka-configs.sh before brokers are started. This option can be used to avoid storing clear passwords in server.properties as all password configs may be stored encrypted in ZooKeeper.</li><li>ZooKeeper hosts are now re-resolved if connection attempt fails. But if your ZooKeeper host names resolve to multiple addresses and some of them are not reachable, then you may need to increase the connection timeout <code>zookeeper.connection.timeout.ms</code>.</li></ul><h3 id=new-protocol-versions>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over>KIP-279</a>: OffsetsForLeaderEpochResponse v1 introduces a partition-level <code>leader_epoch</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication>KIP-219</a>: Bump up the protocol versions of non-cluster action requests and responses that are throttled on quota violation.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs>KIP-290</a>: Bump up the protocol versions ACL create, describe and delete requests and responses.</li></ul><h3 id=upgrading-a-11-kafka-streams-application>Upgrading a 1.1 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-11-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 1.1 to 2.0 does not require a broker upgrade. A Kafka Streams 2.0 application can connect to 2.0, 1.1, 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>Note that in 2.0 we have removed the public APIs that are deprecated prior to 1.0; users leveraging on those deprecated APIs need to make code changes accordingly. See <a href=/26/streams/upgrade-guide/#streams_api_changes_200>Streams API changes in 2.0.0</a> for more details.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-0110x-or-10x-to-11x>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, or 1.0.x to 1.1.x<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-0110x-or-10x-to-11x aria-label="Heading self-link"></a></h2><p>Kafka 1.1.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 1.1.0 before upgrading.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 1.0).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x or 1.0.x and you have not overridden the message format, then you only need to override the inter-broker protocol format.</li><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (0.11.0 or 1.0).</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 1.1.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 1.1 on each broker and restart them one by one. Note that the older Scala consumer does not support the new message format introduced in 0.11, so to avoid the performance cost of down-conversion (or to take advantage of exactly once semantics), the newer Java consumer must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>If you are willing to accept downtime, you can simply take all the brokers down, update the code and start them back up. They will start with the new protocol by default.</li><li>Bumping the protocol version and restarting can be done any time after the brokers are upgraded. It does not have to be immediately after. Similarly for the message format version.</li><li>If you are using Java8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguties. Hot-swapping the jar-file only might not work.</li></ol><h3 id=notable-changes-in-111>Notable changes in 1.1.1<a class=td-heading-self-link href=#notable-changes-in-111 aria-label="Heading self-link"></a></h3><ul><li>New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x</li><li>See the <a href=/26/streams/upgrade-guide/><strong>Kafka Streams upgrade guide</strong></a> for details about this new config.</li></ul><h3 id=notable-changes-in-110>Notable changes in 1.1.0<a class=td-heading-self-link href=#notable-changes-in-110 aria-label="Heading self-link"></a></h3><ul><li>The kafka artifact in Maven no longer depends on log4j or slf4j-log4j12. Similarly to the kafka-clients artifact, users can now choose the logging back-end by including the appropriate slf4j module (slf4j-log4j12, logback, etc.). The release tarball still includes log4j and slf4j-log4j12.</li><li><a href=https://cwiki.apache.org/confluence/x/uaBzB>KIP-225</a> changed the metric &ldquo;records.lag&rdquo; to use tags for topic and partition. The original version with the name format &ldquo;{topic}-{partition}.records-lag&rdquo; is deprecated and will be removed in 2.0.0.</li><li>Kafka Streams is more robust against broker communication errors. Instead of stopping the Kafka Streams client with a fatal exception, Kafka Streams tries to self-heal and reconnect to the cluster. Using the new <code>AdminClient</code> you have better control of how often Kafka Streams retries and can <a href=/26/streams/developer-guide/config-streams/>configure</a> fine-grained timeouts (instead of hard coded retries as in older version).</li><li>Kafka Streams rebalance time was reduced further making Kafka Streams more responsive.</li><li>Kafka Connect now supports message headers in both sink and source connectors, and to manipulate them via simple message transforms. Connectors must be changed to explicitly use them. A new <code>HeaderConverter</code> is introduced to control how headers are (de)serialized, and the new &ldquo;SimpleHeaderConverter&rdquo; is used by default to use string representations of values.</li><li>kafka.tools.DumpLogSegments now automatically sets deep-iteration option if print-data-log is enabled explicitly or implicitly due to any of the other options like decoder.</li></ul><h3 id=new-protocol-versions-1>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions-1 aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-226+-+Dynamic+Broker+Configuration>KIP-226</a> introduced DescribeConfigs Request/Response v1.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability>KIP-227</a> introduced Fetch Request/Response v7.</li></ul><h3 id=upgrading-a-10-kafka-streams-application>Upgrading a 1.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-10-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 1.0 to 1.1 does not require a broker upgrade. A Kafka Streams 1.1 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_110>Streams API changes in 1.1.0</a> for more details.</li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-0102x-or-0110x-to-100>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x or 0.11.0.x to 1.0.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-0102x-or-0110x-to-100 aria-label="Heading self-link"></a></h2><p>Kafka 1.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 1.0.0 before upgrading.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use. If you have previously overridden the message format version, you should keep its current value. Alternatively, if you are upgrading from a version prior to 0.11.0.x, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1, 0.10.2, 0.11.0).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)
If you are upgrading from 0.11.0.x and you have not overridden the message format, you must set both the message format version and the inter-broker protocol version to 0.11.0.</li><li>inter.broker.protocol.version=0.11.0</li><li>log.message.format.version=0.11.0</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 1.0.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If you have overridden the message format version as instructed above, then you need to do one more rolling restart to upgrade it to its latest version. Once all (or most) consumers have been upgraded to 0.11.0 or later, change log.message.format.version to 1.0 on each broker and restart them one by one. If you are upgrading from 0.11.0 and log.message.format.version is set to 0.11.0, you can update the config and skip the rolling restart. Note that the older Scala consumer does not support the new message format introduced in 0.11, so to avoid the performance cost of down-conversion (or to take advantage of exactly once semantics), the newer Java consumer must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>If you are willing to accept downtime, you can simply take all the brokers down, update the code and start them back up. They will start with the new protocol by default.</li><li>Bumping the protocol version and restarting can be done any time after the brokers are upgraded. It does not have to be immediately after. Similarly for the message format version.</li></ol><h3 id=notable-changes-in-102>Notable changes in 1.0.2<a class=td-heading-self-link href=#notable-changes-in-102 aria-label="Heading self-link"></a></h3><ul><li>New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x</li><li>See the <a href=/26/streams/upgrade-guide/><strong>Kafka Streams upgrade guide</strong></a> for details about this new config.</li></ul><h3 id=notable-changes-in-101>Notable changes in 1.0.1<a class=td-heading-self-link href=#notable-changes-in-101 aria-label="Heading self-link"></a></h3><ul><li>Restored binary compatibility of AdminClient&rsquo;s Options classes (e.g. CreateTopicsOptions, DeleteTopicsOptions, etc.) with 0.11.0.x. Binary (but not source) compatibility had been broken inadvertently in 1.0.0.</li></ul><h3 id=notable-changes-in-100>Notable changes in 1.0.0<a class=td-heading-self-link href=#notable-changes-in-100 aria-label="Heading self-link"></a></h3><ul><li>Topic deletion is now enabled by default, since the functionality is now stable. Users who wish to to retain the previous behavior should set the broker config <code>delete.topic.enable</code> to <code>false</code>. Keep in mind that topic deletion removes data and the operation is not reversible (i.e. there is no &ldquo;undelete&rdquo; operation)</li><li>For topics that support timestamp search if no offset can be found for a partition, that partition is now included in the search result with a null offset value. Previously, the partition was not included in the map. This change was made to make the search behavior consistent with the case of topics not supporting timestamp search.</li><li>If the <code>inter.broker.protocol.version</code> is 1.0 or later, a broker will now stay online to serve replicas on live log directories even if there are offline log directories. A log directory may become offline due to IOException caused by hardware failure. Users need to monitor the per-broker metric <code>offlineLogDirectoryCount</code> to check whether there is offline log directory.</li><li>Added KafkaStorageException which is a retriable exception. KafkaStorageException will be converted to NotLeaderForPartitionException in the response if the version of client&rsquo;s FetchRequest or ProducerRequest does not support KafkaStorageException.</li><li>-XX:+DisableExplicitGC was replaced by -XX:+ExplicitGCInvokesConcurrent in the default JVM settings. This helps avoid out of memory exceptions during allocation of native memory by direct buffers in some cases.</li><li>The overridden <code>handleError</code> method implementations have been removed from the following deprecated classes in the <code>kafka.api</code> package: <code>FetchRequest</code>, <code>GroupCoordinatorRequest</code>, <code>OffsetCommitRequest</code>, <code>OffsetFetchRequest</code>, <code>OffsetRequest</code>, <code>ProducerRequest</code>, and <code>TopicMetadataRequest</code>. This was only intended for use on the broker, but it is no longer in use and the implementations have not been maintained. A stub implementation has been retained for binary compatibility.</li><li>The Java clients and tools now accept any string as a client-id.</li><li>The deprecated tool <code>kafka-consumer-offset-checker.sh</code> has been removed. Use <code>kafka-consumer-groups.sh</code> to get consumer group details.</li><li>SimpleAclAuthorizer now logs access denials to the authorizer log by default.</li><li>Authentication failures are now reported to clients as one of the subclasses of <code>AuthenticationException</code>. No retries will be performed if a client connection fails authentication.</li><li>Custom <code>SaslServer</code> implementations may throw <code>SaslAuthenticationException</code> to provide an error message to return to clients indicating the reason for authentication failure. Implementors should take care not to include any security-critical information in the exception message that should not be leaked to unauthenticated clients.</li><li>The <code>app-info</code> mbean registered with JMX to provide version and commit id will be deprecated and replaced with metrics providing these attributes.</li><li>Kafka metrics may now contain non-numeric values. <code>org.apache.kafka.common.Metric#value()</code> has been deprecated and will return <code>0.0</code> in such cases to minimise the probability of breaking users who read the value of every client metric (via a <code>MetricsReporter</code> implementation or by calling the <code>metrics()</code> method). <code>org.apache.kafka.common.Metric#metricValue()</code> can be used to retrieve numeric and non-numeric metric values.</li><li>Every Kafka rate metric now has a corresponding cumulative count metric with the suffix <code>-total</code> to simplify downstream processing. For example, <code>records-consumed-rate</code> has a corresponding metric named <code>records-consumed-total</code>.</li><li>Mx4j will only be enabled if the system property <code>kafka_mx4jenable</code> is set to <code>true</code>. Due to a logic inversion bug, it was previously enabled by default and disabled if <code>kafka_mx4jenable</code> was set to <code>true</code>.</li><li>The package <code>org.apache.kafka.common.security.auth</code> in the clients jar has been made public and added to the javadocs. Internal classes which had previously been located in this package have been moved elsewhere.</li><li>When using an Authorizer and a user doesn&rsquo;t have required permissions on a topic, the broker will return TOPIC_AUTHORIZATION_FAILED errors to requests irrespective of topic existence on broker. If the user have required permissions and the topic doesn&rsquo;t exists, then the UNKNOWN_TOPIC_OR_PARTITION error code will be returned.</li><li>config/consumer.properties file updated to use new consumer config properties.</li></ul><h3 id=new-protocol-versions-2>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions-2 aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD>KIP-112</a>: LeaderAndIsrRequest v1 introduces a partition-level <code>is_new</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD>KIP-112</a>: UpdateMetadataRequest v4 introduces a partition-level <code>offline_replicas</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD>KIP-112</a>: MetadataResponse v5 introduces a partition-level <code>offline_replicas</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD>KIP-112</a>: ProduceResponse v4 introduces error code for KafkaStorageException.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD>KIP-112</a>: FetchResponse v6 introduces error code for KafkaStorageException.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-152+-+Improve+diagnostics+for+SASL+authentication+failures>KIP-152</a>: SaslAuthenticate request has been added to enable reporting of authentication failures. This request will be used if the SaslHandshake request version is greater than 0.</li></ul><h3 id=upgrading-a-0110-kafka-streams-application>Upgrading a 0.11.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0110-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.11.0 to 1.0 does not require a broker upgrade. A Kafka Streams 1.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats.</li><li>If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed.</li><li>There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs. We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_100>Streams API changes in 1.0.0</a> for more details.</li></ul><h3 id=upgrading-a-0102-kafka-streams-application>Upgrading a 0.10.2 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0102-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade. A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed.</li><li>There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs. We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.</li><li>If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_0110>Streams API changes in 0.11.0</a> for more details.</li></ul><h3 id=upgrading-a-0101-kafka-streams-application>Upgrading a 0.10.1 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0101-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade. A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed.</li><li>There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs. We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.</li><li>If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated.</li><li>If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed.</li><li>If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_100>Streams API changes in 1.0.0</a>, <a href=/26/streams/upgrade-guide/#streams_api_changes_0110>Streams API changes in 0.11.0</a> and <a href=/26/streams/upgrade-guide/#streams_api_changes_0102>Streams API changes in 0.10.2</a> for more details.</li></ul><h3 id=upgrading-a-0100-kafka-streams-application>Upgrading a 0.10.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0100-kafka-streams-application aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.0 to 1.0 does require a broker upgrade because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers.</li><li>There are couple of API changes, that are not backward compatible (cf. <a href=/26/streams/upgrade-guide/#streams_api_changes_100>Streams API changes in 1.0.0</a>, <a href=/26/streams/#streams_api_changes_0110>Streams API changes in 0.11.0</a>, <a href=/26/streams/#streams_api_changes_0102>Streams API changes in 0.10.2</a>, and <a href=/26/streams/#streams_api_changes_0101>Streams API changes in 0.10.1</a> for more details). Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config <code>upgrade.from="0.10.0"</code> set for first upgrade phase (cf. <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade>KIP-268</a>). As an alternative, an offline upgrade is also possible.<ul><li>prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>"0.10.0"</code> for new version 0.11.0.3</li><li>bounce each instance of your application once</li><li>prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code></li><li>bounce each instance of your application once more to complete the upgrade</li></ul></li><li>Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)<ul><li>stop all old (0.10.0.x) application instances</li><li>update your code and swap old code and jar file with new code and new jar file</li><li>restart all new (1.0.0 or 1.0.1) application instances</li></ul></li></ul><h2 id=upgrading-from-08x-09x-0100x-0101x-or-0102x-to-01100>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-0101x-or-0102x-to-01100 aria-label="Heading self-link"></a></h2><p>Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 0.11.0.0 before upgrading.</p><p>Starting with version 0.10.2, Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.11.0 clients can talk to version 0.10.0 or newer brokers. However, if your brokers are older than 0.10.0, you must upgrade all the brokers in the Kafka cluster before upgrading your clients. Version 0.11.0 brokers support 0.8.x and newer clients.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the current message format version currently in use. If you have not overridden the message format previously, then CURRENT_MESSAGE_FORMAT_VERSION should be set to match CURRENT_KAFKA_VERSION.<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1 or 0.10.2).</li><li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing <code>inter.broker.protocol.version</code> and setting it to 0.11.0, but do not change <code>log.message.format.version</code> yet.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>Once all (or most) consumers have been upgraded to 0.11.0 or later, then change log.message.format.version to 0.11.0 on each broker and restart them one by one. Note that the older Scala consumer does not support the new message format, so to avoid the performance cost of down-conversion (or to take advantage of exactly once semantics), the new Java consumer must be used.</li></ol><p><strong>Additional Upgrade Notes:</strong></p><ol><li>If you are willing to accept downtime, you can simply take all the brokers down, update the code and start them back up. They will start with the new protocol by default.</li><li>Bumping the protocol version and restarting can be done any time after the brokers are upgraded. It does not have to be immediately after. Similarly for the message format version.</li><li>It is also possible to enable the 0.11.0 message format on individual topics using the topic admin tool (<code>bin/kafka-topics.sh</code>) prior to updating the global setting <code>log.message.format.version</code>.</li><li>If you are upgrading from a version prior to 0.10.0, it is NOT necessary to first update the message format to 0.10.0 before you switch to 0.11.0.</li></ol><h3 id=upgrading-a-0102-kafka-streams-application-1>Upgrading a 0.10.2 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0102-kafka-streams-application-1 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade. A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_0110>Streams API changes in 0.11.0</a> for more details.</li></ul><h3 id=upgrading-a-0101-kafka-streams-application-1>Upgrading a 0.10.1 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0101-kafka-streams-application-1 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade. A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated.</li><li>If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed.</li><li>If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_0110>Streams API changes in 0.11.0</a> and <a href=/26/streams/upgrade-guide/#streams_api_changes_0102>Streams API changes in 0.10.2</a> for more details.</li></ul><h3 id=upgrading-a-0100-kafka-streams-application-1>Upgrading a 0.10.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0100-kafka-streams-application-1 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.0 to 0.11.0 does require a broker upgrade because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers.</li><li>There are couple of API changes, that are not backward compatible (cf. <a href=/26/streams/#streams_api_changes_0110>Streams API changes in 0.11.0</a>, <a href=/26/streams/#streams_api_changes_0102>Streams API changes in 0.10.2</a>, and <a href=/26/streams/#streams_api_changes_0101>Streams API changes in 0.10.1</a> for more details). Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config <code>upgrade.from="0.10.0"</code> set for first upgrade phase (cf. <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade>KIP-268</a>). As an alternative, an offline upgrade is also possible.<ul><li>prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>"0.10.0"</code> for new version 0.11.0.3</li><li>bounce each instance of your application once</li><li>prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code></li><li>bounce each instance of your application once more to complete the upgrade</li></ul></li><li>Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)<ul><li>stop all old (0.10.0.x) application instances</li><li>update your code and swap old code and jar file with new code and new jar file</li><li>restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances</li></ul></li></ul><h3 id=notable-changes-in-01103>Notable changes in 0.11.0.3<a class=td-heading-self-link href=#notable-changes-in-01103 aria-label="Heading self-link"></a></h3><ul><li>New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x</li><li>See the <a href=/26/streams/upgrade-guide/><strong>Kafka Streams upgrade guide</strong></a> for details about this new config.</li></ul><h3 id=notable-changes-in-01100>Notable changes in 0.11.0.0<a class=td-heading-self-link href=#notable-changes-in-01100 aria-label="Heading self-link"></a></h3><ul><li>Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to to retain the previous behavior should set the broker config <code>unclean.leader.election.enable</code> to <code>true</code>.</li><li>Producer configs <code>block.on.buffer.full</code>, <code>metadata.fetch.timeout.ms</code> and <code>timeout.ms</code> have been removed. They were initially deprecated in Kafka 0.9.0.0.</li><li>The <code>offsets.topic.replication.factor</code> broker config is now enforced upon auto topic creation. Internal auto topic creation will fail with a GROUP_COORDINATOR_NOT_AVAILABLE error until the cluster size meets this replication factor requirement.</li><li>When compressing data with snappy, the producer and broker will use the compression scheme&rsquo;s default block size (2 x 32 KB) instead of 1 KB in order to improve the compression ratio. There have been reports of data compressed with the smaller block size being 50% larger than when compressed with the larger block size. For the snappy case, a producer with 5000 partitions will require an additional 315 MB of JVM heap.</li><li>Similarly, when compressing data with gzip, the producer and broker will use 8 KB instead of 1 KB as the buffer size. The default for gzip is excessively low (512 bytes).</li><li>The broker configuration <code>max.message.bytes</code> now applies to the total size of a batch of messages. Previously the setting applied to batches of compressed messages, or to non-compressed messages individually. A message batch may consist of only a single message, so in most cases, the limitation on the size of individual messages is only reduced by the overhead of the batch format. However, there are some subtle implications for message format conversion (see below for more detail). Note also that while previously the broker would ensure that at least one message is returned in each fetch request (regardless of the total and partition-level fetch sizes), the same behavior now applies to one message batch.</li><li>GC log rotation is enabled by default, see KAFKA-3754 for details.</li><li>Deprecated constructors of RecordMetadata, MetricName and Cluster classes have been removed.</li><li>Added user headers support through a new Headers interface providing user headers read and write access.</li><li>ProducerRecord and ConsumerRecord expose the new Headers API via <code>Headers headers()</code> method call.</li><li>ExtendedSerializer and ExtendedDeserializer interfaces are introduced to support serialization and deserialization for headers. Headers will be ignored if the configured serializer and deserializer are not the above classes.</li><li>A new config, <code>group.initial.rebalance.delay.ms</code>, was introduced. This config specifies the time, in milliseconds, that the <code>GroupCoordinator</code> will delay the initial consumer rebalance. The rebalance will be further delayed by the value of <code>group.initial.rebalance.delay.ms</code> as new members join the group, up to a maximum of <code>max.poll.interval.ms</code>. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.</li><li><code>org.apache.kafka.common.Cluster#partitionsForTopic</code>, <code>partitionsForNode</code> and <code>availablePartitionsForTopic</code> methods will return an empty list instead of <code>null</code> (which is considered a bad practice) in case the metadata for the required topic does not exist.</li><li>Streams API configuration parameters <code>timestamp.extractor</code>, <code>key.serde</code>, and <code>value.serde</code> were deprecated and replaced by <code>default.timestamp.extractor</code>, <code>default.key.serde</code>, and <code>default.value.serde</code>, respectively.</li><li>For offset commit failures in the Java consumer&rsquo;s <code>commitAsync</code> APIs, we no longer expose the underlying cause when instances of <code>RetriableCommitFailedException</code> are passed to the commit callback. See <a href=https://issues.apache.org/jira/browse/KAFKA-5052>KAFKA-5052</a> for more detail.</li></ul><h3 id=new-protocol-versions-3>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions-3 aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-107:+Add+purgeDataBefore%5C%28%5C%29+API+in+AdminClient>KIP-107</a>: FetchRequest v5 introduces a partition-level <code>log_start_offset</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-107:+Add+purgeDataBefore%5C%28%5C%29+API+in+AdminClient>KIP-107</a>: FetchResponse v5 introduces a partition-level <code>log_start_offset</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers>KIP-82</a>: ProduceRequest v3 introduces an array of <code>header</code> in the message protocol, containing <code>key</code> field and <code>value</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers>KIP-82</a>: FetchResponse v5 introduces an array of <code>header</code> in the message protocol, containing <code>key</code> field and <code>value</code> field.</li></ul><h3 id=notes-on-exactly-once-semantics>Notes on Exactly Once Semantics<a class=td-heading-self-link href=#notes-on-exactly-once-semantics aria-label="Heading self-link"></a></h3><p>Kafka 0.11.0 includes support for idempotent and transactional capabilities in the producer. Idempotent delivery ensures that messages are delivered exactly once to a particular topic partition during the lifetime of a single producer. Transactional delivery allows producers to send data to multiple partitions such that either all messages are successfully delivered, or none of them are. Together, these capabilities enable &ldquo;exactly once semantics&rdquo; in Kafka. More details on these features are available in the user guide, but below we add a few specific notes on enabling them in an upgraded cluster. Note that enabling EoS is not required and there is no impact on the broker&rsquo;s behavior if unused.</p><ol><li>Only the new Java producer and consumer support exactly once semantics.</li><li>These features depend crucially on the 0.11.0 message format. Attempting to use them on an older format will result in unsupported version errors.</li><li>Transaction state is stored in a new internal topic <code>__transaction_state</code>. This topic is not created until the the first attempt to use a transactional request API. Similar to the consumer offsets topic, there are several settings to control the topic&rsquo;s configuration. For example, <code>transaction.state.log.min.isr</code> controls the minimum ISR for this topic. See the configuration section in the user guide for a full list of options.</li><li>For secure clusters, the transactional APIs require new ACLs which can be turned on with the <code>bin/kafka-acls.sh</code>. tool.</li><li>EoS in Kafka introduces new request APIs and modifies several existing ones. See <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-RPCProtocolSummary>KIP-98</a> for the full details</li></ol><h3 id=notes-on-the-new-message-format-in-0110>Notes on the new message format in 0.11.0<a class=td-heading-self-link href=#notes-on-the-new-message-format-in-0110 aria-label="Heading self-link"></a></h3><p>The 0.11.0 message format includes several major enhancements in order to support better delivery semantics for the producer (see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging>KIP-98</a>) and improved replication fault tolerance (see <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation>KIP-101</a>). Although the new format contains more information to make these improvements possible, we have made the batch format much more efficient. As long as the number of messages per batch is more than 2, you can expect lower overall overhead. For smaller batches, however, there may be a small performance impact. See <a href=bit.ly/kafka-eos-perf>here</a> for the results of our initial performance analysis of the new message format. You can also find more detail on the message format in the <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-MessageFormat>KIP-98</a> proposal.</p><p>One of the notable differences in the new message format is that even uncompressed messages are stored together as a single batch. This has a few implications for the broker configuration <code>max.message.bytes</code>, which limits the size of a single batch. First, if an older client produces messages to a topic partition using the old format, and the messages are individually smaller than <code>max.message.bytes</code>, the broker may still reject them after they are merged into a single batch during the up-conversion process. Generally this can happen when the aggregate size of the individual messages is larger than <code>max.message.bytes</code>. There is a similar effect for older consumers reading messages down-converted from the new format: if the fetch size is not set at least as large as <code>max.message.bytes</code>, the consumer may not be able to make progress even if the individual uncompressed messages are smaller than the configured fetch size. This behavior does not impact the Java client for 0.10.1.0 and later since it uses an updated fetch protocol which ensures that at least one message can be returned even if it exceeds the fetch size. To get around these problems, you should ensure 1) that the producer&rsquo;s batch size is not set larger than <code>max.message.bytes</code>, and 2) that the consumer&rsquo;s fetch size is set at least as large as <code>max.message.bytes</code>.</p><p>Most of the discussion on the performance impact of upgrading to the 0.10.0 message format remains pertinent to the 0.11.0 upgrade. This mainly affects clusters that are not secured with TLS since &ldquo;zero-copy&rdquo; transfer is already not possible in that case. In order to avoid the cost of down-conversion, you should ensure that consumer applications are upgraded to the latest 0.11.0 client. Significantly, since the old consumer has been deprecated in 0.11.0.0, it does not support the new message format. You must upgrade to use the new consumer to use the new message format without the cost of down-conversion. Note that 0.11.0 consumers support backwards compatibility with 0.10.0 brokers and upward, so it is possible to upgrade the clients first before the brokers.</p><h2 id=upgrading-from-08x-09x-0100x-or-0101x-to-01020>Upgrading from 0.8.x, 0.9.x, 0.10.0.x or 0.10.1.x to 0.10.2.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-0100x-or-0101x-to-01020 aria-label="Heading self-link"></a></h2><p>0.10.2.0 has wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 0.10.2.0 before upgrading.</p><p>Starting with version 0.10.2, Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.10.2 clients can talk to version 0.10.0 or newer brokers. However, if your brokers are older than 0.10.0, you must upgrade all the brokers in the Kafka cluster before upgrading your clients. Version 0.10.2 brokers support 0.8.x and newer clients.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following properties:<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0 or 0.10.1).</li><li>log.message.format.version=CURRENT_KAFKA_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.2.</li><li>If your previous message format is 0.10.0, change log.message.format.version to 0.10.2 (this is a no-op as the message format is the same for 0.10.0, 0.10.1 and 0.10.2). If your previous message format version is lower than 0.10.0, do not change log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0 or later.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If log.message.format.version is still lower than 0.10.0 at this point, wait until all consumers have been upgraded to 0.10.0 or later, then change log.message.format.version to 0.10.2 on each broker and restart them one by one.</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=upgrading-a-0101-kafka-streams-application-2>Upgrading a 0.10.1 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0101-kafka-streams-application-2 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.1 to 0.10.2 does not require a broker upgrade. A Kafka Streams 0.10.2 application can connect to 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed.</li><li>If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed.</li><li>See <a href=/26/streams/upgrade-guide/#streams_api_changes_0102>Streams API changes in 0.10.2</a> for more details.</li></ul><h3 id=upgrading-a-0100-kafka-streams-application-2>Upgrading a 0.10.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0100-kafka-streams-application-2 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.0 to 0.10.2 does require a broker upgrade because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers.</li><li>There are couple of API changes, that are not backward compatible (cf. <a href=/26/streams/#streams_api_changes_0102>Streams API changes in 0.10.2</a> for more details). Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from="0.10.0"</code> set for first upgrade phase (cf. <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade>KIP-268</a>). As an alternative, an offline upgrade is also possible.<ul><li>prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>"0.10.0"</code> for new version 0.10.2.2</li><li>bounce each instance of your application once</li><li>prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code></li><li>bounce each instance of your application once more to complete the upgrade</li></ul></li><li>Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)<ul><li>stop all old (0.10.0.x) application instances</li><li>update your code and swap old code and jar file with new code and new jar file</li><li>restart all new (0.10.2.0 or 0.10.2.1) application instances</li></ul></li></ul><h3 id=notable-changes-in-01022>Notable changes in 0.10.2.2<a class=td-heading-self-link href=#notable-changes-in-01022 aria-label="Heading self-link"></a></h3><ul><li>New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x</li></ul><h3 id=notable-changes-in-01021>Notable changes in 0.10.2.1<a class=td-heading-self-link href=#notable-changes-in-01021 aria-label="Heading self-link"></a></h3><ul><li>The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code> default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.</li></ul><h3 id=notable-changes-in-01020>Notable changes in 0.10.2.0<a class=td-heading-self-link href=#notable-changes-in-01020 aria-label="Heading self-link"></a></h3><ul><li>The Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.10.2 clients can talk to version 0.10.0 or newer brokers. Note that some features are not available or are limited when older brokers are used.</li><li>Several methods on the Java consumer may now throw <code>InterruptException</code> if the calling thread is interrupted. Please refer to the <code>KafkaConsumer</code> Javadoc for a more in-depth explanation of this change.</li><li>Java consumer now shuts down gracefully. By default, the consumer waits up to 30 seconds to complete pending requests. A new close API with timeout has been added to <code>KafkaConsumer</code> to control the maximum wait time.</li><li>Multiple regular expressions separated by commas can be passed to MirrorMaker with the new Java consumer via the &ndash;whitelist option. This makes the behaviour consistent with MirrorMaker when used the old Scala consumer.</li><li>Upgrading your Streams application from 0.10.1 to 0.10.2 does not require a broker upgrade. A Kafka Streams 0.10.2 application can connect to 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).</li><li>The Zookeeper dependency was removed from the Streams API. The Streams API now uses the Kafka protocol to manage internal topics instead of modifying Zookeeper directly. This eliminates the need for privileges to access Zookeeper directly and &ldquo;StreamsConfig.ZOOKEEPER_CONFIG&rdquo; should not be set in the Streams app any more. If the Kafka cluster is secured, Streams apps must have the required security privileges to create new topics.</li><li>Several new fields including &ldquo;security.protocol&rdquo;, &ldquo;connections.max.idle.ms&rdquo;, &ldquo;retry.backoff.ms&rdquo;, &ldquo;reconnect.backoff.ms&rdquo; and &ldquo;request.timeout.ms&rdquo; were added to StreamsConfig class. User should pay attention to the default values and set these if needed. For more details please refer to <a href=/26/#streamsconfigs>3.5 Kafka Streams Configs</a>.</li></ul><h3 id=new-protocol-versions-4>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions-4 aria-label="Heading self-link"></a></h3><ul><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-88%3A+OffsetFetch+Protocol+Update>KIP-88</a>: OffsetFetchRequest v2 supports retrieval of offsets for all topics if the <code>topics</code> array is set to <code>null</code>.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-88%3A+OffsetFetch+Protocol+Update>KIP-88</a>: OffsetFetchResponse v2 introduces a top-level <code>error_code</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic>KIP-103</a>: UpdateMetadataRequest v3 introduces a <code>listener_name</code> field to the elements of the <code>end_points</code> array.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-108%3A+Create+Topic+Policy>KIP-108</a>: CreateTopicsRequest v1 introduces a <code>validate_only</code> field.</li><li><a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-108%3A+Create+Topic+Policy>KIP-108</a>: CreateTopicsResponse v1 introduces an <code>error_message</code> field to the elements of the <code>topic_errors</code> array.</li></ul><h2 id=upgrading-from-08x-09x-or-0100x-to-01010>Upgrading from 0.8.x, 0.9.x or 0.10.0.X to 0.10.1.0<a class=td-heading-self-link href=#upgrading-from-08x-09x-or-0100x-to-01010 aria-label="Heading self-link"></a></h2><p>0.10.1.0 has wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please notice the Potential breaking changes in 0.10.1.0 before upgrade.<br>Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients (i.e. 0.10.1.x clients only support 0.10.1.x or later brokers while 0.10.1.x brokers also support older clients).</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following properties:<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2.0, 0.9.0.0 or 0.10.0.0).</li><li>log.message.format.version=CURRENT_KAFKA_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers one at a time: shut down the broker, update the code, and restart it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.1.0.</li><li>If your previous message format is 0.10.0, change log.message.format.version to 0.10.1 (this is a no-op as the message format is the same for both 0.10.0 and 0.10.1). If your previous message format version is lower than 0.10.0, do not change log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0 or later.</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>If log.message.format.version is still lower than 0.10.0 at this point, wait until all consumers have been upgraded to 0.10.0 or later, then change log.message.format.version to 0.10.1 on each broker and restart them one by one.</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-breaking-changes-in-01010>Potential breaking changes in 0.10.1.0<a class=td-heading-self-link href=#potential-breaking-changes-in-01010 aria-label="Heading self-link"></a></h3><ul><li>The log retention time is no longer based on last modified time of the log segments. Instead it will be based on the largest timestamp of the messages in a log segment.</li><li>The log rolling time is no longer depending on log segment create time. Instead it is now based on the timestamp in the messages. More specifically. if the timestamp of the first message in the segment is T, the log will be rolled out when a new message has a timestamp greater than or equal to T + log.roll.ms</li><li>The open file handlers of 0.10.0 will increase by ~33% because of the addition of time index files for each segment.</li><li>The time index and offset index share the same index size configuration. Since each time index entry is 1.5x the size of offset index entry. User may need to increase log.index.size.max.bytes to avoid potential frequent log rolling.</li><li>Due to the increased number of index files, on some brokers with large amount the log segments (e.g. >15K), the log loading process during the broker startup could be longer. Based on our experiment, setting the num.recovery.threads.per.data.dir to one may reduce the log loading time.</li></ul><h3 id=upgrading-a-0100-kafka-streams-application-3>Upgrading a 0.10.0 Kafka Streams Application<a class=td-heading-self-link href=#upgrading-a-0100-kafka-streams-application-3 aria-label="Heading self-link"></a></h3><ul><li>Upgrading your Streams application from 0.10.0 to 0.10.1 does require a broker upgrade because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers.</li><li>There are couple of API changes, that are not backward compatible (cf. <a href=/26/streams/upgrade-guide/#streams_api_changes_0101>Streams API changes in 0.10.1</a> for more details). Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application.</li><li>Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from="0.10.0"</code> set for first upgrade phase (cf. <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade>KIP-268</a>). As an alternative, an offline upgrade is also possible.<ul><li>prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>"0.10.0"</code> for new version 0.10.1.2</li><li>bounce each instance of your application once</li><li>prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code></li><li>bounce each instance of your application once more to complete the upgrade</li></ul></li><li>Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)<ul><li>stop all old (0.10.0.x) application instances</li><li>update your code and swap old code and jar file with new code and new jar file</li><li>restart all new (0.10.1.0 or 0.10.1.1) application instances</li></ul></li></ul><h3 id=notable-changes-in-01010>Notable changes in 0.10.1.0<a class=td-heading-self-link href=#notable-changes-in-01010 aria-label="Heading self-link"></a></h3><ul><li>The new Java consumer is no longer in beta and we recommend it for all new development. The old Scala consumers are still supported, but they will be deprecated in the next release and will be removed in a future major release.</li><li>The <code>--new-consumer</code>/<code>--new.consumer</code> switch is no longer required to use tools like MirrorMaker and the Console Consumer with the new consumer; one simply needs to pass a Kafka broker to connect to instead of the ZooKeeper ensemble. In addition, usage of the Console Consumer with the old consumer has been deprecated and it will be removed in a future major release.</li><li>Kafka clusters can now be uniquely identified by a cluster id. It will be automatically generated when a broker is upgraded to 0.10.1.0. The cluster id is available via the kafka.server:type=KafkaServer,name=ClusterId metric and it is part of the Metadata response. Serializers, client interceptors and metric reporters can receive the cluster id by implementing the ClusterResourceListener interface.</li><li>The BrokerState &ldquo;RunningAsController&rdquo; (value 4) has been removed. Due to a bug, a broker would only be in this state briefly before transitioning out of it and hence the impact of the removal should be minimal. The recommended way to detect if a given broker is the controller is via the kafka.controller:type=KafkaController,name=ActiveControllerCount metric.</li><li>The new Java Consumer now allows users to search offsets by timestamp on partitions.</li><li>The new Java Consumer now supports heartbeating from a background thread. There is a new configuration <code>max.poll.interval.ms</code> which controls the maximum time between poll invocations before the consumer will proactively leave the group (5 minutes by default). The value of the configuration <code>request.timeout.ms</code> must always be larger than <code>max.poll.interval.ms</code> because this is the maximum time that a JoinGroup request can block on the server while the consumer is rebalancing, so we have changed its default value to just above 5 minutes. Finally, the default value of <code>session.timeout.ms</code> has been adjusted down to 10 seconds, and the default value of <code>max.poll.records</code> has been changed to 500.</li><li>When using an Authorizer and a user doesn&rsquo;t have <strong>Describe</strong> authorization on a topic, the broker will no longer return TOPIC_AUTHORIZATION_FAILED errors to requests since this leaks topic names. Instead, the UNKNOWN_TOPIC_OR_PARTITION error code will be returned. This may cause unexpected timeouts or delays when using the producer and consumer since Kafka clients will typically retry automatically on unknown topic errors. You should consult the client logs if you suspect this could be happening.</li><li>Fetch responses have a size limit by default (50 MB for consumers and 10 MB for replication). The existing per partition limits also apply (1 MB for consumers and replication). Note that neither of these limits is an absolute maximum as explained in the next point.</li><li>Consumers and replicas can make progress if a message larger than the response/partition size limit is found. More concretely, if the first message in the first non-empty partition of the fetch is larger than either or both limits, the message will still be returned.</li><li>Overloaded constructors were added to <code>kafka.api.FetchRequest</code> and <code>kafka.javaapi.FetchRequest</code> to allow the caller to specify the order of the partitions (since order is significant in v3). The previously existing constructors were deprecated and the partitions are shuffled before the request is sent to avoid starvation issues.</li></ul><h3 id=new-protocol-versions-5>New Protocol Versions<a class=td-heading-self-link href=#new-protocol-versions-5 aria-label="Heading self-link"></a></h3><ul><li>ListOffsetRequest v1 supports accurate offset search based on timestamps.</li><li>MetadataResponse v2 introduces a new field: &ldquo;cluster_id&rdquo;.</li><li>FetchRequest v3 supports limiting the response size (in addition to the existing per partition limit), it returns messages bigger than the limits if required to make progress and the order of partitions in the request is now significant.</li><li>JoinGroup v1 introduces a new field: &ldquo;rebalance_timeout&rdquo;.</li></ul><h2 id=upgrading-from-08x-or-09x-to-01000>Upgrading from 0.8.x or 0.9.x to 0.10.0.0<a class=td-heading-self-link href=#upgrading-from-08x-or-09x-to-01000 aria-label="Heading self-link"></a></h2><p>0.10.0.0 has potential breaking changes (please review before upgrading) and possible performance impact following the upgrade. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.<br>Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.</p><p><strong>Notes to clients with version 0.9.0.0:</strong> Due to a bug introduced in 0.9.0.0, clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 <strong>before</strong> brokers are upgraded to 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following properties:<ul><li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2 or 0.9.0.0).</li><li>log.message.format.version=CURRENT_KAFKA_VERSION (See potential performance impact following the upgrade for the details on what this configuration does.)</li></ul></li><li>Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.0.0. NOTE: You shouldn&rsquo;t touch log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0</li><li>Restart the brokers one by one for the new protocol version to take effect.</li><li>Once all consumers have been upgraded to 0.10.0, change log.message.format.version to 0.10.0 on each broker and restart them one by one.</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-performance-impact-following-upgrade-to-01000>Potential performance impact following upgrade to 0.10.0.0<a class=td-heading-self-link href=#potential-performance-impact-following-upgrade-to-01000 aria-label="Heading self-link"></a></h3><p>The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can&rsquo;t use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore, it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.</p><p>For clients that are upgraded to 0.10.0.0, there is no performance impact.</p><p><strong>Note:</strong> By setting the message format version, one certifies that all existing messages are on or below that message format version. Otherwise consumers before 0.10.0.0 might break. In particular, after the message format is set to 0.10.0, one should not change it back to an earlier format as it may break consumers on versions before 0.10.0.0.</p><p><strong>Note:</strong> Due to the additional timestamp introduced in each message, producers sending small messages may see a message throughput degradation because of the increased overhead. Likewise, replication now transmits an additional 8 bytes per message. If you&rsquo;re running close to the network capacity of your cluster, it&rsquo;s possible that you&rsquo;ll overwhelm the network cards and see failures and performance issues due to the overload.</p><p><strong>Note:</strong> If you have enabled compression on producers, you may notice reduced producer throughput and/or lower compression rate on the broker in some cases. When receiving compressed messages, 0.10.0 brokers avoid recompressing the messages, which in general reduces the latency and improves the throughput. In certain cases, however, this may reduce the batching size on the producer, which could lead to worse throughput. If this happens, users can tune linger.ms and batch.size of the producer for better throughput. In addition, the producer buffer used for compressing messages with snappy is smaller than the one used by the broker, which may have a negative impact on the compression ratio for the messages on disk. We intend to make this configurable in a future Kafka release.</p><h3 id=potential-breaking-changes-in-01000>Potential breaking changes in 0.10.0.0<a class=td-heading-self-link href=#potential-breaking-changes-in-01000 aria-label="Heading self-link"></a></h3><ul><li>Starting from Kafka 0.10.0.0, the message format version in Kafka is represented as the Kafka version. For example, message format 0.9.0 refers to the highest message version supported by Kafka 0.9.0.</li><li>Message format 0.10.0 has been introduced and it is used by default. It includes a timestamp field in the messages and relative offsets are used for compressed messages.</li><li>ProduceRequest/Response v2 has been introduced and it is used by default to support message format 0.10.0</li><li>FetchRequest/Response v2 has been introduced and it is used by default to support message format 0.10.0</li><li>MessageFormatter interface was changed from <code>def writeTo(key: Array[Byte], value: Array[Byte], output: PrintStream)</code> to <code>def writeTo(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]], output: PrintStream)</code></li><li>MessageReader interface was changed from <code>def readMessage(): KeyedMessage[Array[Byte], Array[Byte]]</code> to <code>def readMessage(): ProducerRecord[Array[Byte], Array[Byte]]</code></li><li>MessageFormatter&rsquo;s package was changed from <code>kafka.tools</code> to <code>kafka.common</code></li><li>MessageReader&rsquo;s package was changed from <code>kafka.tools</code> to <code>kafka.common</code></li><li>MirrorMakerMessageHandler no longer exposes the <code>handle(record: MessageAndMetadata[Array[Byte], Array[Byte]])</code> method as it was never called.</li><li>The 0.7 KafkaMigrationTool is no longer packaged with Kafka. If you need to migrate from 0.7 to 0.10.0, please migrate to 0.8 first and then follow the documented upgrade process to upgrade from 0.8 to 0.10.0.</li><li>The new consumer has standardized its APIs to accept <code>java.util.Collection</code> as the sequence type for method parameters. Existing code may have to be updated to work with the 0.10.0 client library.</li><li>LZ4-compressed message handling was changed to use an interoperable framing specification (LZ4f v1.5.1). To maintain compatibility with old clients, this change only applies to Message format 0.10.0 and later. Clients that Produce/Fetch LZ4-compressed messages using v0/v1 (Message format 0.9.0) should continue to use the 0.9.0 framing implementation. Clients that use Produce/Fetch protocols v2 or later should use interoperable LZ4f framing. A list of interoperable LZ4 libraries is available at <a href=http://www.lz4.org/>http://www.lz4.org/</a></li></ul><h3 id=notable-changes-in-01000>Notable changes in 0.10.0.0<a class=td-heading-self-link href=#notable-changes-in-01000 aria-label="Heading self-link"></a></h3><ul><li>Starting from Kafka 0.10.0.0, a new client library named <strong>Kafka Streams</strong> is available for stream processing on data stored in Kafka topics. This new client library only works with 0.10.x and upward versioned brokers due to message format changes mentioned above. For more information please read <a href=/26/streams/>Streams documentation</a>.</li><li>The default value of the configuration parameter <code>receive.buffer.bytes</code> is now 64K for the new consumer.</li><li>The new consumer now exposes the configuration parameter <code>exclude.internal.topics</code> to restrict internal topics (such as the consumer offsets topic) from accidentally being included in regular expression subscriptions. By default, it is enabled.</li><li>The old Scala producer has been deprecated. Users should migrate their code to the Java producer included in the kafka-clients JAR as soon as possible.</li><li>The new consumer API has been marked stable.</li></ul><h2 id=upgrading-from-080-081x-or-082x-to-0900>Upgrading from 0.8.0, 0.8.1.X, or 0.8.2.X to 0.9.0.0<a class=td-heading-self-link href=#upgrading-from-080-081x-or-082x-to-0900 aria-label="Heading self-link"></a></h2><p>0.9.0.0 has potential breaking changes (please review before upgrading) and an inter-broker protocol change from previous versions. This means that upgraded brokers and clients may not be compatible with older versions. It is important that you upgrade your Kafka cluster before upgrading your clients. If you are using MirrorMaker downstream clusters should be upgraded first as well.</p><p><strong>For a rolling upgrade:</strong></p><ol><li>Update server.properties file on all brokers and add the following property: inter.broker.protocol.version=0.8.2.X</li><li>Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.</li><li>Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.9.0.0.</li><li>Restart the brokers one by one for the new protocol version to take effect</li></ol><p><strong>Note:</strong> If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.</p><p><strong>Note:</strong> Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.</p><h3 id=potential-breaking-changes-in-0900>Potential breaking changes in 0.9.0.0<a class=td-heading-self-link href=#potential-breaking-changes-in-0900 aria-label="Heading self-link"></a></h3><ul><li>Java 1.6 is no longer supported.</li><li>Scala 2.9 is no longer supported.</li><li>Broker IDs above 1000 are now reserved by default to automatically assigned broker IDs. If your cluster has existing broker IDs above that threshold make sure to increase the reserved.broker.max.id broker configuration property accordingly.</li><li>Configuration parameter replica.lag.max.messages was removed. Partition leaders will no longer consider the number of lagging messages when deciding which replicas are in sync.</li><li>Configuration parameter replica.lag.time.max.ms now refers not just to the time passed since last fetch request from replica, but also to time since the replica last caught up. Replicas that are still fetching messages from leaders but did not catch up to the latest messages in replica.lag.time.max.ms will be considered out of sync.</li><li>Compacted topics no longer accept messages without key and an exception is thrown by the producer if this is attempted. In 0.8.x, a message without key would cause the log compaction thread to subsequently complain and quit (and stop compacting all compacted topics).</li><li>MirrorMaker no longer supports multiple target clusters. As a result it will only accept a single &ndash;consumer.config parameter. To mirror multiple source clusters, you will need at least one MirrorMaker instance per source cluster, each with its own consumer configuration.</li><li>Tools packaged under <em>org.apache.kafka.clients.tools.*</em> have been moved to <em>org.apache.kafka.tools.*</em>. All included scripts will still function as usual, only custom code directly importing these classes will be affected.</li><li>The default Kafka JVM performance options (KAFKA_JVM_PERFORMANCE_OPTS) have been changed in kafka-run-class.sh.</li><li>The kafka-topics.sh script (kafka.admin.TopicCommand) now exits with non-zero exit code on failure.</li><li>The kafka-topics.sh script (kafka.admin.TopicCommand) will now print a warning when topic names risk metric collisions due to the use of a &lsquo;.&rsquo; or &lsquo;_&rsquo; in the topic name, and error in the case of an actual collision.</li><li>The kafka-console-producer.sh script (kafka.tools.ConsoleProducer) will use the Java producer instead of the old Scala producer be default, and users have to specify &lsquo;old-producer&rsquo; to use the old producer.</li><li>By default, all command line tools will print all logging messages to stderr instead of stdout.</li></ul><h3 id=notable-changes-in-0901>Notable changes in 0.9.0.1<a class=td-heading-self-link href=#notable-changes-in-0901 aria-label="Heading self-link"></a></h3><ul><li>The new broker id generation feature can be disabled by setting broker.id.generation.enable to false.</li><li>Configuration parameter log.cleaner.enable is now true by default. This means topics with a cleanup.policy=compact will now be compacted by default, and 128 MB of heap will be allocated to the cleaner process via log.cleaner.dedupe.buffer.size. You may want to review log.cleaner.dedupe.buffer.size and the other log.cleaner configuration values based on your usage of compacted topics.</li><li>Default value of configuration parameter fetch.min.bytes for the new consumer is now 1 by default.</li></ul><h3 id=deprecations-in-0900>Deprecations in 0.9.0.0<a class=td-heading-self-link href=#deprecations-in-0900 aria-label="Heading self-link"></a></h3><ul><li>Altering topic configuration from the kafka-topics.sh script (kafka.admin.TopicCommand) has been deprecated. Going forward, please use the kafka-configs.sh script (kafka.admin.ConfigCommand) for this functionality.</li><li>The kafka-consumer-offset-checker.sh (kafka.tools.ConsumerOffsetChecker) has been deprecated. Going forward, please use kafka-consumer-groups.sh (kafka.admin.ConsumerGroupCommand) for this functionality.</li><li>The kafka.tools.ProducerPerformance class has been deprecated. Going forward, please use org.apache.kafka.tools.ProducerPerformance for this functionality (kafka-producer-perf-test.sh will also be changed to use the new class).</li><li>The producer config block.on.buffer.full has been deprecated and will be removed in future release. Currently its default value has been changed to false. The KafkaProducer will no longer throw BufferExhaustedException but instead will use max.block.ms value to block, after which it will throw a TimeoutException. If block.on.buffer.full property is set to true explicitly, it will set the max.block.ms to Long.MAX_VALUE and metadata.fetch.timeout.ms will not be honoured</li></ul><h2 id=upgrading-from-081-to-082>Upgrading from 0.8.1 to 0.8.2<a class=td-heading-self-link href=#upgrading-from-081-to-082 aria-label="Heading self-link"></a></h2><p>0.8.2 is fully compatible with 0.8.1. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.</p><h2 id=upgrading-from-080-to-081>Upgrading from 0.8.0 to 0.8.1<a class=td-heading-self-link href=#upgrading-from-080-to-081 aria-label="Heading self-link"></a></h2><p>0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.</p><h2 id=upgrading-from-07>Upgrading from 0.7<a class=td-heading-self-link href=#upgrading-from-07 aria-label="Heading self-link"></a></h2><p>Release 0.7 is incompatible with newer releases. Major changes were made to the API, ZooKeeper data structures, and protocol, and configuration in order to add replication (Which was missing in 0.7). The upgrade from 0.7 to later versions requires a <a href=https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8>special tool</a> for migration. This migration can be done without downtime.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Contact aria-label=Contact><a target=_blank rel=noopener href=/community/contact/ aria-label=Contact><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/apachekafka aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Stack Overflow" aria-label="Stack Overflow"><a target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/apache-kafka aria-label="Stack Overflow"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/apache/kafka aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Developer mailing list" aria-label="Developer mailing list"><a target=_blank rel=noopener href=mailto:dev@kafka.apache.org aria-label="Developer mailing list"><i class="fa fa-envelope"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2014&ndash;2025
<span class=td-footer__authors>By <a href=https://www.apache.org/>Apache Software Foundation</a> under the terms of the <a href=https://www.apache.org/licenses/LICENSE-2.0>Apache License v2</a></span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://privacy.apache.org/policies/privacy-policy-public.html target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.dc2c0119076a0df855e55a8044ce0de74b7b9033c20e853e22d7ec7e9bdde965.js integrity="sha256-3CwBGQdqDfhV5VqARM4N50t7kDPCDoU+Itfsfpvd6WU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>