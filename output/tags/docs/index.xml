<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Docs on</title><link>https://example.kafka-site-md.dev/tags/docs/</link><description>Recent content in Docs on</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://example.kafka-site-md.dev/tags/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>API</title><link>https://example.kafka-site-md.dev/0100/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/apis/api/</guid><description>Kafka includes four core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/0101/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/apis/api/</guid><description>Kafka includes four core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/0102/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/apis/api/</guid><description>Kafka includes four core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/0110/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/08/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/apis/api/</guid><description>Producer API /** * V: type of the message * K: type of the optional key associated with the message */ class kafka.javaapi.producer.Producer&amp;lt;K,V&amp;gt; { public Producer(ProducerConfig config); /** * Sends the data to a single topic, partitioned by key, using either the * synchronous or the asynchronous producer * @param message the producer data object that encapsulates the topic, key and message data */ public void send(KeyedMessage&amp;lt;K,V&amp;gt; message); /** * Use this API to send data to multiple topics * @param messages list of producer data objects that encapsulate the topic, key and message data */ public void send(List&amp;lt;KeyedMessage&amp;lt;K,V&amp;gt;&amp;gt; messages); /** * Close API to close the producer pool connections to all Kafka brokers.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/081/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/apis/api/</guid><description>Producer API /** * V: type of the message * K: type of the optional key associated with the message */ class kafka.javaapi.producer.Producer&amp;lt;K,V&amp;gt; { public Producer(ProducerConfig config); /** * Sends the data to a single topic, partitioned by key, using either the * synchronous or the asynchronous producer * @param message the producer data object that encapsulates the topic, key and message data */ public void send(KeyedMessage&amp;lt;K,V&amp;gt; message); /** * Use this API to send data to multiple topics * @param messages list of producer data objects that encapsulate the topic, key and message data */ public void send(List&amp;lt;KeyedMessage&amp;lt;K,V&amp;gt;&amp;gt; messages); /** * Close API to close the producer pool connections to all Kafka brokers.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/082/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/apis/api/</guid><description>We are in the process of rewritting the JVM clients for Kafka. As of 0.8.2 Kafka includes a newly rewritten Java producer. The next release will include an equivalent Java consumer. These new clients are meant to supplant the existing Scala clients, but for compatability they will co-exist for some time. These clients are available in a seperate jar with minimal dependencies, while the old Scala clients remain packaged with the server.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/090/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/apis/api/</guid><description>Apache Kafka includes new java clients (in the org.apache.kafka.clients package). These are meant to supplant the older Scala clients, but for compatability they will co-exist for some time. These clients are available in a seperate jar with minimal dependencies, while the old Scala clients remain packaged with the server.
Producer API We encourage all new development to use the new Java producer. This client is production tested and generally both faster and more fully featured than the previous Scala client.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/10/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/11/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/20/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/21/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/22/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/23/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/24/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/25/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/26/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/27/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/28/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/30/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/31/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/32/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/33/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/34/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/35/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/36/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/37/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/38/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/39/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/40/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API</title><link>https://example.kafka-site-md.dev/41/apis/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/apis/api/</guid><description>Kafka includes five core apis:
The Producer API allows applications to send streams of data to topics in the Kafka cluster. The Consumer API allows applications to read streams of data from topics in the Kafka cluster. The Streams API allows transforming streams of data from input topics to output topics. The Connect API allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/0100/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/0101/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/0102/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/08/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/081/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/082/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>API Design</title><link>https://example.kafka-site-md.dev/090/implementation/api-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/api-design/</guid><description>API Design Producer APIs The Producer API that wraps the 2 low-level producers - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer.
class Producer { /* Sends the data, partitioned by key to the topic using either the */ /* synchronous or the asynchronous producer */ public void send(kafka.javaapi.producer.ProducerData&amp;lt;K,V&amp;gt; producerData); /* Sends a list of data, partitioned by key to the topic using either */ /* the synchronous or the asynchronous producer */ public void send(java.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/0100/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/0101/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/0102/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/0110/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/081/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/082/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/090/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/10/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/11/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/20/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/21/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/22/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/23/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/24/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/25/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/26/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/27/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/28/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/30/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/31/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/32/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/33/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/34/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/35/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/36/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/37/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/38/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/39/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/40/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Basic Kafka Operations</title><link>https://example.kafka-site-md.dev/41/operations/basic-kafka-operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/basic-kafka-operations/</guid><description>Basic Kafka Operations This section will review the most common operations you will perform on your Kafka cluster. All of the tools reviewed in this section are available under the bin/ directory of the Kafka distribution and each tool will print details on all possible commandline options if it is run with no arguments.
Adding and removing topics You have the option of either adding topics manually or having them be created automatically when data is first published to a non-existent topic.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0100/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0101/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0102/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0110/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/07/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/configuration/configuration/</guid><description>Configuration Important configuration properties for Kafka broker: More details about server configuration can be found in the scala class kafka.server.KafkaConfig.
name default description brokerid none Each broker is uniquely identified by an id. This id serves as the brokers &amp;ldquo;name&amp;rdquo;, and allows the broker to be moved to a different host/port without confusing consumers. enable.zookeeper true enable zookeeper registration in the server log.flush.interval 500 Controls the number of messages accumulated in each topic (partition) before the data is flushed to disk and made available to consumers.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/08/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/configuration/configuration/</guid><description>Kafka uses the property file format for configuration. These can be supplied either from a file or programmatically.
Some configurations have both a default global setting as well as a topic-level overrides. The topic level properties have the format of csv (e.g., &amp;ldquo;xyz.per.topic=topic1:value1,topic2:value2&amp;rdquo;) and they override the default value for the specified topics.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Property Default Description broker.id Each broker is uniquely identified by a non-negative integer id.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/081/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Property Default Description broker.id Each broker is uniquely identified by a non-negative integer id. This id serves as the broker&amp;rsquo;s &amp;ldquo;name&amp;rdquo; and allows the broker to be moved to a different host/port without confusing consumers.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/082/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Property Default Description broker.id Each broker is uniquely identified by a non-negative integer id. This id serves as the broker&amp;rsquo;s &amp;ldquo;name&amp;rdquo; and allows the broker to be moved to a different host/port without confusing consumers.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/090/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/10/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance zookeeper.connectZookeeper host stringstringhigh advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/11/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance Dynamic Update Mode zookeeper.connectZookeeper host stringstringhighread-only advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/20/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance Dynamic Update Mode zookeeper.connectZookeeper host stringstringhighread-only advertised.host.nameDEPRECATED: only used when `advertised.listeners` or `listeners` are not set. Use `advertised.listeners` instead. Hostname to publish to ZooKeeper for clients to use.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/21/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance Dynamic Update Mode zookeeper.connectSpecifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/22/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance Dynamic Update Mode zookeeper.connectSpecifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/23/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. Name Description Type Default Valid Values Importance Dynamic Update Mode zookeeper.connectSpecifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/24/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. zookeeper.connect: Specifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form hostname1:port1,hostname2:port2,hostname3:port3.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/25/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. zookeeper.connect Specifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form hostname1:port1,hostname2:port2,hostname3:port3.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/26/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. zookeeper.connect Specifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form hostname1:port1,hostname2:port2,hostname3:port3.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/27/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. zookeeper.connect Specifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server. To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down you can also specify multiple hosts in the form hostname1:port1,hostname2:port2,hostname3:port3.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/28/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.host.name DEPRECATED: only used when advertised.listeners or listeners are not set. Use advertised.listeners instead. Hostname to publish to ZooKeeper for clients to use. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/30/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/31/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/32/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/33/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs For ZooKeeper clusters, brokers must have the following configuration:
broker.id log.dirs zookeeper.connect For KRaft clusters, brokers and controllers must have the following configurations:
node.id log.dirs process.roles On KRaft brokers, if broker.id is set, it must be equal to node.id. Topic-level configurations and defaults are discussed in more detail below.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/34/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/35/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/36/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/37/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/38/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/39/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
broker.id log.dirs zookeeper.connect Topic-level configurations and defaults are discussed in more detail below. advertised.listeners Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. In IaaS environments, this may need to be different from the interface to which the broker binds.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/40/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
node.id log.dirs process.roles controller.quorum.bootstrap.servers Topic-level configurations and defaults are discussed in more detail below. node.id The node ID associated with the roles this process is playing when process.roles is non-empty. This is required configuration when running in KRaft mode.
Type:int Default: Valid Values:[0,.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/41/configuration/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/configuration/configuration/</guid><description>Kafka uses key-value pairs in the property file format for configuration. These values can be supplied either from a file or programmatically.
Broker Configs The essential configurations are the following:
node.id log.dirs process.roles controller.quorum.bootstrap.servers Topic configurations and defaults are discussed in more detail below. node.id The node ID associated with the roles this process is playing when process.roles is non-empty. This is required configuration when running in KRaft mode.
Type:int Default: Valid Values:[0,.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/08/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our approach to this is to deploy a local Kafka cluster in each datacenter and machines in each location interact only with their local cluster.
For applications that need a global view of all data we use the mirror maker tool to provide clusters which have aggregate data mirrored from all datacenters. These aggregator clusters are used for reads by applications that require this.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0100/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0101/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0102/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0110/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/08/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/081/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/082/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/090/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/10/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/11/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/20/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/21/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/22/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/23/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/24/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/25/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/26/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/27/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/28/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/30/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/31/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/32/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/33/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/34/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/35/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/36/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/37/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/38/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/39/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/40/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/41/design/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/design/design/</guid><description>Motivation We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.
It would have to have high-throughput to support high volume event streams such as real-time log aggregation.
It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</description></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/0100/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/0101/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/0102/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/0110/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/07/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/08/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/081/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/082/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/090/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/10/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/11/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/20/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/21/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/22/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/23/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/24/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/25/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/26/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/27/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/28/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/30/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/31/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/32/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/33/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/34/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/35/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/36/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/37/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/38/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/39/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/40/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/</guid><description/></item><item><title>Getting Started</title><link>https://example.kafka-site-md.dev/41/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/</guid><description/></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0100/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/introduction/</guid><description>Kafka is a distributed streaming platform. What exactly does that mean? We think of a streaming platform as having three key capabilities:
It let&amp;rsquo;s you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system. It let&amp;rsquo;s you store streams of records in a fault-tolerant way. It let&amp;rsquo;s you process streams of records as they occur. What is Kafka good for?</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0101/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/introduction/</guid><description>Kafka is a distributed streaming platform. What exactly does that mean? We think of a streaming platform as having three key capabilities:
It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system. It lets you store streams of records in a fault-tolerant way. It lets you process streams of records as they occur. What is Kafka good for?</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0102/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? We think of a streaming platform as having three key capabilities:
It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system. It lets you store streams of records in a fault-tolerant way. It lets you process streams of records as they occur. What is Kafka good for?</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0102/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/introduction/</guid><description>Streams Core Concepts Architecture Developer Guide Low-level Processor API High-level Streams DSL Application Configuration and Execution Upgrade Guide and API Changes Overview Kafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0110/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/0110/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/introduction/</guid><description>Kafka Streams API The easiest way to write mission-critical real-time applications and microservices with all the benefits of Kafka&amp;rsquo;s server-side cluster technology. Write your first app Play with demo app
Write standard Java applications Exactly-once processing semantics No seperate processing cluster required Develop on Mac, Linux, Windows Elastic, highly scalable, fault-tolerant Deploy to containers, VMs, bare metal, cloud Equally viable for small, medium, &amp;amp; large use cases Fully integrated with Kafka security Developer manual Tutorials Concepts Hello Kafka Streams The code example below implements a WordCount application that is elastic, highly scalable, fault-tolerant, stateful, and ready to run in production at large scale</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/08/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/getting-started/introduction/</guid><description>Introduction Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?
First let&amp;rsquo;s review some basic messaging terminology:
Kafka maintains feeds of messages in categories called topics. We&amp;rsquo;ll call processes that publish messages to a Kafka topic producers. We&amp;rsquo;ll call processes that subscribe to topics and process the feed of published messages consumers.. Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/081/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/introduction/</guid><description>Introduction Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?
First let&amp;rsquo;s review some basic messaging terminology:
Kafka maintains feeds of messages in categories called topics. We&amp;rsquo;ll call processes that publish messages to a Kafka topic producers. We&amp;rsquo;ll call processes that subscribe to topics and process the feed of published messages consumers.. Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/082/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/introduction/</guid><description>Introduction Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?
First let&amp;rsquo;s review some basic messaging terminology:
Kafka maintains feeds of messages in categories called topics. We&amp;rsquo;ll call processes that publish messages to a Kafka topic producers. We&amp;rsquo;ll call processes that subscribe to topics and process the feed of published messages consumers.. Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/090/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/introduction/</guid><description>Introduction Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.
What does all that mean?
First let&amp;rsquo;s review some basic messaging terminology:
Kafka maintains feeds of messages in categories called topics. We&amp;rsquo;ll call processes that publish messages to a Kafka topic producers. We&amp;rsquo;ll call processes that subscribe to topics and process the feed of published messages consumers.. Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/10/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/10/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/11/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/11/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/20/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/20/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/21/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/21/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/22/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/22/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/23/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/23/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/24/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/introduction/</guid><description>Apache Kafka is a distributed streaming platform. What exactly does that mean? A streaming platform has three key capabilities:
Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. Kafka is generally used for two broad classes of applications:
Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data To understand how Kafka does these things, let&amp;rsquo;s dive in and explore Kafka&amp;rsquo;s capabilities from the bottom up.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/24/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/25/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/25/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/26/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/26/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/27/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/27/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/28/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/28/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/30/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/30/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/31/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/31/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/32/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/32/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/33/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/33/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/34/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/34/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/35/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/35/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/36/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/36/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/37/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/37/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/38/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/38/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/39/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/39/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/40/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/40/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Javadoc Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/41/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/introduction/</guid><description>What is event streaming? Event streaming is the digital equivalent of the human body&amp;rsquo;s central nervous system. It is the technological foundation for the &amp;lsquo;always-on&amp;rsquo; world where businesses are increasingly software-defined and automated, and where the user of software is more software.
Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed.</description></item><item><title>Introduction</title><link>https://example.kafka-site-md.dev/41/streams/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/introduction/</guid><description>Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Javadoc Upgrade
The easiest way to write mission-critical real-time applications and microservices Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/0110/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/10/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/11/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/20/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/21/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/22/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/23/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/24/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/25/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/26/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/27/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/28/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/30/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/31/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/32/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/33/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/34/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/35/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/36/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/37/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/38/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/39/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/40/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/41/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the TransferableRecords interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/0100/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/0101/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/0102/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/0110/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/090/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/10/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/11/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/20/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/21/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/22/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/23/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/24/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/25/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/26/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/27/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/28/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/30/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/31/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/32/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/33/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/34/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/35/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/36/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/37/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/38/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/39/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/40/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Overview</title><link>https://example.kafka-site-md.dev/41/kafka-connect/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/kafka-connect/overview/</guid><description>Overview Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems. It makes it simple to quickly define connectors that move large collections of data into and out of Kafka. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency. An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/0100/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. These features are considered to be of beta quality. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL (Kerberos). SASL/PLAIN can also be used from release 0.10.0.0 onwards. Authentication of connections from brokers to ZooKeeper Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/0101/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. These features are considered to be of beta quality. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL (Kerberos). SASL/PLAIN can also be used from release 0.10.0.0 onwards. Authentication of connections from brokers to ZooKeeper Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/0102/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/0110/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/090/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. These features are considered to be of beta quality. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL (Kerberos) Authentication of connections from brokers to ZooKeeper Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/10/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/11/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/20/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/21/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/22/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/23/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/24/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/25/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/26/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/27/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/28/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/30/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/31/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/32/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/33/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/34/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/35/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/36/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/37/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/38/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/39/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/security-overview/</guid><description>Security Overview In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/40/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/security-overview/</guid><description>Security Overview The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0 SASL/OAUTHBEARER - starting at version 2.0 Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.</description></item><item><title>Security Overview</title><link>https://example.kafka-site-md.dev/41/security/security-overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/security-overview/</guid><description>Security Overview The following security measures are currently supported:
Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0 SASL/PLAIN - starting at version 0.10.0.0 SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0 SASL/OAUTHBEARER - starting at version 2.0 Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Any Java application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Any Java application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>Writing a Streams Application</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/write-streams-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/write-streams-app/</guid><description>Writing a Streams Application Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/0100/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/0101/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/0102/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/0110/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/07/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/08/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/081/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/082/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/090/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/10/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/11/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/20/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/21/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/22/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/23/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/24/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/25/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/26/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/27/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/28/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/30/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/31/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/32/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/33/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/34/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/35/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/36/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/37/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/38/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/39/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/40/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/apis/</guid><description/></item><item><title>APIs</title><link>https://example.kafka-site-md.dev/41/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/apis/</guid><description/></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a StreamsConfig instance.
Create a java.util.Properties instance.
Set the parameters.
Construct a StreamsConfig instance from the Properties instance. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , .</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a StreamsConfig instance.
Create a java.util.Properties instance.
Set the parameters.
Construct a StreamsConfig instance from the Properties instance. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , .</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Configuring a Streams Application</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/config-streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/config-streams/</guid><description>Configuring a Streams Application Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig;
Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;quot;my-first-streams-application&amp;quot;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;quot;kafka-broker1:9092&amp;quot;); // Any further settings settings.put(... , ...); Configuration parameter reference This section contains the most common Streams configuration parameters.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/0100/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/0101/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/0102/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/0110/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/081/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/082/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/090/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/10/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/11/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/20/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/21/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/22/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/23/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/24/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/25/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter with application instances in each datacenter interacting only with their local cluster and mirroring between clusters (see the documentation on the mirror maker tool for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/26/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/27/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/28/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/30/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/31/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/32/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/33/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/34/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/35/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/36/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/37/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/38/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/39/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/40/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Datacenters</title><link>https://example.kafka-site-md.dev/41/operations/datacenters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/datacenters/</guid><description>Datacenters Some deployments will need to manage a data pipeline that spans multiple datacenters. Our recommended approach to this is to deploy a local Kafka cluster in each datacenter, with application instances in each datacenter interacting only with their local cluster and mirroring data between clusters (see the documentation on Geo-Replication for how to do this).
This deployment pattern allows datacenters to act as independent entities and allows us to manage and tune inter-datacenter replication centrally.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/0100/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying HTTPS is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task. We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/0101/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying HTTPS is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task. We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/0102/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying HTTPS is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task. We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/0110/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/090/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying HTTPS is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task. We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/10/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/11/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/20/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/21/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/22/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/23/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/24/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/25/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to connect over SSL. By default, SSL is disabled but can be turned on as needed.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with the SSL support is to generate the key and the certificate for each machine in the cluster. You can use Java&amp;rsquo;s keytool utility to accomplish this task.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/26/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/27/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/28/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/30/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/31/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/32/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/08/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/kafka-configuration/</guid><description>Kafka Configuration Kafka 0.8 is the version we currently run. We are currently running with replication but with producers acks = 1.
Important Server Configurations The most important server configurations for performance are those that control the disk flush rate. The more often data is flushed to disk, the more &amp;ldquo;seek-bound&amp;rdquo; Kafka will be and the lower the throughput. However very low application flush rates can lead to high latency when the flush finally does occur (because of the volume of data that must be flushed).</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/33/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/34/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/35/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/36/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/37/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/38/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/39/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/40/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Listener Configuration</title><link>https://example.kafka-site-md.dev/41/security/listener-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/listener-configuration/</guid><description>Listener Configuration In order to secure a Kafka cluster, it is necessary to secure the channels that are used to communicate with the servers. Each server must define the set of listeners that are used to receive requests from clients as well as other servers. Each listener may be configured to authenticate clients using various mechanisms and to ensure traffic between the server and the client is encrypted. This section provides a primer for the configuration of listeners.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/0110/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/10/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/11/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/20/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/21/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/22/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/23/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/24/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/25/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/26/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/27/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/28/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/30/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/31/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/32/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/33/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/34/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/35/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/36/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/37/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/38/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/39/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/40/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/41/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/messages/</guid><description>Messages Messages consist of a variable-length header, a variable-length opaque key byte array and a variable-length opaque value byte array. The format of the header is described in the following section. Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/0100/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/0101/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/0102/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/08/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/081/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/082/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Network Layer</title><link>https://example.kafka-site-md.dev/090/implementation/network-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/network-layer/</guid><description>Network Layer The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the MessageSet interface a writeTo method. This allows the file-backed message set to use the more efficient transferTo implementation instead of an in-process buffered write. The threading model is a single acceptor thread and N processor threads which handle a fixed number of connections each.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/0100/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/0101/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/0102/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/0110/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/090/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/design/protocol/</guid><description>Kafka Wire Protocol This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/10/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/11/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/20/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Message Sets Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/21/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/22/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/23/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/24/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/25/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described in the design documentation.
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/26/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/27/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/28/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/30/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/31/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/32/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/33/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/34/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/35/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/36/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/37/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/38/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/39/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Record Batch Evolving the Protocol The Request Header Versioning Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/40/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Request and Response Headers Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Protocol</title><link>https://example.kafka-site-md.dev/41/design/protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/design/protocol/</guid><description>Kafka protocol guide This document covers the wire protocol implemented in Kafka. It is meant to give a readable guide to the protocol that covers the available requests, their binary format, and the proper way to make use of them to implement a client. This document assumes you understand the basic design and terminology described here
Preliminaries Network Partitioning and bootstrapping Partitioning Strategies Batching Versioning and Compatibility Retrieving Supported API versions SASL Authentication Sequence The Protocol Protocol Primitive Types Notes on reading the request format grammars Common Request and Response Structure Request and Response Headers Record Batch Constants Error Codes Api Keys The Messages Some Common Philosophical Questions Preliminaries Network Kafka uses a binary protocol over TCP.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/0110/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/quickstart/</guid><description>Play with a Streams Application This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and Zookeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters. Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/10/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/quickstart/</guid><description>Run Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/11/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/20/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/21/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/22/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/23/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/24/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/25/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/26/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/27/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/28/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/30/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/31/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/32/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka and ZooKeeper, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/33/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/34/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/35/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/36/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/37/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/38/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/39/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/40/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters. Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/41/streams/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/quickstart/</guid><description>Run Kafka Streams Demo Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
This tutorial assumes you are starting fresh and have no existing Kafka data. However, if you have already started Kafka, feel free to skip the first two steps.
Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters. Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka&amp;rsquo;s server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/0100/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/0101/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/0102/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/0110/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/08/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/getting-started/uses/</guid><description>Use Cases Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this paper.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/081/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/uses/</guid><description>Use Cases Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/082/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/uses/</guid><description>Use Cases Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/090/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/uses/</guid><description>Use Cases Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/10/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/11/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/20/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/21/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/22/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/23/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/24/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/25/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/26/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/27/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/28/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/30/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/31/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/32/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/33/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/34/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/35/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/36/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/37/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/38/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/39/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/40/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>Use Cases</title><link>https://example.kafka-site-md.dev/41/getting-started/uses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/uses/</guid><description>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.
Messaging Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/0100/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed. In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/0101/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed. In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/0102/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed. In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/0110/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed. In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/090/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed. In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/10/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/11/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/20/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/21/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/22/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/23/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/24/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/25/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/26/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/27/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/28/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/30/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/31/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/32/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/33/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/34/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/35/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/36/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/37/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/38/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/39/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/40/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>User Guide</title><link>https://example.kafka-site-md.dev/41/kafka-connect/user-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/kafka-connect/user-guide/</guid><description>User Guide The quickstart provides a brief example of how to run a standalone version of Kafka Connect. This section describes how to configure, run, and manage Kafka Connect in more detail.
Running Kafka Connect Kafka Connect currently supports two modes of execution: standalone (single process) and distributed.
In standalone mode all work is performed in a single process. This configuration is simpler to setup and get started with and may be useful in situations where only one worker makes sense (e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/0100/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/authentication-using-sasl/</guid><description>Authentication using SASL SASL configuration for Kafka brokers 1. Select one or more supported mechanisms to enable in the broker. `GSSAPI` and `PLAIN` are the mechanisms currently supported in Kafka. 2. Add a JAAS config file for the selected mechanisms as described in the examples for setting up GSSAPI (Kerberos) or PLAIN. 3. Pass the JAAS config file location as JVM parameter to each Kafka broker. For example: -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf 4. Configure a SASL port in server.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/0101/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/authentication-using-sasl/</guid><description>Authentication using SASL SASL configuration for Kafka brokers 1. Select one or more supported mechanisms to enable in the broker. `GSSAPI` and `PLAIN` are the mechanisms currently supported in Kafka. 2. Add a JAAS config file for the selected mechanisms as described in the examples for setting up GSSAPI (Kerberos) or PLAIN. 3. Pass the JAAS config file location as JVM parameter to each Kafka broker. For example: -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf 4. Configure a SASL port in server.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/0102/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication.
Client section is used to authenticate a SASL connection with zookeeper. It also allows the brokers to set SASL ACL on zookeeper nodes which locks these nodes down so that only the brokers can modify it.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/0110/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication.
Client section is used to authenticate a SASL connection with zookeeper. It also allows the brokers to set SASL ACL on zookeeper nodes which locks these nodes down so that only the brokers can modify it.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/090/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/authentication-using-sasl/</guid><description>Authentication using SASL Prerequisites 1. **Kerberos** If your organization is already using a Kerberos server (for example, by using Active Directory), there is no need to install a new server just for Kafka. Otherwise you will need to install one, your Linux vendor likely has packages for Kerberos and a short guide on how to install and configure it (Ubuntu, Redhat). Note that if you are using Oracle Java, you will need to download JCE policy files for your Java version and copy them to $JAVA_HOME/jre/lib/security.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/10/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication.
Client section is used to authenticate a SASL connection with zookeeper. It also allows the brokers to set SASL ACL on zookeeper nodes which locks these nodes down so that only the brokers can modify it.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/11/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/20/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/21/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/22/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/23/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/24/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/25/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/26/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/27/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/28/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/30/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/31/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/32/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0100/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0101/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0102/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/0110/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/07/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/08/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/081/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/082/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/090/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/10/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/11/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/20/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/21/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/22/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/23/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/24/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/25/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/26/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/27/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/28/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/30/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/31/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/32/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/33/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/34/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/35/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/36/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/37/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/38/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/39/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/40/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/configuration/</guid><description/></item><item><title>Configuration</title><link>https://example.kafka-site-md.dev/41/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/configuration/</guid><description/></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/0100/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/0101/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/0102/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/0110/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/090/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/10/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/11/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/20/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/21/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/22/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/23/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/24/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/25/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/26/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/27/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/28/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/30/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/31/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/32/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/33/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/34/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/35/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/36/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/37/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/38/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/39/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/40/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Connector Development Guide</title><link>https://example.kafka-site-md.dev/41/kafka-connect/connector-development-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/kafka-connect/connector-development-guide/</guid><description>Connector Development Guide This guide describes how developers can write new connectors for Kafka Connect to move data between Kafka and other systems. It briefly reviews a few key concepts and then describes how to create a simple connector.
Core Concepts and APIs Connectors and Tasks To copy data between Kafka and another system, users create a Connector for the system they want to pull data from or push data to.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/33/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/34/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/35/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/36/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/37/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/38/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/39/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/40/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Encryption and Authentication using SSL</title><link>https://example.kafka-site-md.dev/41/security/encryption-and-authentication-using-ssl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/encryption-and-authentication-using-ssl/</guid><description>Encryption and Authentication using SSL Apache Kafka allows clients to use SSL for encryption of traffic as well as authentication. By default, SSL is disabled but can be turned on if needed. The following paragraphs explain in detail how to set up your own PKI infrastructure, use it to create certificates and configure Kafka to use these.
Generate SSL key and certificate for each Kafka broker The first step of deploying one or more brokers with SSL support is to generate a public/private keypair for every server.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/26/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/27/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/28/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/30/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/31/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/32/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/33/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/34/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/35/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/36/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/37/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/38/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/39/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/40/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Geo-Replication (Cross-Cluster Data Mirroring)</title><link>https://example.kafka-site-md.dev/41/operations/geo-replication-cross-cluster-data-mirroring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/geo-replication-cross-cluster-data-mirroring/</guid><description>Geo-Replication (Cross-Cluster Data Mirroring) Geo-Replication Overview Kafka administrators can define data flows that cross the boundaries of individual Kafka clusters, data centers, or geo-regions. Such event streaming setups are often needed for organizational, technical, or legal requirements. Common scenarios include:
Geo-replication Disaster recovery Feeding edge clusters into a central, aggregate cluster Physical isolation of clusters (such as production vs. testing) Cloud migration or hybrid cloud deployments Legal and compliance requirements Administrators can set up such inter-cluster data flows with Kafka&amp;rsquo;s MirrorMaker (version 2), a tool to replicate data between different Kafka environments in a streaming manner.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/08/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/java-version/</guid><description>Java Version Any version of Java 1.6 or later should work fine, we are using 1.6.0_21. Here are our command line options:
java -server -Xms3072m -Xmx3072m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSConcurrentMTEnabled -XX:+CMSScavengeBeforeRemark -XX:CMSInitiatingOccupancyFraction=30 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:logs/gc.log -Djava.awt.headless=true -Dcom.sun.management.jmxremote -classpath &amp;lt;long list of jars&amp;gt; the.actual.Class</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/0100/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.retention.hours=168 log.flush.interval.ms=10000 log.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/0101/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.retention.hours=168 log.flush.interval.ms=10000 log.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/0102/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important old Scala producer configurations control
acks compression sync vs async production batch size (for async producers) The most important new Java producer configurations control
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/0110/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important old Scala producer configurations control
acks compression sync vs async production batch size (for async producers) The most important new Java producer configurations control
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/081/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our server production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 replica.lag.max.messages=4000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/082/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our server production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 replica.lag.max.messages=4000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/090/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations control
compression sync vs async production batch size (for async producers) The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is our server production server configuration:
# Replication configurations num.replica.fetchers=4 replica.fetch.max.bytes=1048576 replica.fetch.wait.max.ms=500 replica.high.watermark.checkpoint.interval.ms=5000 replica.socket.timeout.ms=30000 replica.socket.receive.buffer.bytes=65536 replica.lag.time.max.ms=10000 controller.socket.timeout.ms=30000 controller.message.queue.size=10 # Log configuration num.partitions=8 message.max.bytes=1000000 auto.create.topics.enable=true log.index.interval.bytes=4096 log.index.size.max.bytes=10485760 log.retention.hours=168 log.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/10/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important old Scala producer configurations control
acks compression sync vs async production batch size (for async producers) The most important new Java producer configurations control
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/11/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important old Scala producer configurations control
acks compression sync vs async production batch size (for async producers) The most important new Java producer configurations control
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/20/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important old Scala producer configurations control
acks compression sync vs async production batch size (for async producers) The most important new Java producer configurations control
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/21/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/22/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/23/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/24/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/25/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/0110/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below for Kafka version 0.11.0 and later (message format version v2, or magic=2).</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/10/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below for Kafka version 0.11.0 and later (message format version v2, or magic=2).</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/11/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below for Kafka version 0.11.0 and later (message format version v2, or magic=2).</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/20/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/21/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/22/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/23/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/24/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/25/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/26/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/27/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/28/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/30/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/31/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/32/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/33/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/34/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/35/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/36/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/37/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/38/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/39/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/40/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/41/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/message-format/</guid><description>Message Format Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record. Record batches and records have their own headers. The format of each is described below.
Record Batch The following is the on-disk format of a RecordBatch.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/0100/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header, a variable length opaque key byte array and a variable length opaque value byte array. The header contains the following fields:
A CRC32 checksum to detect corruption or truncation. A format version. An attributes identifier A timestamp Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/0101/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header, a variable length opaque key byte array and a variable length opaque value byte array. The header contains the following fields:
A CRC32 checksum to detect corruption or truncation. A format version. An attributes identifier A timestamp Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/0102/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header, a variable length opaque key byte array and a variable length opaque value byte array. The header contains the following fields:
A CRC32 checksum to detect corruption or truncation. A format version. An attributes identifier A timestamp Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/08/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header and variable length opaque byte array payload. The header contains a format version and a CRC32 checksum to detect corruption or truncation. Leaving the payload opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/081/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header and variable length opaque byte array payload. The header contains a format version and a CRC32 checksum to detect corruption or truncation. Leaving the payload opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/082/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header and variable length opaque byte array payload. The header contains a format version and a CRC32 checksum to detect corruption or truncation. Leaving the payload opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Messages</title><link>https://example.kafka-site-md.dev/090/implementation/messages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/messages/</guid><description>Messages Messages consist of a fixed-size header and variable length opaque byte array payload. The header contains a format version and a CRC32 checksum to detect corruption or truncation. Leaving the payload opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/0100/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
Step 1: Download the code Download the 0.10.0.0 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.11-0.10.0.0.tgz** &amp;gt; **cd kafka_2.11-0.10.0.0** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/0101/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 0.10.1.0 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.11-0.10.1.0.tgz** &amp;gt; **cd kafka_2.11-0.10.1.0** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/0102/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 0.10.2.0 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.11-0.10.2.0.tgz** &amp;gt; **cd kafka_2.11-0.10.2.0** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/0110/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 0.11.0.2 release and un-tar it.
&amp;gt; tar -xzf kafka_2.11-0.11.0.2.tgz &amp;gt; cd kafka_2.11-0.11.0.2 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/07/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/getting-started/quickstart/</guid><description>Quick Start Step 1: Download the code Download a recent stable release.
**&amp;gt; tar xzf kafka-&amp;lt;VERSION&amp;gt;.tgz** **&amp;gt; cd kafka-&amp;lt;VERSION&amp;gt;** **&amp;gt; ./sbt update** **&amp;gt; ./sbt package** Step 2: Start the server Kafka brokers and consumers use this for co-ordination.
First start the zookeeper server. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node zookeeper instance.
**&amp;gt; bin/zookeeper-server-start.sh config/zookeeper.properties** [2010-11-21 23:45:02,335] INFO Reading configuration from: config/zookeeper.properties ... Now start the Kafka server:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/08/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/getting-started/quickstart/</guid><description>Quick Start Step 1: Download the code Download the 0.8 release.
**&amp;gt; tar xzf kafka-&amp;lt;VERSION&amp;gt;.tgz** **&amp;gt; cd kafka-&amp;lt;VERSION&amp;gt;** **&amp;gt; ./sbt update** **&amp;gt; ./sbt package** **&amp;gt; ./sbt assembly-package-dependency** This tutorial assumes you are starting on a fresh zookeeper instance with no pre-existing data. If you want to migrate from an existing 0.7 installation you will need to follow the migration instructions.
Step 2: Start the server Kafka uses zookeeper so you need to first start a zookeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/081/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/quickstart/</guid><description>Quick Start This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
Step 1: Download the code Download the 0.8.1.1 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.9.2-0.8.1.1.tgz** &amp;gt; **cd kafka_2.9.2-0.8.1.1** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/082/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/quickstart/</guid><description>Quick Start This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
Step 1: Download the code Download the 0.8.2.0 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.10-0.8.2.0.tgz** &amp;gt; **cd kafka_2.10-0.8.2.0** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/090/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/quickstart/</guid><description>Quick Start This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.
Step 1: Download the code Download the 0.9.0.0 release and un-tar it.
&amp;gt; **tar -xzf kafka_2.11-0.9.0.0.tgz** &amp;gt; **cd kafka_2.11-0.9.0.0** Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/10/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 1.0.2 release and un-tar it.
&amp;gt; tar -xzf kafka_2.11-1.0.2.tgz &amp;gt; cd kafka_2.11-1.0.2 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/11/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 1.1.0 release and un-tar it.
&amp;gt; tar -xzf kafka_2.11-1.1.0.tgz &amp;gt; cd kafka_2.11-1.1.0 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/20/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.0.0 release and un-tar it.
&amp;gt; tar -xzf kafka_2.11-2.0.0.tgz &amp;gt; cd kafka_2.11-2.0.0 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/21/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.1.0 release and un-tar it.
&amp;gt; tar -xzf kafka_2.11-2.1.0.tgz &amp;gt; cd kafka_2.11-2.1.0 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/22/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.2.0 release and un-tar it.
&amp;gt; tar -xzf kafka_2.12-2.2.0.tgz &amp;gt; cd kafka_2.12-2.2.0 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/23/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.3.0 release and un-tar it.
&amp;gt; tar -xzf kafka_2.12-2.3.0.tgz &amp;gt; cd kafka_2.12-2.3.0 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/24/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.4.1 release and un-tar it.
&amp;gt; tar -xzf kafka_2.12-2.4.1.tgz &amp;gt; cd kafka_2.12-2.4.1 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/25/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.12-2.5.0.tgz $ cd kafka_2.12-2.5.0 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/26/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/quickstart/</guid><description>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data. Since Kafka console scripts are different for Unix-based and Windows platforms, on Windows platforms use bin\windows\ instead of bin/, and change the script extension to .bat.
Step 1: Download the code Download the 2.6.3 release and un-tar it.
&amp;gt; tar -xzf kafka_2.13-2.6.3.tgz &amp;gt; cd kafka_2.13-2.6.3 Step 2: Start the server Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don&amp;rsquo;t already have one.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/27/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-2.7.0.tgz $ cd kafka_2.13-2.7.0 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/28/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-2.8.2.tgz $ cd kafka_2.13-2.8.2 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/30/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.0.1.tgz $ cd kafka_2.13-3.0.1 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/31/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.1.1-SNAPSHOT.tgz $ cd kafka_2.13-3.1.1-SNAPSHOT Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/32/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.2.1.tgz $ cd kafka_2.13-3.2.1 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Run the following commands in order to start all services in the correct order:
# Start the ZooKeeper service # Note: Soon, ZooKeeper will no longer be required by Apache Kafka. $ bin/zookeeper-server-start.sh config/zookeeper.properties Open another terminal session and run:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/33/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.3.1.tgz $ cd kafka_2.13-3.3.1 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using ZooKeeper or KRaft. To get started with either configuration follow one the sections below but not both.
Kafka with ZooKeeper Run the following commands in order to start all services in the correct order:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/34/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.4.0.tgz $ cd kafka_2.13-3.4.0 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using ZooKeeper or KRaft. To get started with either configuration follow one the sections below but not both.
Kafka with ZooKeeper Run the following commands in order to start all services in the correct order:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/35/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.5.2.tgz $ cd kafka_2.13-3.5.2 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using ZooKeeper or KRaft. To get started with either configuration follow one the sections below but not both.
Kafka with ZooKeeper Run the following commands in order to start all services in the correct order:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/36/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.6.2.tgz $ cd kafka_2.13-3.6.2 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using ZooKeeper or KRaft. To get started with either configuration follow one the sections below but not both.
Kafka with ZooKeeper Run the following commands in order to start all services in the correct order:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/37/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.7.2.tgz $ cd kafka_2.13-3.7.2 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using ZooKeeper or KRaft. To get started with either configuration follow one of the sections below but not both.
Kafka with ZooKeeper Run the following commands in order to start all services in the correct order:</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/38/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.8.1.tgz $ cd kafka_2.13-3.8.1 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using KRaft or ZooKeeper. To get started with either configuration follow one of the sections below but not both.
Kafka with KRaft Kafka can be run using KRaft mode using local scripts and downloaded files or the docker image.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/39/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-3.9.1.tgz $ cd kafka_2.13-3.9.1 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 8+ installed.
Apache Kafka can be started using KRaft or ZooKeeper. To get started with either configuration follow one of the sections below but not both.
Kafka with KRaft Kafka can be run using KRaft mode using local scripts and downloaded files or the docker image.</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/40/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-4.0.0.tgz $ cd kafka_2.13-4.0.0 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 17+ installed.
Kafka can be run using local scripts and downloaded files or the docker image.
Using downloaded files Generate a Cluster UUID
$ KAFKA_CLUSTER_ID=&amp;quot;$(bin/kafka-storage.sh random-uuid)&amp;quot; Format Log Directories
$ bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties Start the Kafka Server</description></item><item><title>Quick Start</title><link>https://example.kafka-site-md.dev/41/getting-started/quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/quickstart/</guid><description>Step 1: Get Kafka Download the latest Kafka release and extract it:
$ tar -xzf kafka_2.13-4.1.0.tgz $ cd kafka_2.13-4.1.0 Step 2: Start the Kafka environment NOTE: Your local environment must have Java 17+ installed.
Kafka can be run using local scripts and downloaded files or the docker image.
Using downloaded files Generate a Cluster UUID
$ KAFKA_CLUSTER_ID=&amp;quot;$(bin/kafka-storage.sh random-uuid)&amp;quot; Format Log Directories
$ bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties Start the Kafka Server</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Applying processors and transformers (Processor API integration) Writing streams back to Kafka Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Applying processors and transformers (Processor API integration) Writing streams back to Kafka Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Applying processors and transformers (Processor API integration) Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Tumbling time windows Hopping time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit SerDes User-Defined SerDes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors (Processor API integration) Transformers removal and migration to processors Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Streams DSL</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/dsl-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/dsl-api/</guid><description>Streams DSL The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors (Processor API integration) Transformers removal and migration to processors Naming Operators in a Streams DSL application Controlling KTable update rate Using timestamp-based semantics for table processors Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview In comparison to the Processor API, only the DSL supports:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/0110/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/tutorial/</guid><description>Write your own Streams Applications In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka&amp;rsquo;s Streams API. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/10/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/tutorial/</guid><description>Tutorial: Write a Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/11/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/20/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/21/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/22/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/23/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/24/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/25/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/26/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/27/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/28/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/30/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/31/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/32/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/33/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/34/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/35/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/36/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/tutorial/</guid><description>z
Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/37/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/38/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/39/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/40/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Write a streams app</title><link>https://example.kafka-site-md.dev/41/streams/tutorial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/tutorial/</guid><description>Tutorial: Write a Kafka Streams Application Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
In this guide we will start from scratch on setting up your own project to write a stream processing application using Kafka Streams. It is highly recommended to read the quickstart first on how to run a Streams application written in Kafka Streams if you have not done so.
Setting up a Maven Project We are going to use a Kafka Streams Maven Archetype for creating a Streams project structure with the following commands:</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/35/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/36/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/37/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/38/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/39/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/40/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Administration</title><link>https://example.kafka-site-md.dev/41/kafka-connect/administration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/kafka-connect/administration/</guid><description>Administration Kafka Connect&amp;rsquo;s REST layer provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
When a connector is first submitted to the cluster, a rebalance is triggered between the Connect workers in order to distribute the load that consists of the tasks of the new connector.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/33/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/34/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/35/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/36/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/37/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/38/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/39/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/40/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authentication using SASL</title><link>https://example.kafka-site-md.dev/41/security/authentication-using-sasl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/authentication-using-sasl/</guid><description>Authentication using SASL JAAS configuration Kafka uses the Java Authentication and Authorization Service (JAAS) for SASL configuration.
1. ##### JAAS configuration for Kafka brokers KafkaServer is the section name in the JAAS file used by each KafkaServer/Broker. This section provides SASL configuration options for the broker including any SASL client connections made by the broker for inter-broker communication. If multiple listeners are configured to use SASL, the section name may be prefixed with the listener name in lower-case followed by a period, e.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/0100/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/0101/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/0102/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/0110/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/090/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/10/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/11/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H On Resource R&amp;rdquo;. You can read more about the acl structure on KIP-11.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/20/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/21/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/22/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/23/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/24/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/25/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/26/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/27/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/28/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/30/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/31/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/32/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. The Authorizer is configured by setting authorizer.class.name in server.properties. To enable the out of the box implementation use:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;. You can read more about the acl structure in KIP-11 and resource patterns in KIP-290.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/0102/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/core-concepts/</guid><description>Core Concepts We first summarize the key concepts of Kafka Streams.
Stream Processing Topology A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair. A stream processing application is any program that makes use of the Kafka Streams library.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/0110/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/core-concepts/</guid><description>Core Concepts Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/10/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/11/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/20/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/21/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/22/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/23/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/24/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/25/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/26/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/27/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/28/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/30/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/31/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/32/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/33/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/34/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/35/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/36/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/37/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/38/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/39/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/40/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Core Concepts</title><link>https://example.kafka-site-md.dev/41/streams/core-concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/core-concepts/</guid><description>Core Concepts Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management and real-time querying of application state.
Kafka Streams has a low barrier to entry : You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads.</description></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0100/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0101/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0102/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/0110/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/07/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/08/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/081/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/082/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/090/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/10/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/11/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/20/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/21/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/22/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/23/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/24/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/25/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/26/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/27/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/28/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/30/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/31/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/32/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/33/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/34/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/35/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/36/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/37/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/38/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/39/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/40/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/design/</guid><description/></item><item><title>Design</title><link>https://example.kafka-site-md.dev/41/design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/design/</guid><description/></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/0100/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/0101/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/0102/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/0110/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/081/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/ecosystem/</guid><description>Ecosystem There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/082/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/ecosystem/</guid><description>Ecosystem There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/090/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/ecosystem/</guid><description>Ecosystem There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/10/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/11/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/20/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/21/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/22/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/23/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/24/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/25/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/26/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/27/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/28/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/30/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/31/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/32/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/33/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/34/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/35/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/36/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/37/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/38/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/39/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/40/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Ecosystem</title><link>https://example.kafka-site-md.dev/41/getting-started/ecosystem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/ecosystem/</guid><description>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/08/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is more better.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/0100/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/0101/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/0102/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/0110/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/081/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/java-version/</guid><description>Java Version We&amp;rsquo;re currently running JDK 1.7 u51, and we&amp;rsquo;ve switched over to the G1 collector. If you do this (and we highly recommend it), make sure you&amp;rsquo;re on u51. We tried out u21 in testing, but we had a number of problems with the GC implementation in that version. Our tuning looks like this:
-Xms4g -Xmx4g -XX:PermSize=48m -XX:MaxPermSize=48m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak): - 15 brokers - 15.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/082/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/java-version/</guid><description>Java Version We&amp;rsquo;re currently running JDK 1.7 u51, and we&amp;rsquo;ve switched over to the G1 collector. If you do this (and we highly recommend it), make sure you&amp;rsquo;re on u51. We tried out u21 in testing, but we had a number of problems with the GC implementation in that version. Our tuning looks like this:
-Xms4g -Xmx4g -XX:PermSize=48m -XX:MaxPermSize=48m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak): - 15 brokers - 15.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/090/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/10/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/11/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/20/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/21/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/22/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/23/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/24/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/25/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/java-version/</guid><description>Java Version From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities. LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&amp;rsquo;s tuning looks like this:
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 For reference, here are the stats on one of LinkedIn&amp;rsquo;s busiest clusters (at peak):</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/26/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/0110/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/10/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/11/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/20/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/21/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/22/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/23/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/24/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/25/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/26/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/27/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/28/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/30/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/31/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/32/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/33/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/34/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/35/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/36/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/37/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/38/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/39/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/40/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/41/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my-topic&amp;rdquo; with two partitions consists of two directories (namely my-topic-0 and my-topic-1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/0100/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/message-format/</guid><description>Message Format /** * 1. 4 byte CRC32 of the message * 2. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes, value is 0 or 1 * 3. 1 byte &amp;quot;attributes&amp;quot; identifier to allow annotations on the message independent of the version * bit 0 ~ 2 : Compression codec. * 0 : no compression * 1 : gzip * 2 : snappy * 3 : lz4 * bit 3 : Timestamp type * 0 : create time * 1 : log append time * bit 4 ~ 7 : reserved * 4.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/0101/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/message-format/</guid><description>Message Format /** * 1. 4 byte CRC32 of the message * 2. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes, value is 0 or 1 * 3. 1 byte &amp;quot;attributes&amp;quot; identifier to allow annotations on the message independent of the version * bit 0 ~ 2 : Compression codec. * 0 : no compression * 1 : gzip * 2 : snappy * 3 : lz4 * bit 3 : Timestamp type * 0 : create time * 1 : log append time * bit 4 ~ 7 : reserved * 4.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/0102/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/message-format/</guid><description>Message Format /** * 1. 4 byte CRC32 of the message * 2. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes, value is 0 or 1 * 3. 1 byte &amp;quot;attributes&amp;quot; identifier to allow annotations on the message independent of the version * bit 0 ~ 2 : Compression codec. * 0 : no compression * 1 : gzip * 2 : snappy * 3 : lz4 * bit 3 : Timestamp type * 0 : create time * 1 : log append time * bit 4 ~ 7 : reserved * 4.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/08/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/message-format/</guid><description>Message Format /** * A message. The format of an N byte message is the following: * * If magic byte is 0 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2. 4 byte CRC32 of the payload * * 3. N - 5 byte payload * * If magic byte is 1 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/081/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/message-format/</guid><description>Message Format /** * A message. The format of an N byte message is the following: * * If magic byte is 0 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2. 4 byte CRC32 of the payload * * 3. N - 5 byte payload * * If magic byte is 1 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/082/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/message-format/</guid><description>Message Format /** * A message. The format of an N byte message is the following: * * If magic byte is 0 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2. 4 byte CRC32 of the payload * * 3. N - 5 byte payload * * If magic byte is 1 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2.</description></item><item><title>Message Format</title><link>https://example.kafka-site-md.dev/090/implementation/message-format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/message-format/</guid><description>Message Format /** * A message. The format of an N byte message is the following: * * If magic byte is 0 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2. 4 byte CRC32 of the payload * * 3. N - 5 byte payload * * If magic byte is 1 * * 1. 1 byte &amp;quot;magic&amp;quot; identifier to allow format changes * * 2.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/27/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/28/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/30/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/31/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/32/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/33/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/34/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/35/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/36/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/37/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/38/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/39/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/40/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Multi-Tenancy</title><link>https://example.kafka-site-md.dev/41/operations/multi-tenancy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/multi-tenancy/</guid><description>Multi-Tenancy Multi-Tenancy Overview As a highly scalable event streaming platform, Kafka is used by many users as their central nervous system, connecting in real-time a wide range of different systems and applications from various teams and lines of businesses. Such multi-tenant cluster environments command proper control and management to ensure the peaceful coexistence of these different needs. This section highlights features and best practices to set up such shared environments, which should help you operate clusters that meet SLAs/OLAs and that minimize potential collateral damage caused by &amp;ldquo;noisy neighbors&amp;rdquo;.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Readonly State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Readonly State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Readonly State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Processor API</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/processor-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/processor-api/</guid><description>Processor API The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Versioned Key-Value State Stores Readonly State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/0102/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/architecture/</guid><description>Architecture Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library. Let&amp;rsquo;s walk through some details.
Stream Partitions and Tasks The messaging layer of Kafka partitions data for storing and transporting it.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/0110/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/architecture/</guid><description>Architecture Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library. Let&amp;rsquo;s walk through some details.
Stream Partitions and Tasks The messaging layer of Kafka partitions data for storing and transporting it.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/10/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/11/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/20/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/21/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/22/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/23/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/24/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/25/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/26/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/27/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/28/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/30/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/31/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/32/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/33/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/34/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/35/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/36/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/37/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/38/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/39/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/40/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Architecture</title><link>https://example.kafka-site-md.dev/41/streams/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/architecture/</guid><description>Architecture Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers.
The picture below shows the anatomy of an application that uses the Kafka Streams library.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/33/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server confgiuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer Kafka acls are defined in the general format of &amp;ldquo;Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP&amp;rdquo;.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/34/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server confgiuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/35/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server confgiuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/36/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/37/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/38/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/39/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides default implementations which store ACLs in the cluster metadata (either Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the provided implementation is configured as follows:
authorizer.class.name=kafka.security.authorizer.AclAuthorizer For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/40/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides a default implementation which store ACLs in the cluster metadata (KRaft metadata log). For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):
authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer Kafka ACLs are defined in the general format of &amp;ldquo;Principal {P} is [Allowed|Denied] Operation {O} From Host {H} on any Resource {R} matching ResourcePattern {RP}&amp;rdquo;.</description></item><item><title>Authorization and ACLs</title><link>https://example.kafka-site-md.dev/41/security/authorization-and-acls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/authorization-and-acls/</guid><description>Authorization and ACLs Kafka ships with a pluggable authorization framework, which is configured with the authorizer.class.name property in the server configuration. Configured implementations must extend org.apache.kafka.server.authorizer.Authorizer. Kafka provides a default implementation which store ACLs in the cluster metadata (KRaft metadata log). For KRaft clusters, use the following configuration on all nodes (brokers, controllers, or combined broker/controller nodes):
authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer Kafka ACLs are defined in the general format of &amp;ldquo;Principal {P} is [Allowed|Denied] Operation {O} From Host {H} on any Resource {R} matching ResourcePattern {RP}&amp;rdquo;.</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/0110/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/10/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/11/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/20/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/21/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/22/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/23/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/24/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/25/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/26/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/27/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/28/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/30/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/31/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/32/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/33/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/34/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/35/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/36/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/37/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/38/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/39/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/40/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/41/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the group coordinator. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that group coordinator (broker).</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/0100/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/0101/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/0102/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/0110/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/081/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is more better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/082/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is more better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/090/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is more better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/10/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/11/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/20/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/21/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/22/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/23/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/24/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/25/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/0100/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/0101/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/0102/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/0110/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/07/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/08/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/081/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/082/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/090/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/10/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/11/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/20/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/21/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/22/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/23/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/24/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/25/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/26/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/27/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/28/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/30/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/31/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/32/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/33/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/34/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/35/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/36/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/37/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/38/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/39/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/40/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/implementation/</guid><description/></item><item><title>Implementation</title><link>https://example.kafka-site-md.dev/41/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/implementation/</guid><description/></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/0100/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/0101/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/0102/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/0110/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/10/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/11/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/20/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/21/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/22/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/23/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/24/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/25/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/26/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/27/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/28/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/30/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/31/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/32/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/26/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/java-version/</guid><description>Java Version Java 8 and Java 11 are supported. Java 11 performs significantly better if TLS is enabled, so it is highly recommended (it also includes a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. Typical arguments for running Kafka with OpenJDK-based Java implementations (including Oracle JDK) are:</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/40/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/java-version/</guid><description>Java Version Java 17 and Java 21 are fully supported while Java 11 is supported for a subset of modules (clients, streams and related). Support for versions newer than the most recent LTS version are best-effort and the project typically only tests with the most recent non LTS version.
We generally recommend running Apache Kafka with the most recent LTS release (Java 21 at the time of writing) for performance, efficiency and support reasons.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/41/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/java-version/</guid><description>Java Version Java 17 and Java 21 are fully supported while Java 11 is supported for a subset of modules (clients, streams and related). Support for versions newer than the most recent LTS version are best-effort and the project typically only tests with the most recent non LTS version.
We generally recommend running Apache Kafka with the most recent LTS release (Java 21 at the time of writing) for performance, efficiency and support reasons.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/27/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/28/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/30/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/31/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/32/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/33/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/34/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/35/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/36/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/37/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/38/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Kafka Configuration</title><link>https://example.kafka-site-md.dev/39/operations/kafka-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/kafka-configuration/</guid><description>Kafka Configuration Important Client Configurations The most important producer configurations are:
acks compression batch size The most important consumer configuration is the fetch size.
All configurations are documented in the configuration section.
A Production Server Config Here is an example production server configuration:
# ZooKeeper zookeeper.connect=[list of ZooKeeper servers] # Log configuration num.partitions=8 default.replication.factor=3 log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).] # Other configurations broker.id=[An integer.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/0100/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/0101/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/0102/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/08/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/081/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/082/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Log</title><link>https://example.kafka-site-md.dev/090/implementation/log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/log/</guid><description>Log A log for a topic named &amp;ldquo;my_topic&amp;rdquo; with two partitions consists of two directories (namely my_topic_0 and my_topic_1) populated with data files containing the messages for that topic. The format of the log files is a sequence of &amp;ldquo;log entries&amp;rdquo;&amp;quot;; each log entry is a 4 byte integer N storing the message length which is followed by the N message bytes. Each message is uniquely identified by a 64-bit integer offset giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/08/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.
We pay particular we do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate &amp;ldquo;kafka.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/dsl-topology-naming/</guid><description>Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/dsl-topology-naming/</guid><description>Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/dsl-topology-naming/</guid><description>Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DLS layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Naming Operators in a Streams DSL application</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/dsl-topology-naming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/dsl-topology-naming/</guid><description>Developer Guide for Kafka Streams Naming Operators in a Kafka Streams DSL Application You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/0100/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/getting-started/upgrade/</guid><description>Upgrading from 0.8.x or 0.9.x to 0.10.0.0 0.10.0.0 has potential breaking changes (please review before upgrading) and possible performance impact following the upgrade. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.
Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.
Notes to clients with version 0.9.0.0: Due to a bug introduced in 0.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/0101/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x or 0.10.0.X to 0.10.1.0 0.10.1.0 has wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please notice the Potential breaking changes in 0.10.1.0 before upgrade.
Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients (i.e. 0.10.1.x clients only support 0.10.1.x or later brokers while 0.10.1.x brokers also support older clients).</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/0102/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x or 0.10.1.x to 0.10.2.0 0.10.2.0 has wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 0.10.2.0 before upgrading.
Starting with version 0.10.2, Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.10.2 clients can talk to version 0.10.0 or newer brokers. However, if your brokers are older than 0.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/0110/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0 Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 0.11.0.0 before upgrading.
Starting with version 0.10.2, Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.11.0 clients can talk to version 0.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/08/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/getting-started/upgrade/</guid><description>Upgrading from 0.7 Since 0.8 is not backward compatible with 0.7.x, we provide a tool for migrating data in an 0.7 cluster to an 0.8 cluster. Details of the tool can be found here.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/081/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/getting-started/upgrade/</guid><description>Upgrading From Previous Versions Upgrading from 0.8.0 to 0.8.1 0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.
Upgrading from 0.7 0.8, the release in which added replication, was our first backwards-incompatible release: major changes were made to the API, ZooKeeper data structures, and protocol, and configuration. The upgrade from 0.7 to 0.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/082/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/getting-started/upgrade/</guid><description>Upgrading From Previous Versions Upgrading from 0.8.1 to 0.8.2.0 0.8.2.0 is fully compatible with 0.8.1. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.
Upgrading from 0.8.0 to 0.8.1 0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.
Upgrading from 0.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/090/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/getting-started/upgrade/</guid><description>Upgrading From Previous Versions Upgrading from 0.8.0, 0.8.1.X or 0.8.2.X to 0.9.0.0 0.9.0.0 has potential breaking changes (please review before upgrading) and an inter-broker protocol change from previous versions. This means that upgraded brokers and clients may not be compatible with older versions. It is important that you upgrade your Kafka cluster before upgrading your clients. If you are using MirrorMaker downstream clusters should be upgraded first as well.
For a rolling upgrade:</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/10/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x or 0.11.0.x to 1.0.0 Kafka 1.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 1.0.0 before upgrading.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/11/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x or 1.0.x to 1.1.x Kafka 1.1.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 1.1.0 before upgrading.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/20/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, or 1.1.x to 2.0.0 Kafka 2.0.0 introduces wire protocol changes. By following the recommended rolling upgrade plan below, you guarantee no downtime during the upgrade. However, please review the notable changes in 2.0.0 before upgrading.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from. CURRENT_MESSAGE_FORMAT_VERSION refers to the message format version currently in use.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/21/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, or 2.0.0 to 2.1.0 Note that 2.1.x contains a change to the internal schema used to store consumer offsets. Once the upgrade is complete, it will not be possible to downgrade to previous versions. See the rolling upgrade notes below for more detail.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties. CURRENT_KAFKA_VERSION refers to the version you are upgrading from.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/22/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x to 2.2.0 If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/23/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x or 2.2.x to 2.3.0 If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/24/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x or 2.2.x or 2.3.x to 2.4.0 If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/25/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/getting-started/upgrade/</guid><description>Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x, 0.10.2.x, 0.11.0.x, 1.0.x, 1.1.x, 2.0.x or 2.1.x or 2.2.x, 2.3.x, 2.4.x to 2.5.0 If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/26/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/getting-started/upgrade/</guid><description>Upgrading to 2.6.3 from any version 0.8.x through 2.5.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/27/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/getting-started/upgrade/</guid><description>Upgrading to 2.7.2 from any version 0.8.x through 2.6.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/28/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/getting-started/upgrade/</guid><description>Upgrading to 2.8.2 from any version 0.8.x through 2.7.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/30/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/getting-started/upgrade/</guid><description>Notable changes in 3.0.1 Idempotence for the producer is enabled by default if no conflicting configurations are set. A bug prevented the producer idempotence default from being applied which meant that it remained disabled unless the user had explicitly set enable.idempotence to true. See KAFKA-13598for more details. This issue was fixed and the default is properly applied. Notable changes in 3.0.0 The producer has stronger delivery guarantees by default: idempotence is enabled and acks is set to all instead of 1.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/31/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/getting-started/upgrade/</guid><description>Upgrading to 3.1.1 from any version 0.8.x through 3.0.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/32/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/getting-started/upgrade/</guid><description>Upgrading to 3.2.1 from any version 0.8.x through 3.1.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/33/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/getting-started/upgrade/</guid><description>Upgrading to 3.3.x from any version 0.8.x through 3.2.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/34/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/getting-started/upgrade/</guid><description>Upgrading to 3.4.0 from any version 0.8.x through 3.3.x If you are upgrading from a version prior to 2.1.x, please see the note below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/35/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/getting-started/upgrade/</guid><description>Upgrading to 3.5.2 from any version 0.8.x through 3.4.x All upgrade steps remain same as upgrading to 3.5.0
Notable changes in 3.5.2 When migrating producer ID blocks from ZK to KRaft, there could be duplicate producer IDs being given to transactional or idempotent producers. This can cause long term problems since the producer IDs are persisted and reused for a long time. See KAFKA-15552 for more details. In 3.5.0 and 3.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/36/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/getting-started/upgrade/</guid><description>Upgrading to 3.6.2 from any version 0.8.x through 3.5.x Upgrading ZooKeeper-based clusters If you are upgrading from a version prior to 2.1.x, please see the note in step 5 below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/37/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/upgrade/</guid><description>Upgrading to 3.7.2 from any version 0.8.x through 3.6.x Notable changes in 3.7.2 In case you run your Kafka clusters with no execution permission for the /tmp partition, Kafka will not work properly. It might either refuse to start or fail when producing and consuming messages. This is due to the compression libraries zstd-jni and snappy. To remediate this problem you need to pass the following JVM flags to Kafka ZstdTempFolder and org.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/38/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/upgrade/</guid><description>Upgrading to 3.8.1 from any version 0.8.x through 3.7.x Upgrading ZooKeeper-based clusters If you are upgrading from a version prior to 2.1.x, please see the note in step 5 below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/39/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/upgrade/</guid><description>Upgrading to 3.9.1 from any version 0.8.x through 3.8.x Upgrading ZooKeeper-based clusters If you are upgrading from a version prior to 2.1.x, please see the note in step 5 below about the change to the schema used to store consumer offsets. Once you have changed the inter.broker.protocol.version to the latest version, it will not be possible to downgrade to a version prior to 2.1.
For a rolling upgrade:
Update server.properties on all brokers and add the following properties.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/40/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/upgrade/</guid><description>Upgrading to 4.0.0 Upgrading Clients to 4.0.0 For a rolling upgrade:
Upgrade the clients one at a time: shut down the client, update the code, and restart it. Clients (including Streams and Connect) must be on version 2.1 or higher before upgrading to 4.0. Many deprecated APIs were removed in Kafka 4.0. For more information about the compatibility, please refer to the compatibility matrix or KIP-1124. Upgrading Servers to 4.0.0 from any version 3.</description></item><item><title>Upgrading</title><link>https://example.kafka-site-md.dev/41/getting-started/upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/upgrade/</guid><description>Upgrading to 4.1.0 Upgrading Servers to 4.1.0 from any version 3.3.x through 4.0.x Notable changes in 4.1.0 Apache Kafka 4.1 ships with a preview of Queues for Kafka (KIP-932). This feature introduces a new kind of group called share groups, as an alternative to consumer groups. Consumers in a share group cooperatively consume records from topics, without assigning each partition to just one consumer. Share groups also introduce per-record acknowledgement and counting of delivery attempts.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/090/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper is such that only brokers will be able to modify the corresponding znodes, but znodes are world readable. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of znodes can cause cluster disruption.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes via a StreamsConfig instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes via a StreamsConfig instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), through(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide SerDes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such SerDes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide SerDes by using either of these methods:
By setting default SerDes in the java.util.Properties config instance. By specifying explicit SerDes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Data Types and Serialization</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/datatypes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/datatypes/</guid><description>Data Types and Serialization Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance. By specifying explicit Serdes when calling the appropriate API methods, thus overriding the defaults.</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/0100/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/0101/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/0102/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/08/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/implementation/distribution/</guid><description>Distribution Zookeeper Directories The following gives the zookeeper structures and algorithms used for co-ordination between consumers and brokers.
Notation When an element in a path is denoted [xyz], that means that the value of xyz is not fixed and there is in fact a zookeeper znode for each possible value of xyz. For example /topics/[topic] would be a directory named /topics containing a sub-directory for each topic name. Numerical ranges are also given such as [0&amp;hellip;5] to indicate the subdirectories 0, 1, 2, 3, 4.</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/081/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/implementation/distribution/</guid><description>Distribution ZooKeeper Directories The following gives the ZooKeeper structures and algorithms used for co-ordination between consumers and brokers.
Notation When an element in a path is denoted [xyz], that means that the value of xyz is not fixed and there is in fact a ZooKeeper znode for each possible value of xyz. For example /topics/[topic] would be a directory named /topics containing a sub-directory for each topic name. Numerical ranges are also given such as [0&amp;hellip;5] to indicate the subdirectories 0, 1, 2, 3, 4.</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/082/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Distribution</title><link>https://example.kafka-site-md.dev/090/implementation/distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/implementation/distribution/</guid><description>Distribution Consumer Offset Tracking The high-level consumer tracks the maximum offset it has consumed in each partition and periodically commits its offset vector so that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for a given consumer group in a designated broker (for that group) called the offset manager. i.e., any consumer instance in that consumer group should send its offset commits and fetches to that offset manager (broker).</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/26/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/40/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/41/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/33/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/34/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/35/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/36/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/37/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/38/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/39/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/40/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Incorporating Security Features in a Running Cluster</title><link>https://example.kafka-site-md.dev/41/security/incorporating-security-features-in-a-running-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/incorporating-security-features-in-a-running-cluster/</guid><description>Incorporating Security Features in a Running Cluster You can secure a running cluster via one or more of the supported protocols discussed previously. This is done in phases:
Incrementally bounce the cluster nodes to open additional secured port(s). Restart clients using the secured rather than PLAINTEXT port (assuming you are securing the client-broker connection). Incrementally bounce the cluster again to enable broker-to-broker security (if this is required) A final incremental bounce to close the PLAINTEXT port.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/27/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/java-version/</guid><description>Java Version Java 8 and Java 11 are supported. Java 11 performs significantly better if TLS is enabled, so it is highly recommended (it also includes a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. Typical arguments for running Kafka with OpenJDK-based Java implementations (including Oracle JDK) are:</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/28/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/java-version/</guid><description>Java Version Java 8 and Java 11 are supported. Java 11 performs significantly better if TLS is enabled, so it is highly recommended (it also includes a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. Typical arguments for running Kafka with OpenJDK-based Java implementations (including Oracle JDK) are:</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/30/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/java-version/</guid><description>Java Version Java 8 and Java 11 are supported. Java 11 performs significantly better if TLS is enabled, so it is highly recommended (it also includes a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. Typical arguments for running Kafka with OpenJDK-based Java implementations (including Oracle JDK) are:</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/31/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/java-version/</guid><description>Java Version Java 8 and Java 11 are supported. Java 11 performs significantly better if TLS is enabled, so it is highly recommended (it also includes a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities. Typical arguments for running Kafka with OpenJDK-based Java implementations (including Oracle JDK) are:</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/32/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0. Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/33/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0. Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/34/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0. Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/35/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0. Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/36/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported. Note that Java 8 support has been deprecated since Apache Kafka 3.0 and will be removed in Apache Kafka 4.0. Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more). From a security perspective, we recommend the latest released patch version as older freely available versions have disclosed security vulnerabilities.</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/37/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported.
Note that Java 8 support project-wide has been deprecated since Apache Kafka 3.0 and Java 11 support for the broker and tools has been deprecated since Apache Kafka 3.7. Both will be removed in Apache Kafka 4.0.
Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more).</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/38/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported.
Note that Java 8 support project-wide has been deprecated since Apache Kafka 3.0 and Java 11 support for the broker and tools has been deprecated since Apache Kafka 3.7. Both will be removed in Apache Kafka 4.0.
Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more).</description></item><item><title>Java Version</title><link>https://example.kafka-site-md.dev/39/operations/java-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/java-version/</guid><description>Java Version Java 8, Java 11, and Java 17 are supported.
Note that Java 8 support project-wide has been deprecated since Apache Kafka 3.0 and Java 11 support for the broker and tools has been deprecated since Apache Kafka 3.7. Both will be removed in Apache Kafka 4.0.
Java 11 and later versions perform significantly better if TLS is enabled, so they are highly recommended (they also include a number of other performance improvements: G1GC, CRC32C, Compact Strings, Thread-Local Handshakes and more).</description></item><item><title>KRaft vs ZooKeeper</title><link>https://example.kafka-site-md.dev/40/getting-started/zk2kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/zk2kraft/</guid><description>Differences Between KRaft mode and ZooKeeper mode Removed ZooKeeper Features This section documents differences in behavior between KRaft mode and ZooKeeper mode. Specifically, several configurations, metrics and features have changed or are no longer required in KRaft mode. To migrate an existing cluster from ZooKeeper mode to KRaft mode, please refer to the ZooKeeper to KRaft Migration section.
Configurations Removed password encoder-related configurations. These configurations were used in ZooKeeper mode to define the key and backup key for encrypting sensitive data (e.</description></item><item><title>KRaft vs ZooKeeper</title><link>https://example.kafka-site-md.dev/41/getting-started/zk2kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/zk2kraft/</guid><description>Differences Between KRaft mode and ZooKeeper mode Removed ZooKeeper Features This section documents differences in behavior between KRaft mode and ZooKeeper mode. Specifically, several configurations, metrics and features have changed or are no longer required in KRaft mode. To migrate an existing cluster from ZooKeeper mode to KRaft mode, please refer to the ZooKeeper to KRaft Migration section.
Configurations Removed password encoder-related configurations. These configurations were used in ZooKeeper mode to define the key and backup key for encrypting sensitive data (e.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/0100/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
We do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/0101/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
We do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/0102/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.
We do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/0110/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or server; this will allow browsing all metrics with JMX.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/081/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.
We pay particular we do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate &amp;ldquo;kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/082/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.
We pay particular we do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/090/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in both the server and the client. This can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
The easiest way to see the available metrics to fire up jconsole and point it at a running kafka client or server; this will all browsing all metrics with JMX.
We pay particular we do graphing and alerting on the following metrics: Description Mbean name Normal value Message in rate kafka.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/10/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/11/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/20/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server and Scala clients. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/21/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/22/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/23/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/24/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/25/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/0100/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/0101/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/0102/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/0110/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/07/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/08/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/081/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/082/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/090/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/10/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/11/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/20/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/21/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/22/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/23/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/24/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/25/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/26/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/27/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/28/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/30/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/31/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/32/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/33/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/34/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/35/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/36/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/37/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/38/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/39/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/40/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/</guid><description/></item><item><title>Operations</title><link>https://example.kafka-site-md.dev/41/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/</guid><description/></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/0102/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/upgrade-guide/</guid><description>Upgrade Guide &amp;amp; API Changes If you want to upgrade from 0.10.1.x to 0.10.2, see the Upgrade Section for 0.10.2. It highlights incompatible changes you need to consider to upgrade your code and application. See below a complete list of 0.10.2 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
If you want to upgrade from 0.10.0.x to 0.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/0110/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/upgrade-guide/</guid><description>Upgrade Guide &amp;amp; API Changes If you want to upgrade from 0.10.2.x to 0.11.0 you don&amp;rsquo;t need to do any code changes as the public API is fully backward compatible. However, some configuration parameters were deprecated and thus it is recommend to update your code eventually to allow for future upgrades. See below a complete list of 0.11.0 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/10/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/upgrade-guide/</guid><description>Upgrade Guide &amp;amp; API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.0.x you don&amp;rsquo;t need to do any code changes as the public API is fully backward compatible. However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades. See below a complete list of 1.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/11/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
If you want to upgrade from 1.0.x to 1.1.0 and you have customized window store implementations on the ReadOnlyWindowStore interface you&amp;rsquo;d need to update your code to incorporate the newly added public APIs. Otherwise, if you are using Java 7 you don&amp;rsquo;t need to make any code changes as the public API is fully backward compatible; but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/20/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.0.0 is possible: (1) you need to make sure to update you code and config accordingly, because there are some minor non-compatible API changes since older releases (the code changes are expected to be minimal, please see below for the details), (2) upgrading to 2.0.0 in the online mode requires two rolling bounces.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/21/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.1.0 is possible: (1) if you are upgrading from 2.0.x to 2.1.0 then a single rolling bounce is needed to swap in the new jar, (2) if you are upgrading from older versions than 2.0.x in the online mode, you would need two rolling bounces where the first rolling bounce phase you need to set config upgrade.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/22/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.2.0 is possible: (1) if you are upgrading from 2.0.x to 2.2.0 then a single rolling bounce is needed to swap in the new jar, (2) if you are upgrading from older versions than 2.0.x in the online mode, you would need two rolling bounces where the first rolling bounce phase you need to set config upgrade.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/23/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.3.0 is possible: (1) if you are upgrading from 2.0.x to 2.3.0 then a single rolling bounce is needed to swap in the new jar, (2) if you are upgrading from older versions than 2.0.x in the online mode, you would need two rolling bounces where the first rolling bounce phase you need to set config upgrade.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/24/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.4.1 is possible: you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/25/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.5.0 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/26/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.6.3 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/27/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.7.2 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/28/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 2.8.2 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/30/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.0.0 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/31/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.1.1-SNAPSHOT is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/32/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.2.1 is possible: if upgrading from 2.3 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;2.3&amp;quot;) and during the second you remove it. This is required to safely upgrade to the new cooperative rebalancing protocol of the embedded consumer.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/33/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.3.1 is possible: if upgrading from 3.2 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.2&amp;quot;) and during the second you remove it. This is required to safely handle 2 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/34/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.4.0 is possible: if upgrading from 3.2 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.2&amp;quot;) and during the second you remove it. This is required to safely handle 2 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/35/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.5.2 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/36/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.6.2 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/37/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.7.2 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/38/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.8.1 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/39/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 3.9.1 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/40/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 4.0.0 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Upgrade Guide</title><link>https://example.kafka-site-md.dev/41/streams/upgrade-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/upgrade-guide/</guid><description>Upgrade Guide and API Changes Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade
Upgrading from any older version to 4.1.0 is possible: if upgrading from 3.4 or below, you will need to do two rolling bounces, where during the first rolling bounce phase you set the config upgrade.from=&amp;quot;older version&amp;quot; (possible values are &amp;quot;0.10.0&amp;quot; - &amp;quot;3.4&amp;quot;) and during the second you remove it. This is required to safely handle 3 changes.</description></item><item><title>Zookeeper</title><link>https://example.kafka-site-md.dev/08/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/operations/zookeeper/</guid><description>Zookeeper Stable version At LinkedIn, we are running Zookeeper 3.3.*. Version 3.3.3 has known serious issues regarding ephemeral node deletion and session expirations. After running into those issues in production, we upgraded to 3.3.4 and have been running that smoothly for over a year now.
Operationalizing Zookeeper Operationally, we do the following for a healthy Zookeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/0100/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper is such that only brokers will be able to modify the corresponding znodes, but znodes are world readable. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of znodes can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/0101/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/0102/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/0110/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/10/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/11/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/20/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/21/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/22/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/23/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/24/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication New clusters To enable ZooKeeper authentication on brokers, there are two necessary steps:
Create a JAAS login file and set the appropriate system property to point to it as described above Set the configuration property zookeeper.set.acl in each broker to true The metadata stored in ZooKeeper for the Kafka cluster is world-readable, but can only be modified by the brokers. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of that data can cause cluster disruption.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/25/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/26/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/27/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/28/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/30/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/31/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/32/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>Compatibility</title><link>https://example.kafka-site-md.dev/40/getting-started/compatibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/compatibility/</guid><description>Compatibility With the release of Kafka 4.0, significant changes have been introduced that impact compatibility across various components. To assist users in planning upgrades and ensuring seamless interoperability, a comprehensive compatibility matrix has been prepared.
JDK Compatibility Across Kafka Versions Module Kafka Version Java 11 Java 17 Java 23 Clients 4.0.0    Streams 4.0.0    Connect 4.0.0    Server 4.0.0    Note: Java 8 is removed in Kafka 4.</description></item><item><title>Compatibility</title><link>https://example.kafka-site-md.dev/41/getting-started/compatibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/compatibility/</guid><description>Compatibility With the release of Kafka 4.0, significant changes have been introduced that impact compatibility across various components. To assist users in planning upgrades and ensuring seamless interoperability, a comprehensive compatibility matrix has been prepared.
JDK Compatibility Across Kafka Versions Module Kafka Version Java 11 Java 17 Java 23 Clients 4.0.0    Streams 4.0.0    Connect 4.0.0    Server 4.0.0    Note: Java 8 is removed in Kafka 4.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/27/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/28/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/30/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/31/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/32/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/33/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/34/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/35/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/36/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/37/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/38/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Hardware and OS</title><link>https://example.kafka-site-md.dev/39/operations/hardware-and-os/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/hardware-and-os/</guid><description>Hardware and OS We are using dual quad-core Intel Xeon machines with 24GB of memory.
You need sufficient memory to buffer active readers and writers. You can do a back-of-the-envelope estimate of memory needs by assuming you want to be able to buffer for 30 seconds and compute your memory need as write_throughput*30.
The disk throughput is important. We have 8x7200 rpm SATA drives. In general disk throughput is the performance bottleneck, and more disks is better.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/26/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/40/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/41/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Security</title><link>https://example.kafka-site-md.dev/0100/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/0101/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/0102/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/0110/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/07/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/08/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/081/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/082/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/090/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/10/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/11/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/20/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/21/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/22/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/23/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/24/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/25/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/26/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/27/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/28/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/30/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/31/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/32/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/33/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/34/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/35/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/36/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/37/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/38/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/39/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/40/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/security/</guid><description/></item><item><title>Security</title><link>https://example.kafka-site-md.dev/41/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/security/</guid><description/></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/testing/</guid><description>Testing a Streams Application To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.1.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder. The test driver simulates the library runtime that continuously fetches records from input topics and processes them by traversing the topology.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.0.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.1.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.2.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.3.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.5.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.6.3&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.7.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.8.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.0.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.1.1-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.2.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.3.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.4.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.5.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.6.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.7.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.8.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.9.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;4.0.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>Testing a Streams Application</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/testing/</guid><description>Testing Kafka Streams Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;4.1.0&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/0100/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.6, which is the one ZkClient 0.7 uses. ZkClient is the client layer Kafka uses to interact with ZooKeeper.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/0101/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.8, which is the one ZkClient 0.9 uses. ZkClient is the client layer Kafka uses to interact with ZooKeeper.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/0102/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/0110/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/081/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/operations/zookeeper/</guid><description>ZooKeeper Stable version At LinkedIn, we are running ZooKeeper 3.3.*. Version 3.3.3 has known serious issues regarding ephemeral node deletion and session expirations. After running into those issues in production, we upgraded to 3.3.4 and have been running that smoothly for over a year now.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/082/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/operations/zookeeper/</guid><description>ZooKeeper Stable version At LinkedIn, we are running ZooKeeper 3.3.*. Version 3.3.3 has known serious issues regarding ephemeral node deletion and session expirations. After running into those issues in production, we upgraded to 3.3.4 and have been running that smoothly for over a year now.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/090/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/operations/zookeeper/</guid><description>ZooKeeper Stable version At LinkedIn, we are running ZooKeeper 3.3.*. Version 3.3.3 has known serious issues regarding ephemeral node deletion and session expirations. After running into those issues in production, we upgraded to 3.3.4 and have been running that smoothly for over a year now.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/10/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/11/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/20/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/21/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/22/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/23/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.4 and the latest release of that branch is 3.4.9.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/24/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/25/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/33/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/34/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/35/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/36/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/37/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/38/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Authentication</title><link>https://example.kafka-site-md.dev/39/security/zookeeper-authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/zookeeper-authentication/</guid><description>ZooKeeper Authentication ZooKeeper supports mutual TLS (mTLS) authentication beginning with the 3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and mTLS &amp;ndash; either individually or both together &amp;ndash; beginning with version 2.5. See KIP-515: Enable ZK client to use the new TLS supported authentication for more details.
When using mTLS alone, every broker and any CLI tools (such as the ZooKeeper Security Migration Tool) should identify itself with the same Distinguished Name (DN) because it is the DN that is ACL&amp;rsquo;ed.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/25/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/26/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/27/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/28/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/30/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/31/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/32/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>Docker</title><link>https://example.kafka-site-md.dev/37/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/getting-started/docker/</guid><description>Introduction Docker is a popular container runtime. Docker images for Apache Kafka can be found on Docker Hub and are available from version 3.7.0.
Getting the kafka docker image Docker image can be pulled from Docker Hub using the following command:-
$ docker pull apache/kafka:3.7.2 If you want to fetch the latest version of the docker image use following command:-
$ docker pull apache/kafka:latest Start kafka with default configs Run docker image on default port 9092:-</description></item><item><title>Docker</title><link>https://example.kafka-site-md.dev/38/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/getting-started/docker/</guid><description>JVM Based Apache Kafka Docker Image Docker is a popular container runtime. Docker images for the JVM based Apache Kafka can be found on Docker Hub and are available from version 3.7.0.
Docker image can be pulled from Docker Hub using the following command:
$ docker pull apache/kafka:3.8.1 If you want to fetch the latest version of the Docker image use following command:
$ docker pull apache/kafka:latest To start the Kafka container using this Docker image with default configs and on default port 9092:</description></item><item><title>Docker</title><link>https://example.kafka-site-md.dev/39/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/getting-started/docker/</guid><description>JVM Based Apache Kafka Docker Image Docker is a popular container runtime. Docker images for the JVM based Apache Kafka can be found on Docker Hub and are available from version 3.7.0.
Docker image can be pulled from Docker Hub using the following command:
$ docker pull apache/kafka:3.9.1 If you want to fetch the latest version of the Docker image use following command:
$ docker pull apache/kafka:latest To start the Kafka container using this Docker image with default configs and on default port 9092:</description></item><item><title>Docker</title><link>https://example.kafka-site-md.dev/40/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/getting-started/docker/</guid><description>JVM Based Apache Kafka Docker Image Docker is a popular container runtime. Docker images for the JVM based Apache Kafka can be found on Docker Hub and are available from version 3.7.0.
Docker image can be pulled from Docker Hub using the following command:
$ docker pull apache/kafka:4.0.0 If you want to fetch the latest version of the Docker image use following command:
$ docker pull apache/kafka:latest To start the Kafka container using this Docker image with default configs and on default port 9092:</description></item><item><title>Docker</title><link>https://example.kafka-site-md.dev/41/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/getting-started/docker/</guid><description>JVM Based Apache Kafka Docker Image Docker is a popular container runtime. Docker images for the JVM based Apache Kafka can be found on Docker Hub and are available from version 3.7.0.
Docker image can be pulled from Docker Hub using the following command:
$ docker pull apache/kafka:4.1.0 If you want to fetch the latest version of the Docker image use following command:
$ docker pull apache/kafka:latest To start the Kafka container using this Docker image with default configs and on default port 9092:</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams API enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Interactive Queries</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/interactive-queries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/interactive-queries/</guid><description>Interactive Queries Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/0100/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/0101/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/0102/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/0110/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/07/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/08/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/081/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/082/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/090/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/10/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/11/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/20/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/21/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/22/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/23/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/24/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/25/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/26/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/27/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/28/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/30/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/31/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/32/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/33/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/34/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/35/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/36/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/37/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/38/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/39/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/40/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/kafka-connect/</guid><description/></item><item><title>Kafka Connect</title><link>https://example.kafka-site-md.dev/41/kafka-connect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/kafka-connect/</guid><description/></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/40/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/41/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/27/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/28/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/30/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/31/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/32/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/33/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/34/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/35/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/36/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/37/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/38/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>Monitoring</title><link>https://example.kafka-site-md.dev/39/operations/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/monitoring/</guid><description>Monitoring Kafka uses Yammer Metrics for metrics reporting in the server. The Java clients use Kafka Metrics, a built-in metrics registry that minimizes transitive dependencies pulled into client applications. Both expose metrics via JMX and can be configured to report stats using pluggable stats reporters to hook up to your monitoring system.
All Kafka rate metrics have a corresponding cumulative count metric with suffix -total. For example, records-consumed-rate has a corresponding metric named records-consumed-total.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/26/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/33/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/34/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/35/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/36/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/37/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/38/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>ZooKeeper Encryption</title><link>https://example.kafka-site-md.dev/39/security/zookeeper-encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/security/zookeeper-encryption/</guid><description>ZooKeeper Encryption ZooKeeper connections that use mutual TLS are encrypted. Beginning with ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5) ZooKeeper supports a sever-side config ssl.clientAuth (case-insensitively: want/need/none are the valid options, the default is need), and setting this value to none in ZooKeeper allows clients to connect via a TLS-encrypted connection without presenting their own certificate. Here is a sample (partial) Kafka Broker configuration for connecting to ZooKeeper with just TLS encryption.</description></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/0100/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/0101/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/0102/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/0110/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/07/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/08/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/081/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/082/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/090/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/10/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/11/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/20/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/21/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/22/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/23/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/24/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/25/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/26/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/27/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/28/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/30/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/31/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/32/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/33/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/34/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/35/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/36/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/37/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/38/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/39/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/40/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/</guid><description/></item><item><title>Kafka Streams</title><link>https://example.kafka-site-md.dev/41/streams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/</guid><description/></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Memory Management</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/memory-mgmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/memory-mgmt/</guid><description>Memory Management You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/40/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/41/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/27/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/28/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/30/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/31/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/32/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/33/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/34/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
Operationalizing ZooKeeper Operationally, we do the following for a healthy ZooKeeper installation:
Redundancy in the physical/hardware/network layout: try not to put them all in the same rack, decent (but don&amp;rsquo;t go nuts) hardware, try to keep redundant power and network paths, etc. A typical ZooKeeper ensemble has 5 or 7 servers, which tolerates 2 and 3 servers down, respectively.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/35/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
ZooKeeper Deprecation With the release of Apache Kafka 3.5, Zookeeper is now marked deprecated. Removal of ZooKeeper is planned in the next major release of Apache Kafka (version 4.0), which is scheduled to happen no sooner than April 2024. During the deprecation phase, ZooKeeper is still supported for metadata management of Kafka clusters, but it is not recommended for new deployments.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/36/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.5. Kafka is regularly updated to include the latest release in the 3.5 series.
ZooKeeper Deprecation With the release of Apache Kafka 3.5, Zookeeper is now marked deprecated. Removal of ZooKeeper is planned in the next major release of Apache Kafka (version 4.0), which is scheduled to happen no sooner than April 2024. During the deprecation phase, ZooKeeper is still supported for metadata management of Kafka clusters, but it is not recommended for new deployments.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/37/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.8. Kafka is regularly updated to include the latest release in the 3.8 series.
ZooKeeper Deprecation With the release of Apache Kafka 3.5, Zookeeper is now marked deprecated. Removal of ZooKeeper is planned in the next major release of Apache Kafka (version 4.0), which is scheduled to happen no sooner than April 2024. During the deprecation phase, ZooKeeper is still supported for metadata management of Kafka clusters, but it is not recommended for new deployments.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/38/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.8. Kafka is regularly updated to include the latest release in the 3.8 series.
ZooKeeper Deprecation With the release of Apache Kafka 3.5, Zookeeper is now marked deprecated. Removal of ZooKeeper is planned in the next major release of Apache Kafka (version 4.0), which is scheduled to happen no sooner than April 2024. During the deprecation phase, ZooKeeper is still supported for metadata management of Kafka clusters, but it is not recommended for new deployments.</description></item><item><title>ZooKeeper</title><link>https://example.kafka-site-md.dev/39/operations/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/zookeeper/</guid><description>ZooKeeper Stable version The current stable branch is 3.8. Kafka is regularly updated to include the latest release in the 3.8 series.
ZooKeeper Deprecation With the release of Apache Kafka 3.5, Zookeeper is now marked deprecated. Removal of ZooKeeper is planned in the next major release of Apache Kafka (version 4.0), which is scheduled to happen no sooner than April 2024. During the deprecation phase, ZooKeeper is still supported for metadata management of Kafka clusters, but it is not recommended for new deployments.</description></item><item><title>Consumer Rebalance Protocol</title><link>https://example.kafka-site-md.dev/40/operations/consumer-rebalance-protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/consumer-rebalance-protocol/</guid><description>Consumer Rebalance Protocol Overview Starting from Apache Kafka 4.0, the Next Generation of the Consumer Rebalance Protocol (KIP-848) is Generally Available (GA). It improves the scalability of consumer groups while simplifying consumers. It also decreases rebalance times, thanks to its fully incremental design, which no longer relies on a global synchronization barrier.
Consumer Groups using the new protocol are now referred to as Consumer groups, while groups using the old protocol are referred to as Classic groups.</description></item><item><title>Consumer Rebalance Protocol</title><link>https://example.kafka-site-md.dev/41/operations/consumer-rebalance-protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/consumer-rebalance-protocol/</guid><description>Consumer Rebalance Protocol Overview Starting from Apache Kafka 4.0, the Next Generation of the Consumer Rebalance Protocol (KIP-848) is Generally Available (GA). It improves the scalability of consumer groups while simplifying consumers. It also decreases rebalance times, thanks to its fully incremental design, which no longer relies on a global synchronization barrier.
Consumer Groups using the new protocol are now referred to as Consumer groups, while groups using the old protocol are referred to as Classic groups.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/33/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/34/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/35/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/36/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/37/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/38/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>KRaft</title><link>https://example.kafka-site-md.dev/39/operations/kraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/kraft/</guid><description>KRaft Configuration Process Roles In KRaft mode each Kafka server can be configured as a controller, a broker, or both using the process.roles property. This property can have the following values:
If process.roles is set to broker, the server acts as a broker. If process.roles is set to controller, the server acts as a controller. If process.roles is set to broker,controller, the server acts as both a broker and a controller.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements.</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Running Streams Applications</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/running-app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/running-app/</guid><description>Running Streams Applications You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application You can package your Java application as a fat JAR file and then start the application like this:</description></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/0100/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/0101/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/0102/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/0110/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/07/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/08/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/081/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/082/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/090/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/</guid><description/></item><item><title>Streams Developer Guide</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/</guid><description/></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Managing Streams Application Topics</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/manage-topics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/manage-topics/</guid><description>Managing Streams Application Topics A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/36/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/37/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/38/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>Tiered Storage</title><link>https://example.kafka-site-md.dev/39/operations/tiered-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/operations/tiered-storage/</guid><description>Tiered Storage Tiered Storage Overview Kafka data is mostly consumed in a streaming fashion using tail reads. Tail reads leverage OS&amp;rsquo;s page cache to serve the data instead of disk reads. Older data is typically read from the disk for backfill or failure recovery purposes and is infrequent.
In the tiered storage approach, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments.</description></item><item><title>Transaction Protocol</title><link>https://example.kafka-site-md.dev/40/operations/transaction-protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/transaction-protocol/</guid><description>Transaction Protocol Overview Starting from Apache Kafka 4.0, Transactions Server Side Defense (KIP-890) brings a strengthened transactional protocol. When enabled and using 4.0 producer clients, the producer epoch is bumped on every transaction to ensure every transaction includes the intended messages and duplicates are not written as part of the next transaction.
The protocol is automatically enabled on the server since Apache Kafka 4.0. Enabling and disabling the protocol is controlled by the transaction.</description></item><item><title>Transaction Protocol</title><link>https://example.kafka-site-md.dev/41/operations/transaction-protocol/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/transaction-protocol/</guid><description>Transaction Protocol Overview Starting from Apache Kafka 4.0, Transactions Server Side Defense (KIP-890) brings a strengthened transactional protocol. When enabled and using 4.0 producer clients, the producer epoch is bumped on every transaction to ensure every transaction includes the intended messages and duplicates are not written as part of the next transaction.
The protocol is automatically enabled on the server since Apache Kafka 4.0. Enabling and disabling the protocol is controlled by the transaction.</description></item><item><title>Eligible Leader Replicas</title><link>https://example.kafka-site-md.dev/40/operations/eligible-leader-replicas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/operations/eligible-leader-replicas/</guid><description>Eligible Leader Replicas Overview Starting from Apache Kafka 4.0, Eligible Leader Replicas (KIP-966 Part 1) is available for the users to an improvement to Kafka replication. As the &amp;ldquo;strict min ISR&amp;rdquo; rule has been generally applied, which means the high watermark for the data partition can&amp;rsquo;t advance if the size of the ISR is smaller than the min ISR(min.insync.replicas), it makes some replicas that are not in the ISR safe to become the leader.</description></item><item><title>Eligible Leader Replicas</title><link>https://example.kafka-site-md.dev/41/operations/eligible-leader-replicas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/operations/eligible-leader-replicas/</guid><description>Eligible Leader Replicas Overview Starting from Apache Kafka 4.0, Eligible Leader Replicas (KIP-966 Part 1) is available for the users to an improvement to Kafka replication (ELR is enabled by default on new clusters starting 4.1). As the &amp;ldquo;strict min ISR&amp;rdquo; rule has been generally applied, which means the high watermark for the data partition can&amp;rsquo;t advance if the size of the ISR is smaller than the min ISR(min.insync.replicas), it makes some replicas that are not in the ISR safe to become the leader.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Streams Security</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/security/</guid><description>Streams Security Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/10/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset to the beginning of the topic.</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/11/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/20/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/21/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/22/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/23/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/24/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/25/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/26/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/27/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/28/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/30/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/31/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/32/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/33/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/34/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/35/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/36/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/37/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/38/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/39/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/40/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, and output) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>Application Reset Tool</title><link>https://example.kafka-site-md.dev/41/streams/developer-guide/app-reset-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/streams/developer-guide/app-reset-tool/</guid><description>Application Reset Tool You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, and output) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:
Input topics: Reset offsets to specified position (by default to the beginning of the topic).</description></item><item><title>AK 0.10.0.X</title><link>https://example.kafka-site-md.dev/0100/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0100/</guid><description/></item><item><title>AK 0.10.1.X</title><link>https://example.kafka-site-md.dev/0101/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0101/</guid><description/></item><item><title>AK 0.10.2.X</title><link>https://example.kafka-site-md.dev/0102/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0102/</guid><description/></item><item><title>AK 0.11.0.X</title><link>https://example.kafka-site-md.dev/0110/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/0110/</guid><description/></item><item><title>AK 0.7.X</title><link>https://example.kafka-site-md.dev/07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/07/</guid><description/></item><item><title>AK 0.8.1.X</title><link>https://example.kafka-site-md.dev/081/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/081/</guid><description/></item><item><title>AK 0.8.2.X</title><link>https://example.kafka-site-md.dev/082/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/082/</guid><description/></item><item><title>AK 0.8.X</title><link>https://example.kafka-site-md.dev/08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/08/</guid><description/></item><item><title>AK 0.9.0.X</title><link>https://example.kafka-site-md.dev/090/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/090/</guid><description/></item><item><title>AK 1.0.X</title><link>https://example.kafka-site-md.dev/10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/10/</guid><description/></item><item><title>AK 1.1.X</title><link>https://example.kafka-site-md.dev/11/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/11/</guid><description/></item><item><title>AK 2.0.X</title><link>https://example.kafka-site-md.dev/20/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/20/</guid><description/></item><item><title>AK 2.1.X</title><link>https://example.kafka-site-md.dev/21/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/21/</guid><description/></item><item><title>AK 2.2.X</title><link>https://example.kafka-site-md.dev/22/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/22/</guid><description/></item><item><title>AK 2.3.X</title><link>https://example.kafka-site-md.dev/23/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/23/</guid><description/></item><item><title>AK 2.4.X</title><link>https://example.kafka-site-md.dev/24/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/24/</guid><description/></item><item><title>AK 2.5.X</title><link>https://example.kafka-site-md.dev/25/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/25/</guid><description/></item><item><title>AK 2.6.X</title><link>https://example.kafka-site-md.dev/26/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/26/</guid><description/></item><item><title>AK 2.7.X</title><link>https://example.kafka-site-md.dev/27/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/27/</guid><description/></item><item><title>AK 2.8.X</title><link>https://example.kafka-site-md.dev/28/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/28/</guid><description/></item><item><title>AK 3.0.X</title><link>https://example.kafka-site-md.dev/30/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/30/</guid><description/></item><item><title>AK 3.1.X</title><link>https://example.kafka-site-md.dev/31/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/31/</guid><description/></item><item><title>AK 3.2.X</title><link>https://example.kafka-site-md.dev/32/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/32/</guid><description/></item><item><title>AK 3.3.X</title><link>https://example.kafka-site-md.dev/33/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/33/</guid><description/></item><item><title>AK 3.4.X</title><link>https://example.kafka-site-md.dev/34/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/34/</guid><description/></item><item><title>AK 3.5.X</title><link>https://example.kafka-site-md.dev/35/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/35/</guid><description/></item><item><title>AK 3.6.X</title><link>https://example.kafka-site-md.dev/36/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/36/</guid><description/></item><item><title>AK 3.7.X</title><link>https://example.kafka-site-md.dev/37/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/37/</guid><description/></item><item><title>AK 3.8.X</title><link>https://example.kafka-site-md.dev/38/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/38/</guid><description/></item><item><title>AK 3.9.X</title><link>https://example.kafka-site-md.dev/39/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/39/</guid><description/></item><item><title>AK 4.0.X</title><link>https://example.kafka-site-md.dev/40/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/40/</guid><description/></item><item><title>AK 4.1.X</title><link>https://example.kafka-site-md.dev/41/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.kafka-site-md.dev/41/</guid><description/></item></channel></rss>